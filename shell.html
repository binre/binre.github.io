<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>the refinery.shell documentation</title>
<meta name="description" content="On a Mission to Refine Binaries" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/tomorrow-night-bright.min.css" rel="stylesheet">
<style>.flex{display:flex !important}body{background-color:#0D0D0D;color:#EEEEEE;font-size:16pt}@font-face{font-family:"FixedSysEx";src:local('Fixedsys Excelsior 3.01-L2'),local('Fixedsys Excelsior 3.01'),local('FixedSysEx'),url(FixedSysEx.ttf) format('truetype')}code,pre,body,html{font-family:FixedSysEx,monospace}b,strong{font-weight:normal}#content{padding:20px}#sidebar{padding:1vw;overflow:hidden}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #EEEEEE;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}hr{display:none}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}a{color:#EE8080;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#EEEEEE}// .title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#BB4040}pre code{background:#000000;display:block;padding:1px 0px 4px 0px;line-height:100%}code{background:#000000;padding:1px 0px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#000000;border:0;border-top:1px solid #EEEEEE;border-bottom:1px solid #EEEEEE;margin:1em 0;padding:1ex;overflow-x:auto}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index ul{list-style-type:square;padding:0}// #index h4{font-weight:bold}#index h4 + ul{margin-bottom:.6em}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 10px}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#000000;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#0D0D0D}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#EEEEEE;border-left:5px solid #EEEEEE;padding-left:1em}.inheritance em{font-style:normal}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1ch}img{max-width:100%}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition.note,.admonition.info,.admonition.todo,.admonition.versionadded,.admonition.important,.admonition.tip,.admonition.hint{background:#054000}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#998800}.admonition.error,.admonition.danger,.admonition.caution{background:#300000}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%}#content{width:70%;max-width:100ch;padding:1vw}main{display:flex;flex-direction:row;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #EEEEEE;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>refinery.shell</code></h1>
</header>
<section id="section-intro">
<h1 id="shell-like-unit-interface">Shell-Like Unit Interface</h1>
<p>Any unit from the <code><a title="refinery" href="index.html">refinery</a></code> module can also be imported from this module. When imported from here,
the units are initialized differently: They can be given string arguments as they would receive on
the command line. For example:</p>
<pre><code>&gt;&gt;&gt; from refinery.shell import *
&gt;&gt;&gt; emit('ABC', 'DEF') [ pop('t') | xor('var:t') | pack('-R') ] | str
'575'
</code></pre>
<p>This especially gives easier access to the powerful <code><a title="refinery.lib.meta" href="lib/meta.html">refinery.lib.meta</a></code> variables and the entire
multibin format expressions, see <code><a title="refinery.lib.argformats" href="lib/argformats.html">refinery.lib.argformats</a></code>.</p>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/shell.py#L1-L42" class="git-link">Browse git</a>
</summary>
<pre><code class="python">&#34;&#34;&#34;
# Shell-Like Unit Interface

Any unit from the `refinery` module can also be imported from this module. When imported from here,
the units are initialized differently: They can be given string arguments as they would receive on
the command line. For example:

    &gt;&gt;&gt; from refinery.shell import *
    &gt;&gt;&gt; emit(&#39;ABC&#39;, &#39;DEF&#39;) [ pop(&#39;t&#39;) | xor(&#39;var:t&#39;) | pack(&#39;-R&#39;) ] | str
    &#39;575&#39;

This especially gives easier access to the powerful `refinery.lib.meta` variables and the entire
multibin format expressions, see `refinery.lib.argformats`.
&#34;&#34;&#34;
from functools import wraps
from refinery import __unit_loader__, Unit

with __unit_loader__:
    __all__ = sorted(__unit_loader__.units, key=lambda x: x.lower())


class __pdoc2__:
    def __class_getitem__(*_):
        return &#39;&#39;


def __getattr__(name):
    with __unit_loader__:
        unit: Unit = __unit_loader__.resolve(name)

    if unit is None:
        raise AttributeError(name)

    class _unit(unit):
        def __new__(cls, *args, **kwargs):
            return unit.assemble(*args, **kwargs)

    return wraps(unit, updated=[])(_unit)


def __dir__():
    return __all__</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Units</h2>
<dl>
<dt id="refinery.shell.a3x"><code class="flex name class">
<span>class <span class="ident">a3x</span></span>
<span>(</span><span>*paths, list=False, join_path=False, drop_path=False, fuzzy=0, exact=False, regex=False, path=b'path')</span>
</code></dt>
<dd>
<section class="desc"><p>Extracts embedded resources from compiled AutoIt scripts and decompiles the embedded script
bytecode. The unit also works on compiled AutoIt executables.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/formats/a3x.py#L967-L1059" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class a3x(PathExtractorUnit):
    &#34;&#34;&#34;
    Extracts embedded resources from compiled AutoIt scripts and decompiles the embedded script
    bytecode. The unit also works on compiled AutoIt executables.
    &#34;&#34;&#34;

    def unpack(self, data: bytearray):
        view = memoryview(data)
        cursor = 0
        errors: Dict[int, Exception] = {}
        script_count = 0
        truncated: Set[A3xRecord] = set()
        intact: Set[A3xRecord] = set()

        def _package(records: Iterable[A3xRecord]) -&gt; Generator[UnpackResult, None, None]:
            for k, record in enumerate(records, 1):
                self.log_info(F&#39;record {k} type:&#39;, record.type)
                self.log_info(F&#39;record {k} path:&#39;, record.src_path)
                if record.path is None:
                    continue
                yield UnpackResult(
                    record.path,
                    record.extract,
                    srcpath=record.src_path,
                    created=record.created.isoformat(&#39; &#39;, &#39;seconds&#39;),
                    written=record.written.isoformat(&#39; &#39;, &#39;seconds&#39;),
                )

        while cursor &lt; len(view):
            self.log_debug(F&#39;searching at offset 0x{cursor:08X}&#39;)
            nc = data.find(A3xScript.MAGIC, cursor)
            if nc &gt;= 0:
                cursor = nc
            else:
                rp = data.find(A3xRecord.MAGIC, cursor) - A3xScript.WIDTH
                if rp &lt;= cursor:
                    break
                cursor = rp
            try:
                script = A3xScript(view[cursor:])
            except Exception as E:
                errors[cursor] = E
                cursor += 1
                continue
            else:
                valid = script.has_valid_magic()
                if valid:
                    _m = &#39;correct&#39;
                else:
                    _m = &#39;invalid&#39;
                if not script.body:
                    cursor += A3xScript.WIDTH
                    if not script.has_valid_magic():
                        cursor += len(A3xRecord.MAGIC)
                    continue
                if script.truncated:
                    _a = &#39;truncated&#39;
                    truncated.update(script.body)
                else:
                    script_count += 1
                    _a = &#39;intact&#39;
                    intact.update(script.body)
                self.log_info(
                    F&#39;{_a} script of type&#39;, script.type,
                    F&#39;and length 0x{len(script):08X}&#39;,
                    F&#39;with {len(script.body)} records and {_m} magic:&#39;,
                    script.magic
                )
                cursor += len(script)
                if script.truncated:
                    if not script.has_valid_magic():
                        cursor += len(A3xRecord.MAGIC)
                    continue

            yield from _package(script.body)

        remaining = truncated - intact
        if remaining:
            self.log_warn(&#39;emitting records from truncated scripts&#39;)
            yield from _package(remaining)
            return
        elif truncated:
            self.log_debug(&#39;good news: intact scripts contained all records from truncated scripts&#39;)
        if script_count == 0:
            error = None
            for offset, error in errors.items():
                self.log_warn(F&#39;error at offset 0x{offset:08X}:&#39;, error)
            if error:
                raise error

    @classmethod
    def handles(cls, data: bytearray) -&gt; Optional[bool]:
        return A3xScript.MAGIC in data or A3xRecord.MAGIC in data</code></pre>
</details>
</dd>
<dt id="refinery.shell.a85"><code class="flex name class">
<span>class <span class="ident">a85</span></span>
</code></dt>
<dd>
<section class="desc"><p>Ascii85 encoding and decoding, the predecessor variant of Base85 with a different alphabet.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/encoding/a85.py#L9-L24" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class a85(Unit):
    &#34;&#34;&#34;
    Ascii85 encoding and decoding, the predecessor variant of Base85 with a different alphabet.
    &#34;&#34;&#34;
    def reverse(self, data):
        return base64.a85encode(data)

    def process(self, data):
        if re.search(BR&#39;\s&#39;, data) is not None:
            data = re.sub(BR&#39;\s+&#39;, B&#39;&#39;, data)
        return base64.a85decode(data)

    @classmethod
    def handles(self, data: bytearray):
        from refinery.lib.patterns import formats
        return formats.spaced_a85.value.fullmatch(data)</code></pre>
</details>
</dd>
<dt id="refinery.shell.add"><code class="flex name class">
<span>class <span class="ident">add</span></span>
<span>(</span><span>argument, bigendian=False, blocksize=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Add the given argument to each block.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/blockwise/add.py#L6-L13" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class add(BinaryOperationWithAutoBlockAdjustment):
    &#34;&#34;&#34;
    Add the given argument to each block.
    &#34;&#34;&#34;
    @staticmethod
    def operate(a, b): return a + b
    @staticmethod
    def inplace(a, b): a += b</code></pre>
</details>
</dd>
<dt id="refinery.shell.adler32"><code class="flex name class">
<span>class <span class="ident">adler32</span></span>
<span>(</span><span>text=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Returns the Adler32 Hash of the input data.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/crypto/hash/checksums.py#L20-L25" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class adler32(HashUnit):
    &#34;&#34;&#34;
    Returns the Adler32 Hash of the input data.
    &#34;&#34;&#34;
    def _algorithm(self, data: bytes) -&gt; bytes:
        return struct.pack(&#39;&gt;I&#39;, zlib.adler32(data))</code></pre>
</details>
</dd>
<dt id="refinery.shell.aes"><code class="flex name class">
<span>class <span class="ident">aes</span></span>
<span>(</span><span>key, iv=b'', *, padding=None, mode=None, raw=False, little_endian=False, segment_size=0, mac_len=0, assoc_len=0)</span>
</code></dt>
<dd>
<section class="desc"><p>AES encryption and decryption.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/crypto/cipher/aes.py#L9-L13" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class aes(StandardBlockCipherUnit, cipher=PyCryptoFactoryWrapper(AES)):
    &#34;&#34;&#34;
    AES encryption and decryption.
    &#34;&#34;&#34;
    pass</code></pre>
</details>
</dd>
<dt id="refinery.shell.alu"><code class="flex name class">
<span>class <span class="ident">alu</span></span>
<span>(</span><span>operator, *argument, seed=0, prologue=None, epilogue=None, inc=False, dec=False, cbc=False, bigendian=False, blocksize=None, precision=None)</span>
</code></dt>
<dd>
<section class="desc"><p>The arithmetic-logical unit. It allows you to specify a custom Python expression where the following
variables are allowed:</p>
<ul>
<li>the variable <code>A</code>: same as <code>V[0]</code></li>
<li>the variable <code>B</code>: current block</li>
<li>the variable <code>E</code>: block value of encoded input (not changed after update)</li>
<li>the variable <code>N</code>: number of bytes in the input</li>
<li>the variable <code>K</code>: current index in the input</li>
<li>the variable <code>S</code>: the internal state value</li>
<li>the variable <code>V</code>: the vector of arguments</li>
<li>the variable <code>I</code>: function that casts to a signed int in current precision</li>
<li>the variable <code>U</code>: function that casts to unsigned int in current precision</li>
<li>the variable <code>R</code>: function; <code>R(x,4)</code> rotates x by 4 to the right</li>
<li>the variable <code>L</code>: function; <code>L(x,4)</code> rotates x by 4 to the left</li>
<li>the variable <code>M</code>: function; <code>M(x,8)</code> picks the lower 8 bits of x</li>
<li>the variable <code>X</code>: function that negates the bits of the input</li>
</ul>
<p>(The rotation operations are interpreted as shifts when arbitrary precision is used.)</p>
<p>Each block of the input is replaced by the value of this expression. Additionally, it is possible to
specify prologue and epilogue expressions which are used to update the state variable <code>S</code> before and
after the update of each block, respectively.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/blockwise/alu.py#L27-L184" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class alu(ArithmeticUnit):
    &#34;&#34;&#34;
    The arithmetic-logical unit. It allows you to specify a custom Python expression where the following
    variables are allowed:

    - the variable `A`: same as `V[0]`
    - the variable `B`: current block
    - the variable `E`: block value of encoded input (not changed after update)
    - the variable `N`: number of bytes in the input
    - the variable `K`: current index in the input
    - the variable `S`: the internal state value
    - the variable `V`: the vector of arguments
    - the variable `I`: function that casts to a signed int in current precision
    - the variable `U`: function that casts to unsigned int in current precision
    - the variable `R`: function; `R(x,4)` rotates x by 4 to the right
    - the variable `L`: function; `L(x,4)` rotates x by 4 to the left
    - the variable `M`: function; `M(x,8)` picks the lower 8 bits of x
    - the variable `X`: function that negates the bits of the input

    (The rotation operations are interpreted as shifts when arbitrary precision is used.)

    Each block of the input is replaced by the value of this expression. Additionally, it is possible to
    specify prologue and epilogue expressions which are used to update the state variable `S` before and
    after the update of each block, respectively.
    &#34;&#34;&#34;

    @staticmethod
    def _parse_op(definition, default=None):
        definition = definition or default
        if not definition:
            raise ValueError(&#39;No definition given&#39;)
        return definition

    def __init__(
        self, operator: Arg(type=str, help=&#39;A Python expression defining the operation.&#39;), *argument,
        seed: Arg(&#39;-s&#39;, type=str, help=(
            &#39;Optional seed value for the state variable S. The default is zero. This can be an expression &#39;
            &#39;involving the variable N.&#39;)) = 0,
        prologue: Arg(&#39;-p&#39;, type=str, metavar=&#39;E&#39;, help=(
            &#39;Optional expression with which the state variable S is updated before a block is operated on.&#39;)) = None,
        epilogue: Arg(&#39;-e&#39;, type=str, metavar=&#39;E&#39;, group=&#39;EPI&#39;, help=(
            &#39;Optional expression with which the state variable S is updated after a block was operated on.&#39;)) = None,
        inc: Arg(&#39;-I&#39;, group=&#39;EPI&#39;, help=&#39;equivalent to --epilogue=S+1&#39;) = False,
        dec: Arg(&#39;-D&#39;, group=&#39;EPI&#39;, help=&#39;equivalent to --epilogue=S-1&#39;) = False,
        cbc: Arg(&#39;-X&#39;, group=&#39;EPI&#39;, help=&#39;equivalent to --epilogue=(B)&#39;) = False,
        bigendian=False, blocksize=None, precision=None
    ):
        for flag, flag_is_set, expression in [
            (&#39;--cbc&#39;, cbc, &#39;(B)&#39;),
            (&#39;--inc&#39;, inc, &#39;S+1&#39;),
            (&#39;--dec&#39;, dec, &#39;S-1&#39;),
        ]:
            if flag_is_set:
                if epilogue is not None:
                    raise ValueError(
                        F&#39;Ambiguous specification; epilogue was already set to {epilogue} &#39;
                        F&#39;when {flag} was parsed.&#39;
                    )
                epilogue = expression

        self._index = IndexCounter()

        super().__init__(
            self._index,
            *argument,
            bigendian=bigendian,
            blocksize=blocksize,
            precision=precision,
            seed=seed,
            operator=self._parse_op(operator),
            prologue=self._parse_op(prologue, &#39;S&#39;),
            epilogue=self._parse_op(epilogue, &#39;S&#39;),
        )

    @property
    def _is_ecb(self):
        return not self.args.epilogue and not self.args.prologue

    def _fastblock(self, _):
        raise FastBlockError

    def process(self, data):
        context = dict(metavars(data))
        seed = self.args.seed
        fbits = self.fbits
        fmask = self.fmask
        if isinstance(seed, str):
            seed = PythonExpression(seed, &#39;N&#39;, constants=metavars(data), mask=fmask)
        if callable(seed):
            seed = seed(context, N=len(data))
        self._index.init(self.fmask)

        def _expression(definition: str):
            return PythonExpression(definition, *&#39;IBEASMNVRLX&#39;, all_variables_allowed=True, mask=fmask)

        prologue = _expression(self.args.prologue).expression
        epilogue = _expression(self.args.epilogue).expression
        operator = _expression(self.args.operator).expression

        def cast_unsigned(n) -&gt; int:
            return int(n) &amp; fmask

        def cast_signed(n) -&gt; int:
            n = int(n) &amp; fmask
            if n &gt;&gt; (fbits - 1):
                return -((~n + 1) &amp; fmask)
            else:
                return n

        if fbits is INF:
            def rotate_r(n, k): return n &gt;&gt; k
            def rotate_l(n, k): return n &lt;&lt; k
        else:
            def rotate_r(n, k): return (n &gt;&gt; k) | (n &lt;&lt; (fbits - k)) &amp; fmask
            def rotate_l(n, k): return (n &lt;&lt; k) | (n &gt;&gt; (fbits - k)) &amp; fmask

        def negate_bits(n):
            return n ^ fmask

        def mask_to_bits(x, b):
            return x &amp; ((1 &lt;&lt; b) - 1)

        context.update(
            N=len(data),
            S=seed,
            I=cast_signed,
            U=cast_unsigned,
            R=rotate_r,
            L=rotate_l,
            X=negate_bits,
            M=mask_to_bits,
        )

        def operate(block, index, *args):
            context.update(K=index, B=block, E=block, V=args)
            if args:
                context[&#39;A&#39;] = args[0]
            context[&#39;S&#39;] = eval(prologue, None, context)
            context[&#39;B&#39;] = eval(operator, None, context)
            context[&#39;S&#39;] = eval(epilogue, None, context)
            return context[&#39;B&#39;]

        placeholder = self.operate
        self.operate = operate

        try:
            result = super().process(data)
        finally:
            self.operate = placeholder

        return result

    @staticmethod
    def operate(block, index, *args):
        raise RuntimeError(&#39;This operate method cannot be called.&#39;)

    def inplace(self, block, *args) -&gt; None:
        super().inplace(block, *args)</code></pre>
</details>
</dd>
<dt id="refinery.shell.aplib"><code class="flex name class">
<span>class <span class="ident">aplib</span></span>
</code></dt>
<dd>
<section class="desc"><p>APLib compression and decompression.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/compression/ap.py#L281-L304" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class aplib(Unit):
    &#34;&#34;&#34;
    APLib compression and decompression.
    &#34;&#34;&#34;

    def reverse(self, buf):
        return compressor(buf).compress()

    def process(self, buf):
        view = memoryview(buf)
        size = 0
        if view[:4] == B&#39;AP32&#39;:
            size = int.from_bytes(buf[4:8], &#39;little&#39;)
            if size &gt; 0x80:
                size = 0
            else:
                self.log_info(F&#39;detected aPLib header of size {size}&#39;)
        return decompressor(view[size:]).decompress()

    @classmethod
    def handles(self, data: bytearray):
        if data[:4] == B&#39;AP32&#39;:
            return True
        return None</code></pre>
</details>
</dd>
<dt id="refinery.shell.asm"><code class="flex name class">
<span>class <span class="ident">asm</span></span>
<span>(</span><span>mode='x32', *, count=None, until=None, no_address=False, no_hexdump=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Disassembles the input data using capstone and produces a human-readable disassembly listing.
It internally uses the <code><a title="refinery.opc" href="index.html#refinery.opc">opc</a></code> unit for this, which is an alternative option if you are
looking for more programmatic disassembly.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/sinks/asm.py#L8-L63" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class asm(opc):
    &#34;&#34;&#34;
    Disassembles the input data using capstone and produces a human-readable disassembly listing.
    It internally uses the `refinery.opc` unit for this, which is an alternative option if you are
    looking for more programmatic disassembly.
    &#34;&#34;&#34;
    def __init__(
        self, mode=&#39;x32&#39;, *, count=None, until=None,
        no_address: Arg.Switch(&#39;-A&#39;, help=&#39;Disable address display.&#39;) = False,
        no_hexdump: Arg.Switch(&#39;-H&#39;, help=&#39;Disable opcodes hexdump.&#39;) = False,
    ):
        super().__init__(
            mode=mode,
            nvar=&#39;_name&#39;,
            avar=&#39;_addr&#39;,
            ovar=&#39;_arg&#39;,
            count=count,
            until=until,
            no_address=no_address,
            no_hexdump=no_hexdump,
        )

    def process(self, data):
        insns = list(super().process(data))
        if not insns:
            return

        no_address = self.args.no_address
        no_hexdump = self.args.no_hexdump

        def _hl(x): return len(hex(x))

        args_width = max(len(insn[&#39;_args&#39;]) for insn in insns)
        memo_width = max(len(insn[&#39;_name&#39;]) for insn in insns)
        addr_width = max(_hl(insn[&#39;_addr&#39;]) for insn in insns)

        if no_address:
            addr_width = 0
            memo_width = memo_width + 2

        max_data_bytes_count = max(len(c) for c in insns)

        padding = addr_width + memo_width + args_width + 10
        metrics_opc = HexDumpMetrics(max_data_bytes_count, padding=padding)

        for insn in insns:
            hd = one(hexdump(insn, metrics_opc))
            name = insn.meta.pop(&#39;_name&#39;)
            args = insn.meta.pop(&#39;_args&#39;)
            addr = insn.meta.pop(&#39;_addr&#39;)
            msg = F&#39; {name:&lt;{memo_width}}  {args:&lt;{args_width}}&#39;
            if not no_hexdump:
                msg = F&#39;{msg}  ; {hd}&#39;
            if not no_address:
                msg = F&#39;{addr:0{addr_width}X}: {msg}&#39;
            yield msg.encode(self.codec)</code></pre>
</details>
</dd>
<dt id="refinery.shell.atbash"><code class="flex name class">
<span>class <span class="ident">atbash</span></span>
</code></dt>
<dd>
<section class="desc"><p><a href="https://en.wikipedia.org/wiki/Atbash">https://en.wikipedia.org/wiki/Atbash</a>
Atbash encoding and decoding. Fairly useless in the 21st century, except
for picking out crypto nerds.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/encoding/atbash.py#L6-L25" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class atbash(Unit):
    &#34;&#34;&#34;
    https://en.wikipedia.org/wiki/Atbash
    Atbash encoding and decoding. Fairly useless in the 21st century, except
    for picking out crypto nerds.
    &#34;&#34;&#34;

    def process(self, data: bytearray):
        uc = range(B&#39;A&#39;[0], B&#39;Z&#39;[0] + 1)
        lc = range(B&#39;a&#39;[0], B&#39;z&#39;[0] + 1)
        for k, letter in enumerate(data):
            if letter in uc:
                data[k] = uc[~uc.index(letter)]
                continue
            if letter in lc:
                data[k] = lc[~lc.index(letter)]
                continue
        return data

    reverse = process</code></pre>
</details>
</dd>
<dt id="refinery.shell.autoxor"><code class="flex name class">
<span>class <span class="ident">autoxor</span></span>
<span>(</span><span>range=slice(1, 32, None), no_alph=False, no_crib=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Assumes a XOR-encoded input and automatically attempts to find the correct XOR key.</p>
<p>The unit expects encrypted input which was encrypted byte-wise with a polyalphabetic key. It
attempts do determine this key by three methods:</p>
<ol>
<li>Known plaintext cribs: The unit contains a library of file signatures that are expected to
occur at specific offsets. It uses these to attempt a known-plaintext attack against the
input. If a key is found that is at most half the size of such a crib, it is returned.</li>
<li>Known alphabets: For each given key length, the input is split into slices that would have
been encrypted with a single byte for keys of that length. Each such slice undergoes a
character frequency analysis. If the histogram indicates that an alphabet of a small size
was used (i.e. base64), then the unit attempts to determine the key based on this.</li>
<li>Known high frequency glyph: Works if the plaintext contains one letter that occurs with
very high frequency, i.e. zero padding in PE or ELF files, and the space character in text.
Based on this assumption, the unit computes the most likely key. This method will work best
on uncompressed files that were encrypted with a short key.</li>
</ol></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/misc/autoxor.py#L9-L23" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class autoxor(xkey, extend_docs=True):
    &#34;&#34;&#34;
    Assumes a XOR-encoded input and automatically attempts to find the correct XOR key.
    &#34;&#34;&#34;
    def process(self, data: bytearray):
        key = super().process(data)
        if not key:
            self.log_warn(&#39;No key was found; returning original data.&#39;)
            return data
        bin, = data | xor(key)
        txt, = bin | xor(0x20)
        if re.fullmatch(BR&#39;[\s!-~]+&#39;, txt) and not txt.isspace():
            key = bytes(key | xor(0x20))
            bin = txt
        return self.labelled(bin, key=key)</code></pre>
</details>
</dd>
<dt id="refinery.shell.b32"><code class="flex name class">
<span>class <span class="ident">b32</span></span>
</code></dt>
<dd>
<section class="desc"><p>Base32 encoding and decoding.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/encoding/b32.py#L8-L28" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class b32(Unit):
    &#34;&#34;&#34;
    Base32 encoding and decoding.
    &#34;&#34;&#34;
    def reverse(self, data):
        return base64.b32encode(data)

    def process(self, data: bytearray):
        before_padding = 0
        for before_padding in range(len(data), 0, -1):
            if data[before_padding - 1:before_padding] != B&#39;=&#39;:
                break
        padding_size = -before_padding % 8
        missing = before_padding + padding_size - len(data)
        if missing &gt; 0:
            self.log_info(F&#39;detected incorrect padding: added {missing} padding characters&#39;)
            data.extend(B&#39;=&#39; * missing)
        if missing &lt; 0:
            self.log_info(F&#39;detected incorrect padding: removed {-missing} padding characters&#39;)
            data[padding_size + before_padding:] = []
        return base64.b32decode(data, casefold=True)</code></pre>
</details>
</dd>
<dt id="refinery.shell.b58"><code class="flex name class">
<span>class <span class="ident">b58</span></span>
</code></dt>
<dd>
<section class="desc"><p>Base58 encoding and decoding. It is famously used as an encoding in Bitcoin addresses
because the alphabet omits digits and letters that look similar.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/encoding/b58.py#L6-L12" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class b58(base):
    &#34;&#34;&#34;
    Base58 encoding and decoding. It is famously used as an encoding in Bitcoin addresses
    because the alphabet omits digits and letters that look similar.
    &#34;&#34;&#34;
    def __init__(self):
        super().__init__(b&#39;123456789ABCDEFGHJKLMNPQRSTUVWXYZabcdefghijkmnopqrstuvwxyz&#39;)</code></pre>
</details>
</dd>
<dt id="refinery.shell.b62"><code class="flex name class">
<span>class <span class="ident">b62</span></span>
</code></dt>
<dd>
<section class="desc"><p>Base62 encoding and decoding.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/encoding/b62.py#L6-L11" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class b62(base):
    &#34;&#34;&#34;
    Base62 encoding and decoding.
    &#34;&#34;&#34;
    def __init__(self):
        super().__init__(b&#39;0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz&#39;)</code></pre>
</details>
</dd>
<dt id="refinery.shell.b64"><code class="flex name class">
<span>class <span class="ident">b64</span></span>
<span>(</span><span>urlsafe=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Base64 encoding and decoding.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/encoding/b64.py#L8-L62" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class b64(Unit):
    &#34;&#34;&#34;
    Base64 encoding and decoding.
    &#34;&#34;&#34;
    def __init__(self, urlsafe: Arg.Switch(&#39;-u&#39;, help=&#39;use URL-safe alphabet&#39;) = False):
        super().__init__(urlsafe=urlsafe)

    def reverse(self, data):
        altchars = None
        if self.args.urlsafe:
            altchars = B&#39;-_&#39;
        return base64.b64encode(data, altchars=altchars)

    def process(self, data: bytearray):
        if not data:
            return data
        if len(data) == 1:
            raise ValueError(&#39;single byte can not be base64-decoded.&#39;)
        data.extend(B&#39;===&#39;)
        altchars = None
        if (B&#39;-&#39; in data or B&#39;_&#39; in data) and (B&#39;+&#39; not in data and B&#39;/&#39; not in data) or self.args.urlsafe:
            altchars = B&#39;-_&#39;
        return base64.b64decode(data, altchars=altchars)

    @classmethod
    def handles(self, data: bytearray) -&gt; bool:
        from refinery.lib.patterns import formats
        if not formats.spaced_b64.value.fullmatch(data):
            return False
        histogram = set()
        lcase_count = 0
        ucase_count = 0
        digit_count = 0
        other_count = 0
        total_count = len(data)
        for byte in data:
            histogram.add(byte)
            if len(histogram) &gt; 60:
                return True
            elif byte in range(0x61, 0x7B):
                lcase_count += 1
            elif byte in range(0x41, 0x5B):
                ucase_count += 1
            elif byte in range(0x30, 0x40):
                digit_count += 1
            elif byte in B&#39;\v\f\t\r\n\x20&#39;:
                total_count -= 1
            else:
                other_count += 1
        for c in (lcase_count, ucase_count, digit_count, other_count):
            # Call this a false positive if more than 2/3ds of the data
            # consist of a single category of letters.
            if c * 3 &gt; total_count * 2:
                return False
        return True</code></pre>
</details>
</dd>
<dt id="refinery.shell.b65536"><code class="flex name class">
<span>class <span class="ident">b65536</span></span>
</code></dt>
<dd>
<section class="desc"><p>Base65536 encoding and decoding.
A relatively esoteric encoding scheme utilizing the UTF-16 / UTF-32 character set.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/encoding/b65536.py#L99-L137" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class b65536(Unit):
    &#34;&#34;&#34;
    Base65536 encoding and decoding.
    A relatively esoteric encoding scheme utilizing the UTF-16 / UTF-32 character set.
    &#34;&#34;&#34;
    def reverse(self, data):
        if not data:
            return B&#39;&#39;

        output = MemoryFile()
        length = len(data)
        for x in range(0, length, 2):
            b1 = data[x]
            b2 = data[x + 1] if x + 1 &lt; length else -1
            code_point = _BLOCK_START[b2] + b1
            output.write(chr(code_point).encode())
        return output.getvalue()

    def process(self, data):
        if not data:
            return B&#39;&#39;

        done = False
        output = MemoryFile()
        for ch in data.decode():
            code_point = ord(ch)
            b1 = code_point &amp; ((1 &lt;&lt; 8) - 1)
            try:
                b2 = _B2[code_point - b1]
            except KeyError:
                self.log_info(&#39;Invalid base65536 code point: %d, skipping&#39; % code_point)
                continue
            b = b1.to_bytes(1, &#34;little&#34;) if b2 == -1 else b1.to_bytes(1, &#34;little&#34;) + b2.to_bytes(1, &#34;little&#34;)
            if len(b) == 1:
                if done:
                    raise ValueError(&#39;base65536 sequence continued after final byte&#39;)
                done = True
            output.write(b)
        return output.getvalue()</code></pre>
</details>
</dd>
<dt id="refinery.shell.b85"><code class="flex name class">
<span>class <span class="ident">b85</span></span>
</code></dt>
<dd>
<section class="desc"><p>Base85 encoding and decoding.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/encoding/b85.py#L9-L24" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class b85(Unit):
    &#34;&#34;&#34;
    Base85 encoding and decoding.
    &#34;&#34;&#34;
    def reverse(self, data):
        return base64.b85encode(data)

    def process(self, data):
        if re.search(BR&#39;\s&#39;, data) is not None:
            data = re.sub(BR&#39;\s+&#39;, B&#39;&#39;, data)
        return base64.b85decode(data)

    @classmethod
    def handles(self, data: bytearray):
        from refinery.lib.patterns import formats
        return formats.spaced_b85.value.fullmatch(data)</code></pre>
</details>
</dd>
<dt id="refinery.shell.b92"><code class="flex name class">
<span>class <span class="ident">b92</span></span>
</code></dt>
<dd>
<section class="desc"><p>Base92 encoding and decoding.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/encoding/b92.py#L14-L106" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class b92(Unit):
    &#34;&#34;&#34;
    Base92 encoding and decoding.
    &#34;&#34;&#34;
    def reverse(self, data):
        if not data:
            return B&#39;~&#39;

        reader = StructReader(data, bigendian=True)
        output = MemoryFile()
        while reader.remaining_bits &gt; 0:
            try:
                block = reader.read_integer(13)
            except EOFError:
                count = reader.remaining_bits
                block = reader.read_integer(count)
                self.log_debug(F&#39;reading {count} remaining bits: {block:0{count}b}&#39;)
                shift = 6 - count
                if shift &gt;= 0:
                    block &lt;&lt;= shift
                    self.log_debug(F&#39;encoding block: {block:06b}&#39;)
                    output.write_byte(_B92_ALPHABET[block])
                    break
                block &lt;&lt;= 13 - count
            self.log_debug(F&#39;encoding block: {block:013b}&#39;)
            hi, lo = divmod(block, 91)
            output.write_byte(_B92_ALPHABET[hi])
            output.write_byte(_B92_ALPHABET[lo])
        return output.getvalue()

    def process(self, data):
        if data == B&#39;~&#39;:
            return B&#39;&#39;

        output = MemoryFile()
        buffer = 0
        length = 0

        view = memoryview(data)
        q, r = divmod(len(view), 2)

        if r &gt; 0:
            bits = 6
            tail = _B92_DECODING[data[~0]]
        else:
            bits = 13
            tail = _B92_DECODING[data[~1]] * 91 + _B92_DECODING[data[~0]]
            view = view[:(q - 1) * 2]

        it = iter(view)

        for a, b in zip(it, it):
            block = _B92_DECODING[a] * 91 + _B92_DECODING[b]
            assert length &lt; 8
            buffer &lt;&lt;= 13
            buffer |= block
            length += 13
            size, length = divmod(length, 8)
            assert size &gt; 0
            output.write((buffer &gt;&gt; length).to_bytes(size, &#39;big&#39;))
            buffer &amp;= (1 &lt;&lt; length) - 1

        missing = 8 - length
        shift = bits - missing

        if shift &lt; 8:
            bytecount = 1
        else:
            bytecount = 2
            shift -= 8
            missing += 8

        if shift &lt; 0:
            raise RefineryPartialResult(
                F&#39;Invalid padding, missing {-shift} bits.&#39;,
                output.getvalue())

        buffer &lt;&lt;= missing
        buffer |= tail &gt;&gt; shift
        length += missing
        output.write(buffer.to_bytes(bytecount, &#39;big&#39;))

        if tail &amp; ((1 &lt;&lt; shift) - 1) != 0:
            raise RefineryPartialResult(
                F&#39;Invalid padding, lower {shift} bits of {tail:0{bits}b} are not zero.&#39;,
                output.getvalue())

        return output.getvalue()

    @classmethod
    def handles(self, data: bytearray):
        from refinery.lib.patterns import formats
        return formats.b92.value.fullmatch(data)</code></pre>
</details>
</dd>
<dt id="refinery.shell.base"><code class="flex name class">
<span>class <span class="ident">base</span></span>
<span>(</span><span>base=0, strip_padding=False, little_endian=False, strict_digits=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Encodes and decodes integers in arbitrary base.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/encoding/base.py#L19-L138" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class base(Unit):
    &#34;&#34;&#34;
    Encodes and decodes integers in arbitrary base.
    &#34;&#34;&#34;
    def __init__(
        self,
        base: Arg(type=numseq, metavar=&#39;base|alphabet&#39;, help=(
            R&#39;Either the base to be used or an alphabet. If an explicit alphabet is given, its length &#39;
            R&#39;determines the base. The default base 0 treats the input as a Python integer literal. If &#39;
            F&#39;a numeric base is given, digits from the alphabet &#34;{_DEFAULT_ALPH_STR}&#34; are used. &#39;)) = 0,
        strip_padding: Arg.Switch(&#39;-s&#39;, help=&#39;Do not add leading zeros to the output.&#39;) = False,
        little_endian: Arg.Switch(&#39;-e&#39;, help=&#39;Use little endian byte order instead of big endian.&#39;) = False,
        strict_digits: Arg.Switch(&#39;-d&#39;, help=&#39;Check that all input digits are part of the alphabet.&#39;) = False,
    ):
        super().__init__(
            base=base,
            strip_padding=strip_padding,
            little_endian=little_endian,
            strict_digits=strict_digits,
        )

    @property
    def _args(self):
        base = self.args.base
        if isinstance(base, int):
            if not base:
                return 0, B&#39;&#39;
            if base in _LARGER_ALPHABETS:
                return base, _LARGER_ALPHABETS[base]
            if base not in range(2, len(_DEFAULT_ALPHABET) + 1):
                raise ValueError(F&#39;base may only be an integer between 2 and {len(_DEFAULT_ALPHABET)}&#39;)
            return base, _DEFAULT_ALPHABET[:base]
        if len(set(base)) != len(base):
            raise ValueError(&#39;the given alphabet contains duplicate letters&#39;)
        return len(base), bytearray(base)

    @property
    def byteorder(self):
        return &#39;little&#39; if self.args.little_endian else &#39;big&#39;

    def reverse(self, data):
        base, alphabet = self._args
        self.log_info(&#39;using byte order&#39;, self.byteorder)
        number = int.from_bytes(data, byteorder=self.byteorder)

        if base == 0:
            return B&#39;0x%X&#39; % number
        if base &gt; len(alphabet):
            raise ValueError(F&#39;Only {len(alphabet)} available; not enough to encode base {base}&#39;)

        data_bits = len(data) * 8
        base_bits = math.log2(base)
        result = bytearray()

        while data_bits &gt;= 1:
            number, k = divmod(number, base)
            result.append(alphabet[k])
            if not number and self.args.strip_padding:
                break
            data_bits -= base_bits

        result.reverse()
        return result

    def process(self, data: bytearray):
        base, alphabet = self._args
        be_lenient = not self.args.strict_digits
        if be_lenient and alphabet.upper() == alphabet:
            lcased = (c + 0x20 if 0x41 &lt;= c &lt;= 0x5a else c for c in data)
            if all(x == y for x, y in zip(data, lcased)):
                data = data.upper()
        if base and base != 64 and be_lenient:
            check = set(alphabet)
            index = 0
            it = iter(data)
            for b in it:
                if b not in check:
                    break
                index += 1
            for b in it:
                if b in check:
                    data[index] = b
                    index += 1
            self.log_info(F&#39;stripped {len(data) - index} invalid digits from input data&#39;)
            del data[index:]
        if len(alphabet) &lt;= len(_DEFAULT_ALPHABET):
            defaults = _DEFAULT_ALPHABET[:base]
            if alphabet != defaults:
                self.log_info(&#39;translating input data to a default alphabet for faster conversion&#39;)
                data_translated = data.translate(bytes.maketrans(alphabet, defaults))
                result = int(data_translated, base)
            else:
                result = int(data, base)
        elif len(alphabet) == 64:
            import base64
            _b64_alphabet = _LARGER_ALPHABETS[64]
            if alphabet != _b64_alphabet:
                data = data.translate(bytes.maketrans(alphabet, _b64_alphabet))
            return base64.b64decode(data + b&#39;===&#39;, validate=self.args.strict_digits)
        elif len(alphabet) == 85:
            import base64
            _b85_alphabet = _LARGER_ALPHABETS[85]
            if alphabet != _b85_alphabet:
                data = data.translate(bytes.maketrans(alphabet, _b85_alphabet))
            return base64.b85decode(data)
        else:
            self.log_warn(&#39;very long alphabet, unable to use built-ins; reverting to (slow) fallback.&#39;)
            result = 0
            lookup = {digit: k for k, digit in enumerate(alphabet)}
            for digit in data:
                result *= base
                result += lookup[digit]
        if not base or self.args.strip_padding:
            bits = result.bit_length()
        else:
            bits = (len(data) - 1) * math.log2(base) + math.log2(max(alphabet.index(data[0]), 1))
            bits = math.ceil(bits)
        size, rest = divmod(bits, 8)
        size += int(bool(rest))
        return result.to_bytes(size, byteorder=self.byteorder)</code></pre>
</details>
</dd>
<dt id="refinery.shell.bat"><code class="flex name class">
<span>class <span class="ident">bat</span></span>
<span>(</span><span>keep_all=False, keep_comment=False, keep_definitions=False, keep_echo=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Deobfuscates batch files, based on the batch deobfuscator by DissectMalware. The input script
is interpreted, variables are substituted for previously defined values, including commonly
defined operating system environment variables. Variable definitions that are later evaluated
are removed from the script, as are all echo commands and comments.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/formats/bat.py#L10-L36" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class bat(Unit):
    &#34;&#34;&#34;
    Deobfuscates batch files, based on the batch deobfuscator by DissectMalware. The input script
    is interpreted, variables are substituted for previously defined values, including commonly
    defined operating system environment variables. Variable definitions that are later evaluated
    are removed from the script, as are all echo commands and comments.
    &#34;&#34;&#34;
    def __init__(
        self,
        keep_all         : Unit.Arg.Switch(&#39;-a&#39;, help=&#39;Do not strip anything after deobfuscation.&#39;) = False,
        keep_comment     : Unit.Arg.Switch(&#39;-c&#39;, help=&#39;Do not strip comments from the script.&#39;) = False,
        keep_definitions : Unit.Arg.Switch(&#39;-d&#39;, help=&#39;Do not strip variable definitions.&#39;) = False,
        keep_echo        : Unit.Arg.Switch(&#39;-e&#39;, help=&#39;Do not strip echo calls in the script.&#39;) = False,
    ): ...

    @unicoded
    def process(self, data: str) -&gt; str:
        mode = STRIP.ALL
        if self.args.keep_all:
            mode = STRIP.NONE
        elif self.args.keep_comment:
            mode ^= STRIP.COMMENT
        elif self.args.keep_definitions:
            mode ^= STRIP.DEFINITION
        elif self.args.keep_echo:
            mode ^= STRIP.ECHO
        return BatchDeobfuscator().deobfuscate(data, mode)</code></pre>
</details>
</dd>
<dt id="refinery.shell.bitrev"><code class="flex name class">
<span>class <span class="ident">bitrev</span></span>
<span>(</span><span>bigendian=False, blocksize=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Reverse the bits of every block. Any excess bytes at the end of the input that are not
an integer multiple of the block size are ignored.</p>
<p>Unreadable bit reversal operations due to:
<a href="https://graphics.stanford.edu/~seander/bithacks.html#ReverseByteWith64BitsDiv">https://graphics.stanford.edu/~seander/bithacks.html#ReverseByteWith64BitsDiv</a>
<a href="https://graphics.stanford.edu/~seander/bithacks.html#ReverseParallel">https://graphics.stanford.edu/~seander/bithacks.html#ReverseParallel</a></p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/blockwise/bitrev.py#L6-L42" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class bitrev(UnaryOperation):
    &#34;&#34;&#34;
    Reverse the bits of every block. Any excess bytes at the end of the input that are not
    an integer multiple of the block size are ignored.
    &#34;&#34;&#34;
    @staticmethod
    def operate(arg):
        raise RuntimeError(&#39;operate was called before the unit was initialized&#39;)

    def __init__(self, bigendian=False, blocksize=None):
        &#34;&#34;&#34;
        Unreadable bit reversal operations due to:
        https://graphics.stanford.edu/~seander/bithacks.html#ReverseByteWith64BitsDiv
        https://graphics.stanford.edu/~seander/bithacks.html#ReverseParallel
        &#34;&#34;&#34;
        super().__init__(bigendian=bigendian, blocksize=blocksize, _truncate=1)

        if self.bytestream:
            def operate(v):
                return ((v * 0x202020202) &amp; 0x10884422010) % 1023
        elif self.blocksize in (2, 4, 8):
            def operate(v):
                s = self.fbits
                m = self.fmask
                w = v
                while s &gt; 1:
                    s &gt;&gt;= 1
                    m = m ^ (m &lt;&lt; s)
                    w = ((w &lt;&lt; s) &amp; ~m) | ((w &gt;&gt; s) &amp; m)
                return w
        else:
            def operate(v):
                w = v &amp; 0
                for s in range(self.fbits):
                    w |= ((v &gt;&gt; s) &amp; 1) &lt;&lt; (self.fbits - s - 1)
                return w
        self.operate = operate</code></pre>
</details>
</dd>
<dt id="refinery.shell.bitsnip"><code class="flex name class">
<span>class <span class="ident">bitsnip</span></span>
<span>(</span><span>slices=[slice(0, 1, None)], bigendian=False, blocksize=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Pick a certain range of bits from each block of the input. The extracted ranges of bits are
concatenated. Leftover bits that do not form at least one full byte are discarded. Bits are
indexed from least significant at index 0 to most significant in each block. When the unit
operates in big endian mode, the internal bit buffer is shifted left in each step and new bits
are inserted as the least significant portion. Conversely, in default (little endian) mode,
newly extracted bits are added as the now most significant ones. After concatenating all bit
slices into a large integer, this integer is converted into a byte string according to the
given byte ordering.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/blockwise/bitsnip.py#L7-L74" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class bitsnip(BlockTransformationBase):
    &#34;&#34;&#34;
    Pick a certain range of bits from each block of the input. The extracted ranges of bits are
    concatenated. Leftover bits that do not form at least one full byte are discarded. Bits are
    indexed from least significant at index 0 to most significant in each block. When the unit
    operates in big endian mode, the internal bit buffer is shifted left in each step and new bits
    are inserted as the least significant portion. Conversely, in default (little endian) mode,
    newly extracted bits are added as the now most significant ones. After concatenating all bit
    slices into a large integer, this integer is converted into a byte string according to the
    given byte ordering.
    &#34;&#34;&#34;
    def __init__(
        self, slices: Arg(help=(
            &#39;Specify start:stop:size, where size can be used to pad or truncate the extracted &#39;
            &#39;bits. If size is omitted, it defaults to (stop-start). If no slice is specified, &#39;
            &#39;it defaults to 0, which corresponds to 0:1:1, i.e. extracting the lowest bit.&#39;)
        ) = [slice(0, 1)],
        bigendian=False, blocksize=None
    ):
        super().__init__(slices=slices, bigendian=bigendian, blocksize=blocksize)

    def process(self, data: bytearray):
        bitsnip_data = 0
        bitsnip_size = 0
        slices: List[Tuple[int, int, int]] = []
        maxbits = 8 * self.blocksize
        args: Iterable[slice] = iter(self.args.slices)
        bigendian: bool = self.args.bigendian

        for s in args:
            start = s.start
            stop = s.stop
            if start is None:
                start = 0
            if stop is None:
                stop = maxbits
            elif stop &gt; maxbits:
                raise ValueError(F&#39;the selection {start}:{stop} is out of bounds for the block size {self.blocksize}&#39;)
            if start &gt;= stop:
                continue
            size = stop - start
            mask = (1 &lt;&lt; size) - 1
            size = s.step or size
            slices.append((start, mask, size))

        for item in self.chunk(data):
            for shift, mask, size in slices:
                bits = (item &gt;&gt; shift) &amp; mask
                if bigendian:
                    bitsnip_data &lt;&lt;= size
                    bitsnip_data |= bits
                else:
                    bitsnip_data |= bits &lt;&lt; bitsnip_size
                bitsnip_size += size

        length, remainder = divmod(bitsnip_size, 8)

        if remainder != 0:
            self.log_info(F&#39;discarding {bitsnip_size % 8} bits&#39;)
            if bigendian:
                bitsnip_data &gt;&gt;= remainder
            else:
                bitsnip_data &amp;= (1 &lt;&lt; (8 * length)) - 1

        if bigendian:
            return bitsnip_data.to_bytes(length, &#39;big&#39;)
        else:
            return bitsnip_data.to_bytes(length, &#39;little&#39;)</code></pre>
</details>
</dd>
<dt id="refinery.shell.blabla"><code class="flex name class">
<span>class <span class="ident">blabla</span></span>
<span>(</span><span>key, nonce=b'\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00', rounds=10, discard=0, stateful=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Implements the BlaBla cipher, a 256-bit stream cipher designed by Jean-Philippe Aumasson. It
is similar to ChaCha in design but operates on 64-bit blocks.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/crypto/cipher/blabla.py#L12-L74" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class blabla(StreamCipherUnit):
    &#34;&#34;&#34;
    Implements the BlaBla cipher, a 256-bit stream cipher designed by Jean-Philippe Aumasson. It
    is similar to ChaCha in design but operates on 64-bit blocks.
    &#34;&#34;&#34;
    key_size = {32}

    def __init__(
        self, key,
        nonce: Arg(help=&#39;The 16-byte nonce. The default are 16 null bytes.&#39;) = bytes(16),
        rounds: Arg.Number(&#39;-r&#39;, help=&#39;The number of rounds, default is {default}.&#39;) = 10,
        discard=0, stateful=False
    ):
        super().__init__(key=key, nonce=nonce, rounds=rounds, discard=discard, stateful=stateful)

    def keystream(self):
        r = self.args.rounds
        n = self.args.nonce
        k = struct.unpack(&#39;&lt;4Q&#39;, self.args.key)

        try:
            n = struct.unpack(&#39;&lt;2Q&#39;, n)
        except Exception:
            raise ValueError(F&#39;The given nonce has invalid length of {len(n)}, it must be 16 bytes in size.&#39;)

        q = [
            0x6170786593810fab,  # 0x0
            0x3320646ec7398aee,  # 0x1
            0x79622d3217318274,  # 0x2
            0x6b206574babadada,  # 0x3
            *k,                  # 0x4 .. 0x7
            0x2ae36e593e46ad5f,  # 0x8
            0xb68f143029225fc9,  # 0x9
            0x8da1e08468303aa6,  # 0xA
            0xa48a209acd50a4a7,  # 0xB
            0x7fdc12f23f90778c,  # 0xC
            1,                   # 0xD
            *n                   # 0xE .. 0xF
        ]
        while True:
            v = [*q]
            for _ in range(r):
                for a, b, c, d in [
                    (0x0, 0x4, 0x8, 0xC),
                    (0x1, 0x5, 0x9, 0xD),
                    (0x2, 0x6, 0xA, 0xE),
                    (0x3, 0x7, 0xB, 0xF),
                    (0x0, 0x5, 0xA, 0xF),
                    (0x1, 0x6, 0xB, 0xC),
                    (0x2, 0x7, 0x8, 0xD),
                    (0x3, 0x4, 0x9, 0xE),
                ]:
                    v[a] = v[a] + v[b] &amp; _M64
                    v[d] = rotr64(v[d] ^ v[a], 32)
                    v[c] = v[c] + v[d] &amp; _M64
                    v[b] = rotr64(v[b] ^ v[c], 24)
                    v[a] = v[a] + v[b] &amp; _M64
                    v[d] = rotr64(v[d] ^ v[a], 16)
                    v[c] = v[c] + v[d] &amp; _M64
                    v[b] = rotr64(v[b] ^ v[c], 63)
            v = [x + y &amp; _M64 for x, y in zip(q, v)]
            q[0xD] += 1
            yield from struct.pack(&#39;&lt;16Q&#39;, *v)</code></pre>
</details>
</dd>
<dt id="refinery.shell.blk224"><code class="flex name class">
<span>class <span class="ident">blk224</span></span>
<span>(</span><span>text=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Returns the BLK224 hash of the input data.</p></section>
</dd>
<dt id="refinery.shell.blk256"><code class="flex name class">
<span>class <span class="ident">blk256</span></span>
<span>(</span><span>text=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Returns the BLK256 hash of the input data.</p></section>
</dd>
<dt id="refinery.shell.blk384"><code class="flex name class">
<span>class <span class="ident">blk384</span></span>
<span>(</span><span>text=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Returns the BLK384 hash of the input data.</p></section>
</dd>
<dt id="refinery.shell.blk512"><code class="flex name class">
<span>class <span class="ident">blk512</span></span>
<span>(</span><span>text=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Returns the BLK512 hash of the input data.</p></section>
</dd>
<dt id="refinery.shell.blowfish"><code class="flex name class">
<span>class <span class="ident">blowfish</span></span>
<span>(</span><span>key, iv=b'', *, padding=None, mode=None, raw=False, little_endian=False, segment_size=0, mac_len=0, assoc_len=0)</span>
</code></dt>
<dd>
<section class="desc"><p>Blowfish encryption and decryption.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/crypto/cipher/blowfish.py#L9-L13" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class blowfish(StandardBlockCipherUnit, cipher=PyCryptoFactoryWrapper(Blowfish)):
    &#34;&#34;&#34;
    Blowfish encryption and decryption.
    &#34;&#34;&#34;
    pass</code></pre>
</details>
</dd>
<dt id="refinery.shell.blz"><code class="flex name class">
<span>class <span class="ident">blz</span></span>
</code></dt>
<dd>
<section class="desc"><p>BriefLZ compression and decompression. The compression algorithm uses a pure Python suffix tree
implementation: It requires a lot of time &amp; memory.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/compression/blz.py#L10-L258" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class blz(Unit):
    &#34;&#34;&#34;
    BriefLZ compression and decompression. The compression algorithm uses a pure Python suffix tree
    implementation: It requires a lot of time &amp; memory.
    &#34;&#34;&#34;
    def _begin(self, data):
        self._src = StructReader(memoryview(data))
        self._dst = MemoryFile(bytearray())
        return self

    def _reset(self):
        self._src.seek(0)
        self._dst.seek(0)
        self._dst.truncate()
        return self

    def _decompress(self):
        (
            signature,
            version,
            src_count,
            src_crc32,
            dst_count,
            dst_crc32,
        ) = self._src.read_struct(&#39;&gt;6L&#39;)
        if signature != 0x626C7A1A:
            raise ValueError(F&#39;Invalid BriefLZ signature: {signature:08X}, should be 626C7A1A.&#39;)
        if version &gt; 10:
            raise ValueError(F&#39;Invalid version number {version}, should be less than 10.&#39;)
        self.log_debug(F&#39;signature: 0x{signature:08X} V{version}&#39;)
        self.log_debug(F&#39;src count: 0x{src_count:08X}&#39;)
        self.log_debug(F&#39;src crc32: 0x{src_crc32:08X}&#39;)
        self.log_debug(F&#39;dst count: 0x{dst_count:08X}&#39;)
        self.log_debug(F&#39;dst crc32: 0x{dst_crc32:08X}&#39;)
        src = self._src.getbuffer()
        src = src[24:24 + src_count]
        if len(src) &lt; src_count:
            self.log_warn(F&#39;Only {len(src)} bytes in buffer, but header annoucned a length of {src_count}.&#39;)
        if src_crc32:
            check = zlib.crc32(src)
            if check != src_crc32:
                self.log_warn(F&#39;Invalid source data CRC {check:08X}, should be {src_crc32:08X}.&#39;)
        dst = self._decompress_chunk(dst_count)
        if not dst_crc32:
            return dst
        check = zlib.crc32(dst)
        if check != dst_crc32:
            self.log_warn(F&#39;Invalid result data CRC {check:08X}, should be {dst_crc32:08X}.&#39;)
        return dst

    def _decompress_modded(self):
        self._src.seekrel(8)
        total_size = self._src.u64()
        chunk_size = self._src.u64()
        remaining = total_size
        self.log_debug(F&#39;total size: 0x{total_size:016X}&#39;)
        self.log_debug(F&#39;chunk size: 0x{chunk_size:016X}&#39;)
        while remaining &gt; chunk_size:
            self._decompress_chunk(chunk_size)
            remaining -= chunk_size
        return self._decompress_chunk(remaining)

    def _decompress_chunk(self, size=None):
        bitcount = 0
        bitstore = 0
        decompressed = 1

        def readbit():
            nonlocal bitcount, bitstore
            if not bitcount:
                bitstore = int.from_bytes(self._src.read_exactly(2), &#39;little&#39;)
                bitcount = 0xF
            else:
                bitcount = bitcount - 1
            return (bitstore &gt;&gt; bitcount) &amp; 1

        def readint():
            result = 2 + readbit()
            while readbit():
                result &lt;&lt;= 1
                result += readbit()
            return result

        self._dst.write(self._src.read_exactly(1))

        try:
            while not size or decompressed &lt; size:
                if readbit():
                    length = readint() + 2
                    sector = readint() - 2
                    offset = self._src.read_byte() + 1
                    delta = offset + 0x100 * sector
                    available = self._dst.tell()
                    if delta not in range(available + 1):
                        raise RefineryPartialResult(
                            F&#39;Requested rewind by 0x{delta:08X} bytes with only 0x{available:08X} bytes in output buffer.&#39;,
                            partial=self._dst.getvalue())
                    quotient, remainder = divmod(length, delta)
                    replay = memoryview(self._dst.getbuffer())
                    replay = bytes(replay[-delta:] if quotient else replay[-delta:length - delta])
                    replay = quotient * replay + replay[:remainder]
                    self._dst.write(replay)
                    decompressed += length
                else:
                    self._dst.write(self._src.read_exactly(1))
                    decompressed += 1
        except EOFError as E:
            raise RefineryPartialResult(str(E), partial=self._dst.getbuffer())
        dst = self._dst.getbuffer()
        if decompressed &lt; size:
            raise RefineryPartialResult(
                F&#39;Attempted to decompress {size} bytes, got only {len(dst)}.&#39;, dst)
        if decompressed &gt; size:
            raise RuntimeError(&#39;Decompressed buffer contained more bytes than expected.&#39;)
        return dst

    def _compress(self):
        from refinery.lib.suffixtree import SuffixTree

        try:
            self.log_info(&#39;computing suffix tree&#39;)
            tree = SuffixTree(self._src.getbuffer())
        except Exception:
            raise

        bitstore = 0  # The bit stream to be written
        bitcount = 0  # The number of bits in the bit stream
        buffer = MemoryFile(bytearray())

        # Write empty header and first byte of source
        self._dst.write(bytearray(24))
        self._dst.write(self._src.read_exactly(1))

        def writeint(n: int) -&gt; None:
            &#34;&#34;&#34;
            Write an integer to the bit stream.
            &#34;&#34;&#34;
            nonlocal bitstore, bitcount
            nbits = n.bit_length()
            if nbits &lt; 2:
                raise ValueError
            # The highest bit is implicitly assumed:
            n ^= 1 &lt;&lt; (nbits - 1)
            remaining = nbits - 2
            while remaining:
                remaining -= 1
                bitstore &lt;&lt;= 2
                bitcount += 2
                bitstore |= ((n &gt;&gt; remaining) &amp; 3) | 1
            bitstore &lt;&lt;= 2
            bitcount += 2
            bitstore |= (n &amp; 1) &lt;&lt; 1

        src = self._src.getbuffer()
        remaining = len(src) - 1
        self.log_info(&#39;compressing data&#39;)

        while True:
            cursor = len(src) - remaining
            rest = src[cursor:]
            if bitcount &gt;= 0x10:
                block_count, bitcount = divmod(bitcount, 0x10)
                info_channel = bitstore &gt;&gt; bitcount
                bitstore = info_channel &lt;&lt; bitcount ^ bitstore
                # The decompressor will read bits from top to bottom, and each 16 bit block has to be
                # little-endian encoded. The bit stream is encoded top to bottom bit in the bitstore
                # variable, and by encoding it as a big endian integer, the stream is in the correct
                # order. However, we need to swap adjacent bytes to achieve little endian encoding for
                # each of the blocks:
                info_channel = bytearray(info_channel.to_bytes(block_count * 2, &#39;big&#39;))
                for k in range(block_count):
                    k0 = 2 * k + 0
                    k1 = 2 * k + 1
                    info_channel[k0], info_channel[k1] = info_channel[k1], info_channel[k0]
                info_channel = memoryview(info_channel)
                data_channel = memoryview(buffer.getbuffer())
                self._dst.write(info_channel[:2])
                self._dst.write(data_channel[:-1])
                self._dst.write(info_channel[2:])
                data_channel = bytes(data_channel[-1:])
                buffer.truncate(0)
                store = buffer if bitcount else self._dst
                store.write(data_channel)
            if remaining + bitcount &lt; 0x10:
                buffer = buffer.getbuffer()
                if rest or buffer:
                    bitstore &lt;&lt;= 0x10 - bitcount
                    self._dst.write(bitstore.to_bytes(2, &#39;little&#39;))
                    self._dst.write(buffer)
                    self._dst.write(rest)
                elif bitcount:
                    raise RuntimeError(&#39;Bitbuffer Overflow&#39;)
                break
            node = tree.root
            length = 0
            offset = 0
            sector = None
            while node.children and length &lt; len(rest):
                for child in node.children.values():
                    if tree.data[child.start] == rest[length]:
                        node = child
                        break
                if node.start &gt;= cursor:
                    break
                offset = node.start - length
                length = node.end + 1 - offset
            length = min(remaining, length)
            if length &gt;= 4:
                sector, offset = divmod(cursor - offset - 1, 0x100)
            bitcount += 1
            bitstore &lt;&lt;= 1
            if sector is None:
                buffer.write(rest[:1])
                remaining -= 1
                continue
            bitstore |= 1
            buffer.write(bytes((offset,)))
            writeint(length - 2)
            writeint(sector + 2)
            remaining -= length

        self._dst.seek(24)
        dst = self._dst.peek()
        self._dst.seek(0)
        self._dst.write(struct.pack(&#39;&gt;6L&#39;, 0x626C7A1A, 1, len(dst), zlib.crc32(dst), len(src), zlib.crc32(src)))
        return self._dst.getbuffer()

    def process(self, data):
        self._begin(data)
        partial = None
        try:
            return self._decompress()
        except ValueError as error:
            if isinstance(error, RefineryPartialResult):
                partial = error
            self.log_warn(F&#39;Reverting to modified BriefLZ after decompression error: {error!s}&#39;)
            self._reset()

        try:
            return self._decompress_modded()
        except RefineryPartialResult:
            raise
        except Exception as error:
            if not partial:
                raise
            raise partial from error

    def reverse(self, data):
        return self._begin(data)._compress()</code></pre>
</details>
</dd>
<dt id="refinery.shell.brotli"><code class="flex name class">
<span>class <span class="ident">brotli</span></span>
</code></dt>
<dd>
<section class="desc"><p>Brotli compression and decompression.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/compression/brotli.py#L6-L20" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class brotli(Unit):
    &#34;&#34;&#34;
    Brotli compression and decompression.
    &#34;&#34;&#34;

    @Unit.Requires(&#39;brotlipy&#39;, &#39;all&#39;)
    def _brotli():
        import brotli
        return brotli

    def process(self, data):
        return self._brotli.decompress(bytes(data))

    def reverse(self, data):
        return self._brotli.compress(bytes(data))</code></pre>
</details>
</dd>
<dt id="refinery.shell.bruteforce"><code class="flex name class">
<span>class <span class="ident">bruteforce</span></span>
<span>(</span><span>name, length=slice(1, None, None), format=None, alphabet=None, pattern=None, printable=False, digits=False, identifier=False, letters=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Generates all possible combinations of letters in a given alphabet. For each generated string,
one copy of each input chunk is generated and populated with a meta variable containing that
string. This can be used for simple brute forcing checks.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/strings/bruteforce.py#L11-L91" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class bruteforce(Unit):
    &#34;&#34;&#34;
    Generates all possible combinations of letters in a given alphabet. For each generated string,
    one copy of each input chunk is generated and populated with a meta variable containing that
    string. This can be used for simple brute forcing checks.
    &#34;&#34;&#34;
    def __init__(
        self,
        name  : Arg.String(help=&#39;Name of the meta variable to be populated.&#39;),
        length: Arg.Bounds(metavar=&#39;length&#39;, help=(
            &#39;Specifies the range of characters to brute force, default is {default}.&#39;
        )) = slice(1, None),
        format: Arg.String(help=(
            &#39;Optional format expression for the output string. The format sequence &#34;{0}&#34; is the &#39;
            &#39;current brute force string, the sequence &#34;{1}&#34; represents the input data.&#39;
        )) = None,
        alphabet  : Arg.Binary(&#39;-a&#39;, group=&#39;ALPH&#39;, help=(
            &#39;The alphabet from which to choose the letters. Entire byte range by default.&#39;
        )) = None,
        pattern   : Arg.RegExp(&#39;-r&#39;, group=&#39;ALPH&#39;,
            help=&#39;Provide a regular expression pattern to define the alphabet.&#39;) = None,
        printable : Arg.Switch(&#39;-p&#39;, group=&#39;ALPH&#39;,
            help=&#39;Equivalent to --pattern=[\\s\\x20-\\x7E]&#39;) = False,
        digits    : Arg.Switch(&#39;-d&#39;, group=&#39;ALPH&#39;,
            help=&#39;Equivalent to --pattern=\\d&#39;) = False,
        identifier: Arg.Switch(&#39;-i&#39;, group=&#39;ALPH&#39;,
            help=&#39;Equivalent to --pattern=\\w&#39;) = False,
        letters   : Arg.Switch(&#39;-l&#39;, group=&#39;ALPH&#39;,
            help=&#39;Equivalent to --pattern=[a-zA-Z]&#39;) = False,
    ):
        options = sum(1 for x in [printable, digits, identifier, letters] if x)

        if options &gt; 1 or options and pattern:
            raise ValueError(&#39;Invalid selection.&#39;)

        if printable:
            pattern = b&#39;[\\s\\x20-\\x7E]&#39;
        if digits:
            pattern = b&#39;\\d&#39;
        if identifier:
            pattern = b&#39;\\w&#39;
        if letters:
            pattern = b&#39;[a-zA-Z]&#39;

        super().__init__(
            name=name,
            length=length,
            format=format,
            alphabet=alphabet,
            pattern=pattern,
        )

    def _alphabet(self) -&gt; bytes:
        alphabet = self.args.alphabet
        if alphabet:
            return alphabet
        alphabet = bytes(range(0x100))
        pattern = self.args.pattern
        if not pattern:
            return alphabet
        alphabet = B&#39;&#39;.join(re.findall(pattern, alphabet, flags=re.DOTALL))
        if alphabet:
            return alphabet
        raise ValueError(F&#39;Invalid regular expression: {pattern}&#39;)

    def process(self, data: bytearray):
        format_spec: str = self.args.format
        meta = metavars(data)
        name = self.args.name
        kwargs = {name: None}

        for length in integers_of_slice(self.args.length):
            self.log_info(F&#39;generating {length} digits&#39;)
            if not isinstance(length, int) or length &lt; 0:
                raise ValueError(F&#39;Unable to brute force {length} characters.&#39;)
            for string in itertools.product(self._alphabet(), repeat=length):
                string = bytes(string)
                if format_spec:
                    string = meta.format_bin(format_spec, self.codec, [string, data])
                kwargs[name] = string
                yield self.labelled(data, **kwargs)</code></pre>
</details>
</dd>
<dt id="refinery.shell.byteswap"><code class="flex name class">
<span>class <span class="ident">byteswap</span></span>
<span>(</span><span>size=4)</span>
</code></dt>
<dd>
<section class="desc"><p>Reverses the order of bytes in each block. Excess bytes that are not an integer multiple of the block
size are discarded.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/blockwise/byteswap.py#L12-L42" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class byteswap(UnaryOperation):
    &#34;&#34;&#34;
    Reverses the order of bytes in each block. Excess bytes that are not an integer multiple of the block
    size are discarded.
    &#34;&#34;&#34;
    def __init__(self, size: Arg.Number(help=&#39;the block size in bytes; the default is {default}.&#39;) = 4):
        super().__init__(blocksize=size, _truncate=2)

    def inplace(self, block: ndarray) -&gt; None:
        block.byteswap(True)

    operate = NotImplemented

    def process(self, data):
        try:
            return self._fastblock(data)
        except FastBlockError:
            b = self.blocksize
            n = len(data)
            m = n - n % b
            v = memoryview(data)
            if b == 1:
                self.log_warn(&#39;running this unit with a block size of 1 does not have any effect&#39;)
                return data
            for k in range(0, m, b):
                _end = k and k - 1 or None
                data[k : k + b] = v[k + b - 1:_end:-1]
            if m &lt; n:
                del v
                del data[m:]
            return data</code></pre>
</details>
</dd>
<dt id="refinery.shell.bz2"><code class="flex name class">
<span>class <span class="ident">bz2</span></span>
<span>(</span><span>level=9)</span>
</code></dt>
<dd>
<section class="desc"><p>BZip2 compression and decompression.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/compression/bz2.py#L9-L24" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class bz2(Unit):
    &#34;&#34;&#34;
    BZip2 compression and decompression.
    &#34;&#34;&#34;
    def __init__(self, level: Arg(&#39;-l&#39;, type=number[1:9], help=&#39;compression level preset between 1 and 9&#39;) = 9):
        super().__init__(level=level)

    def process(self, data):
        return bz2_.decompress(data)

    def reverse(self, data):
        return bz2_.compress(data, self.args.level)

    @classmethod
    def handles(self, data: bytearray):
        return data[:3] == B&#39;BZh&#39;</code></pre>
</details>
</dd>
<dt id="refinery.shell.camellia"><code class="flex name class">
<span>class <span class="ident">camellia</span></span>
<span>(</span><span>key, iv=b'', *, padding=None, mode=None, raw=False, little_endian=False, segment_size=0, mac_len=0, assoc_len=0)</span>
</code></dt>
<dd>
<section class="desc"><p>Camellia encryption and decryption.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/crypto/cipher/camellia.py#L220-L224" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class camellia(StandardBlockCipherUnit, cipher=BlockCipherFactory(Camellia)):
    &#34;&#34;&#34;
    Camellia encryption and decryption.
    &#34;&#34;&#34;
    pass</code></pre>
</details>
</dd>
<dt id="refinery.shell.carve"><code class="flex name class">
<span>class <span class="ident">carve</span></span>
<span>(</span><span>format, unique=False, decode=False, single=False, min=1, max=None, len=None, stripspace=False, longest=False, take=None, utf16=True, ascii=True)</span>
</code></dt>
<dd>
<section class="desc"><p>Extracts patches of data in particular formats from the input.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/pattern/carve.py#L7-L99" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class carve(PatternExtractor):
    &#34;&#34;&#34;
    Extracts patches of data in particular formats from the input.
    &#34;&#34;&#34;
    def __init__(
        self, format: Arg.Choice(choices=[p.display for p in formats], metavar=&#39;format&#39;,
            help=&#39;Specify one of the following formats: {choices}&#39;),
        unique: Arg.Switch(&#39;-q&#39;, help=&#39;Yield every match only once.&#39;) = False,
        decode: Arg.Switch(&#39;-d&#39;, help=&#39;Automatically decode known patterns.&#39;) = False,
        single: Arg.Switch(&#39;-s&#39;, help=&#39;Only get the biggest match; equivalent to -qlt1&#39;) = False,
        min=1, max=None, len=None,
        stripspace=False, longest=False, take=None, utf16=True, ascii=True
    ):
        if single:
            take = 1
            longest = True
            unique = True
        super().__init__(
            min=min,
            max=max,
            len=len,
            stripspace=stripspace,
            duplicates=not unique,
            longest=longest,
            take=take,
            ascii=ascii,
            utf16=utf16,
            format=formats.from_dashname(format)
        )
        if not decode:
            decoder = NotImplemented
        elif self.args.format in (formats.multiline_string, formats.string):
            from ..encoding.esc import esc
            decoder = esc(unicode=True, quoted=True)
        elif self.args.format is formats.integer:
            from ..encoding.base import base
            decoder = base()
        elif self.args.format in (formats.uppercase_hex, formats.spaced_hex, formats.hex):
            from ..encoding.hex import hex
            decoder = hex()
        elif self.args.format is formats.hexdump:
            from ..formats.hexload import hexload
            decoder = hexload()
        elif self.args.format is formats.intarray:
            from ..blockwise.pack import pack
            decoder = pack()
        elif self.args.format in (formats.b64, formats.b64any, formats.spaced_b64):
            from ..encoding.b64 import b64
            decoder = b64()
        elif self.args.format in (formats.b85, formats.spaced_b85):
            from ..encoding.b85 import b85
            decoder = b85()
        elif self.args.format is formats.b64url:
            from ..encoding.b64 import b64
            decoder = b64(urlsafe=True)
        elif self.args.format is formats.b32:
            from ..encoding.b32 import b32
            decoder = b32()
        elif self.args.format is formats.ps1str:
            from ..encoding.ps1str import ps1str
            decoder = ps1str()
        elif self.args.format is formats.vbastr:
            from ..encoding.ps1str import ps1str
            decoder = ps1str()
        elif self.args.format is formats.hexarray:
            from ..blockwise.pack import pack
            decoder = pack(0x10)
        elif self.args.format is formats.wshenc:
            from ..encoding.wshenc import wshenc
            decoder = wshenc()
        elif self.args.format is formats.uuencode:
            from ..encoding.uuenc import uuenc
            decoder = uuenc()
        elif self.args.format in (
            formats.urlquote,
            formats.urlquote_coarse,
            formats.urlquote_narrow,
        ):
            from ..encoding.url import url
            decoder = url()
        else:
            decoder = NotImplemented
        self.decoder = decoder

    def process(self, data):
        it = iter(self.matches_filtered(memoryview(data), self.args.format.value.bin_compiled))
        if self.decoder is NotImplemented:
            yield from it
        for chunk in it:
            try:
                yield self.decoder(chunk)
            except Exception as E:
                self.log_info(F&#39;decoder failure: {E!s}&#39;)</code></pre>
</details>
</dd>
<dt id="refinery.shell.carve_7z"><code class="flex name class">
<span>class <span class="ident">carve_7z</span></span>
</code></dt>
<dd>
<section class="desc"><p>Extracts anything from the input data that looks like a 7zip archive file.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/pattern/carve_7z.py#L25-L60" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class carve_7z(Unit):
    &#34;&#34;&#34;
    Extracts anything from the input data that looks like a 7zip archive file.
    &#34;&#34;&#34;
    @Unit.Requires(&#39;py7zr&#39;, &#39;arc&#39;, &#39;default&#39;, &#39;extended&#39;)
    def _py7zr():
        import py7zr
        return py7zr

    HEADER_SIGNATURE = B&#39;7z\xBC\xAF\x27\x1C&#39;

    def process(self, data: bytearray):
        cursor = 0
        mv = memoryview(data)
        while True:
            start = data.find(self.HEADER_SIGNATURE, cursor)
            if start &lt; cursor:
                break
            self.log_debug(F&#39;found header at offset: 0x{start:08X}&#39;)
            try:
                mf = MemoryFileRecorder(mv[start:])
                self.log_debug(&#39;attempting to read archive&#39;)
                archive = self._py7zr.SevenZipFile(mf)
                self.log_debug(&#39;attempting to test archive&#39;)
                success = archive.test() is not False
            except ImportError:
                raise
            except Exception as error:
                self.log_debug(&#39;parsing archive failed:&#39;, error)
                success = False
            if success:
                self.log_info(F&#39;identified archive of size 0x{mf.max_cursor:08X} at offset 0x{start:08X}&#39;)
                cursor = start + mf.max_cursor
                yield self.labelled(mv[start:cursor], offset=start)
            else:
                cursor = start + 5</code></pre>
</details>
</dd>
<dt id="refinery.shell.carve_json"><code class="flex name class">
<span>class <span class="ident">carve_json</span></span>
<span>(</span><span>dictonly=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Extracts anything from the input data that looks like JSON.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/pattern/carve_json.py#L96-L105" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class carve_json(Unit):
    &#34;&#34;&#34;
    Extracts anything from the input data that looks like JSON.
    &#34;&#34;&#34;
    def __init__(self, dictonly: Arg.Switch(&#39;-d&#39;, help=&#39;only extract JSON dictionaries, do not extract lists.&#39;) = False):
        super().__init__(dictonly=dictonly)

    def process(self, data):
        for start, chunk in JSONCarver(data, dictonly=self.args.dictonly):
            yield self.labelled(chunk, offset=start)</code></pre>
</details>
</dd>
<dt id="refinery.shell.carve_lnk"><code class="flex name class">
<span>class <span class="ident">carve_lnk</span></span>
</code></dt>
<dd>
<section class="desc"><p>Extracts anything from the input data that looks like a Windows shortcut (i.e. an LNK file)</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/pattern/carve_lnk.py#L8-L60" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class carve_lnk(Unit):
    &#34;&#34;&#34;
    Extracts anything from the input data that looks like a Windows shortcut (i.e. an LNK file)
    &#34;&#34;&#34;

    @Unit.Requires(&#39;LnkParse3&gt;=1.4.0&#39;, &#39;formats&#39;, &#39;extended&#39;)
    def _LnkParse3():
        import LnkParse3
        import LnkParse3.extra_factory
        return LnkParse3

    def process(self, data: bytearray):
        pos = 0
        mem = memoryview(data)
        sig = B&#39;\x4C\x00\x00\x00\x01\x14\x02\x00&#39;
        lnk = self._LnkParse3

        while True:
            pos = data.find(sig, pos)
            if pos &lt; 0:
                break
            try:
                parsed = lnk.lnk_file(indata=mem[pos:])
            except Exception:
                pos += 1
                continue
            end = pos + parsed.header.size() + parsed.string_data.size()
            if parsed.has_target_id_list():
                end += parsed.targets.size()
            if parsed.has_link_info() and not parsed.force_no_link_info():
                with suppress(AttributeError):
                    end += parsed.info.size()
            with NoLogging():
                while end &lt; len(mem):
                    extra = lnk.extra_factory.ExtraFactory(mem[end:])
                    try:
                        ec = extra.extra_class()
                    except Exception:
                        break
                    if ec is None:
                        break
                    if &#39;UNKNOWN&#39; in ec().name():
                        break
                    end += extra.item_size()

            terminal_block = mem[end:end + 4]
            if terminal_block != B&#39;\0\0\0\0&#39;:
                self.log_warn(F&#39;detected LNK at offset 0x{pos:X}, but size calculation did not end on a terminal block&#39;)
                continue
            else:
                end += 4
            yield self.labelled(mem[pos:end], offset=pos)
            pos = end</code></pre>
</details>
</dd>
<dt id="refinery.shell.carve_pe"><code class="flex name class">
<span>class <span class="ident">carve_pe</span></span>
<span>(</span><span>*paths, list=False, join_path=False, drop_path=False, path=b'name', recursive=False, keep_root=False, memdump=False, fileinfo=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Extracts anything from the input data that looks like a Portable
Executable (PE) file.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/pattern/carve_pe.py#L11-L88" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class carve_pe(PathExtractorUnit):
    &#34;&#34;&#34;
    Extracts anything from the input data that looks like a Portable
    Executable (PE) file.
    &#34;&#34;&#34;
    def __init__(
        self, *paths, list=False, join_path=False, drop_path=False, path=b&#39;name&#39;,
        recursive: Arg.Switch(&#39;-r&#39;, help=&#39;Extract PE files that are contained in already extracted PEs.&#39;) = False,
        keep_root: Arg.Switch(&#39;-k&#39;, help=&#39;If the input chunk is itself a PE, include it as an output chunk.&#39;) = False,
        memdump  : Arg.Switch(&#39;-m&#39;, help=&#39;Use the virtual memory layout of a PE file to calculate its size.&#39;) = False,
        fileinfo : Arg.Switch(&#39;-f&#39;, help=&#39;Use the PE meta information to deduce a file name meta variable.&#39;) = False
    ):
        super().__init__(
            *paths,
            list=list,
            join_path=join_path,
            drop_path=drop_path,
            path=path,
            recursive=recursive,
            keep_root=keep_root,
            memdump=memdump,
            fileinfo=fileinfo,
        )

    def unpack(self, data):
        cursor = 0
        mv = memoryview(data)

        while True:
            offset = data.find(B&#39;MZ&#39;, cursor)
            if offset &lt; cursor: break
            cursor = offset + 2
            ntoffset = mv[offset + 0x3C:offset + 0x3E]
            if len(ntoffset) &lt; 2:
                return
            ntoffset, = unpack(&#39;H&#39;, ntoffset)
            if mv[offset + ntoffset:offset + ntoffset + 2] != B&#39;PE&#39;:
                self.log_debug(F&#39;invalid NT header signature for candidate at 0x{offset:08X}&#39;)
                continue
            try:
                pe = PE(data=data[offset:], fast_load=True)
            except PEFormatError as err:
                self.log_debug(F&#39;parsing of PE header at 0x{offset:08X} failed:&#39;, err)
                continue

            pesize = get_pe_size(pe, memdump=self.args.memdump)
            pedata = mv[offset:offset + pesize]
            info = {}
            if self.args.fileinfo:
                pe_meta_parser = pemeta()
                try:
                    info = pe_meta_parser.parse_version(pe) or {}
                except Exception as error:
                    self.log_warn(F&#39;Unable to obtain file information: {error!s}&#39;)
                try:
                    info.update(pe_meta_parser.parse_header(pe) or {})
                except Exception:
                    pass
            try:
                path = info[&#39;OriginalFilename&#39;]
            except KeyError:
                try:
                    path = info[&#39;ExportName&#39;]
                except KeyError:
                    extension = &#39;exe&#39; if pe.is_exe() else &#39;dll&#39; if pe.is_dll() else &#39;sys&#39;
                    path = F&#39;carve-0x{offset:08X}.{extension}&#39;

            if offset &gt; 0 or self.args.keep_root:
                yield UnpackResult(path, pedata, offset=offset)
                self.log_info(F&#39;extracted PE file of size 0x{pesize:08X} from 0x{offset:08X}&#39;)
            else:
                self.log_info(F&#39;ignored root file of size 0x{pesize:08X} from 0x{offset:08X}&#39;)
                continue

            if not offset or self.args.recursive:
                cursor += pe.OPTIONAL_HEADER.SizeOfHeaders
            else:
                cursor += pesize - 2</code></pre>
</details>
</dd>
<dt id="refinery.shell.carve_rtf"><code class="flex name class">
<span>class <span class="ident">carve_rtf</span></span>
</code></dt>
<dd>
<section class="desc"><p>Extracts anything from the input data that looks like an RTF document.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/pattern/carve_rtf.py#L8-L34" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class carve_rtf(Unit):
    &#34;&#34;&#34;
    Extracts anything from the input data that looks like an RTF document.
    &#34;&#34;&#34;

    def process(self, data: bytearray):
        pos = 0
        mem = memoryview(data)
        sig = re.escape(b&#39;{\\rtf&#39;)

        while True:
            match = re.search(sig, mem[pos:], flags=re.IGNORECASE)
            if match is None:
                break
            pos = pos + match.start()
            end = pos + 1
            depth = 1
            while depth and end &lt; len(mem):
                if mem[end] == 0x7B:  # {
                    depth += 1
                if mem[end] == 0x7D:  # }
                    depth -= 1
                end += 1
            if depth &gt; 0:
                break
            yield self.labelled(mem[pos:end], offset=pos)
            pos = end</code></pre>
</details>
</dd>
<dt id="refinery.shell.carve_xml"><code class="flex name class">
<span>class <span class="ident">carve_xml</span></span>
</code></dt>
<dd>
<section class="desc"><p>Extracts anything from the input data that looks like XML.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/pattern/carve_xml.py#L121-L128" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class carve_xml(Unit):
    &#34;&#34;&#34;
    Extracts anything from the input data that looks like XML.
    &#34;&#34;&#34;

    def process(self, data):
        for offset, chunk in XMLCarver(data):
            yield self.labelled(chunk, offset=offset)</code></pre>
</details>
</dd>
<dt id="refinery.shell.carve_zip"><code class="flex name class">
<span>class <span class="ident">carve_zip</span></span>
</code></dt>
<dd>
<section class="desc"><p>Extracts anything from the input data that looks like a zip archive file.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/pattern/carve_zip.py#L49-L89" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class carve_zip(Unit):
    &#34;&#34;&#34;
    Extracts anything from the input data that looks like a zip archive file.
    &#34;&#34;&#34;

    def process(self, data: bytearray):
        end = len(data)
        mem = memoryview(data)
        rev = []
        while True:
            end = data.rfind(ZipEndOfCentralDirectory.SIGNATURE, 0, end)
            if end &lt; 0:
                break
            try:
                end_marker = ZipEndOfCentralDirectory(mem[end:])
            except ValueError as e:
                self.log_info(F&#39;error parsing end of central directory at 0x{end:X}: {e!s}&#39;)
                continue
            else:
                self.log_info(F&#39;successfully parsed end of central directory at 0x{end:X}&#39;)
            start = end - end_marker.directory_size
            shift = start - end_marker.directory_offset
            if start &lt; 0:
                self.log_debug(&#39;end of central directory size is invalid&#39;)
                continue
            try:
                central_directory = ZipCentralDirectory(mem[start:])
            except ValueError:
                self.log_debug(&#39;computed location of central directory is invalid&#39;)
                end = end - len(ZipEndOfCentralDirectory.SIGNATURE)
                continue
            start = central_directory.header_offset + shift
            if mem[start:start + 4] not in (B&#39;PK\x03\x04&#39;, B&#39;\0\0\0\0&#39;):
                # SFX payloads seem to have a nulled header, so we permit this.
                self.log_debug(&#39;computed start of ZIP archive does not have the correct signature bytes&#39;)
                continue
            rev.append((start, end + len(end_marker)))
            end = start
        for start, end in reversed(rev):
            zip = mem[start:end + len(end_marker)]
            yield self.labelled(zip, offset=start)</code></pre>
</details>
</dd>
<dt id="refinery.shell.cast"><code class="flex name class">
<span>class <span class="ident">cast</span></span>
<span>(</span><span>key, iv=b'', *, padding=None, mode=None, raw=False, little_endian=False, segment_size=0, mac_len=0, assoc_len=0)</span>
</code></dt>
<dd>
<section class="desc"><p>CAST encryption and decryption.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/crypto/cipher/cast.py#L9-L13" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class cast(StandardBlockCipherUnit, cipher=PyCryptoFactoryWrapper(CAST)):
    &#34;&#34;&#34;
    CAST encryption and decryption.
    &#34;&#34;&#34;
    pass</code></pre>
</details>
</dd>
<dt id="refinery.shell.cca"><code class="flex name class">
<span>class <span class="ident">cca</span></span>
<span>(</span><span>data)</span>
</code></dt>
<dd>
<section class="desc"><p>Short for ConCatAppend: This unit concatenates the input data with its argument by
appending the latter to the former. See also <code><a title="refinery.ccp" href="index.html#refinery.ccp">ccp</a></code> for the unit that prepends
instead.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/strings/cca.py#L6-L18" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class cca(Unit):
    &#34;&#34;&#34;
    Short for ConCatAppend: This unit concatenates the input data with its argument by
    appending the latter to the former. See also `refinery.ccp` for the unit that prepends
    instead.
    &#34;&#34;&#34;

    def __init__(self, data: Arg(help=&#39;Binary string to be appended to the input.&#39;)):
        super().__init__(data=data)

    def process(self, data: bytearray):
        data.extend(self.args.data)
        return data</code></pre>
</details>
</dd>
<dt id="refinery.shell.ccp"><code class="flex name class">
<span>class <span class="ident">ccp</span></span>
<span>(</span><span>data)</span>
</code></dt>
<dd>
<section class="desc"><p>Short for ConCatPrepend: This unit concatenates the input data with its argument by
prepending the latter to the former. See also <code><a title="refinery.cca" href="index.html#refinery.cca">cca</a></code> for the unit that appends
instead.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/strings/ccp.py#L6-L18" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class ccp(Unit):
    &#34;&#34;&#34;
    Short for ConCatPrepend: This unit concatenates the input data with its argument by
    prepending the latter to the former. See also `refinery.cca` for the unit that appends
    instead.
    &#34;&#34;&#34;

    def __init__(self, data: Arg(help=&#39;Binary string to be prepended to the input.&#39;)):
        super().__init__(data=data)

    def process(self, data: bytearray):
        data[:0] = self.args.data
        return data</code></pre>
</details>
</dd>
<dt id="refinery.shell.cfmt"><code class="flex name class">
<span>class <span class="ident">cfmt</span></span>
<span>(</span><span>*formats, variable=None, separator=' ', multiplex=False, binary=False, unescape=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Stands for "Convert to ForMaT": Transform a given chunk by applying a format string operation.
The positional format string placeholder <code>{}</code> will be replaced by the incoming data, named
placeholders have to exist as meta variables in the current chunk. For example, the following
pipeline can be used to print all files in a given directory with their corresponding SHA-256
hash:</p>
<pre><code>ef ** [| sha256 -t | cfmt {} {path} ]]
</code></pre>
<p>By default, format string arguments are simply joined along a space character to form a single
format string.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/strings/cfmt.py#L9-L61" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class cfmt(Unit):
    &#34;&#34;&#34;
    Stands for &#34;Convert to ForMaT&#34;: Transform a given chunk by applying a format string operation.
    The positional format string placeholder `{}` will be replaced by the incoming data, named
    placeholders have to exist as meta variables in the current chunk. For example, the following
    pipeline can be used to print all files in a given directory with their corresponding SHA-256
    hash:

        ef ** [| sha256 -t | cfmt {} {path} ]]

    By default, format string arguments are simply joined along a space character to form a single
    format string.
    &#34;&#34;&#34;

    def __init__(
        self,
        *formats : Arg(help=&#39;Format strings.&#39;, type=str, metavar=&#39;format&#39;),
        variable : Arg(&#39;-n&#39;, type=str, metavar=&#39;N&#39;, help=&#39;Store the formatted string in a meta variable.&#39;) = None,
        separator: Arg(&#39;-s&#39;, group=&#39;SEP&#39;, metavar=&#39;S&#39;,
            help=&#39;Separator to insert between format strings. The default is a space character.&#39;) = &#39; &#39;,
        multiplex: Arg.Switch(&#39;-m&#39;, group=&#39;SEP&#39;,
            help=&#39;Do not join the format strings along the separator, generate one output for each.&#39;) = False,
        binary   : Arg.Switch(&#39;-b&#39;, help=&#39;Use the binary formatter instead of the string formatter.&#39;) = False,
        unescape : Arg.Switch(&#39;-e&#39;, help=&#39;Interpret escape sequences in format strings.&#39;) = False,
    ):
        def fixfmt(fmt: bytes):
            if unescape:
                if isinstance(fmt, str):
                    fmt = fmt.encode(&#39;latin1&#39;)
                return fmt.decode(&#39;unicode-escape&#39;)
            elif not isinstance(fmt, str):
                fmt = fmt.decode(self.codec)
            return fmt
        formats = [fixfmt(f) for f in formats]
        if not multiplex:
            formats = [fixfmt(separator).join(formats)]
        super().__init__(formats=formats, variable=variable, binary=binary)

    def process(self, data):
        meta = metavars(data)
        meta.ghost = True
        args = [data]
        variable = self.args.variable
        if self.args.binary:
            formatter = partial(meta.format_bin, codec=self.codec, args=args)
        else:
            def formatter(spec):
                return meta.format_str(spec, self.codec, args).encode(self.codec)
        for spec in self.args.formats:
            result = formatter(spec)
            if variable is not None:
                result = self.labelled(data, **{variable: result})
            yield result</code></pre>
</details>
</dd>
<dt id="refinery.shell.chacha"><code class="flex name class">
<span>class <span class="ident">chacha</span></span>
<span>(</span><span>key, stateful=False, discard=0, nonce=b'REFINERY', magic=b'', offset=0, rounds=20)</span>
</code></dt>
<dd>
<section class="desc"><p>ChaCha encryption and decryption. The nonce must be 8 bytes long as currently, only the
original Bernstein algorithm is implemented. When 64 bytes are provided as the key, this
data is interpreted as the initial state box and all other parameters are ignored.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/crypto/cipher/chacha.py#L61-L79" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class chacha(LatinCipherUnit):
    &#34;&#34;&#34;
    ChaCha encryption and decryption. The nonce must be 8 bytes long as currently, only the
    original Bernstein algorithm is implemented. When 64 bytes are provided as the key, this
    data is interpreted as the initial state box and all other parameters are ignored.
    &#34;&#34;&#34;
    def keystream(self) -&gt; Iterable[int]:
        key = self.args.key
        if len(key) == 64:
            it = ChaChaCipher.FromState(key)
        else:
            it = ChaChaCipher(
                key,
                self.args.nonce,
                self.args.magic,
                self.args.rounds,
                self.args.offset,
            )
        yield from it</code></pre>
</details>
</dd>
<dt id="refinery.shell.chacha20"><code class="flex name class">
<span>class <span class="ident">chacha20</span></span>
<span>(</span><span>key, nonce=b'REFINERY')</span>
</code></dt>
<dd>
<section class="desc"><p>ChaCha20 and XChaCha20 encryption and decryption. For ChaCha20, the IV (nonce) must
be 8 or 12 bytes long; for XChaCha20, choose an IV which is 24 bytes long. Invoking
this unit for ChaCha20 is functionally equivalent to <code><a title="refinery.chacha" href="index.html#refinery.chacha">chacha</a></code> with 20 rounds,
but this unit uses the PyCryptodome library C implementation rather than the pure
Python implementation used by <code><a title="refinery.chacha" href="index.html#refinery.chacha">chacha</a></code>.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/crypto/cipher/chacha.py#L38-L46" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class chacha20(LatinCipherStandardUnit, cipher=PyCryptoFactoryWrapper(ChaCha20)):
    &#34;&#34;&#34;
    ChaCha20 and XChaCha20 encryption and decryption. For ChaCha20, the IV (nonce) must
    be 8 or 12 bytes long; for XChaCha20, choose an IV which is 24 bytes long. Invoking
    this unit for ChaCha20 is functionally equivalent to `refinery.chacha` with 20 rounds,
    but this unit uses the PyCryptodome library C implementation rather than the pure
    Python implementation used by `refinery.chacha`.
    &#34;&#34;&#34;
    pass</code></pre>
</details>
</dd>
<dt id="refinery.shell.chacha20poly1305"><code class="flex name class">
<span>class <span class="ident">chacha20poly1305</span></span>
<span>(</span><span>key, nonce=b'REFINERY')</span>
</code></dt>
<dd>
<section class="desc"><p>ChaCha20-Poly1305 and XChaCha20-Poly1305 encryption and decryption. For the ChaCha20
variant, the nonce must be 8 or 12 bytes long; for XChaCha20, provide a 24 bytes nonce
instead.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/crypto/cipher/chacha.py#L49-L58" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class chacha20poly1305(LatinCipherStandardUnit, cipher=PyCryptoFactoryWrapper(ChaCha20_Poly1305)):
    &#34;&#34;&#34;
    ChaCha20-Poly1305 and XChaCha20-Poly1305 encryption and decryption. For the ChaCha20
    variant, the nonce must be 8 or 12 bytes long; for XChaCha20, provide a 24 bytes nonce
    instead.
    &#34;&#34;&#34;
    def _get_cipher(self, reset_cache=False):
        cipher = super()._get_cipher(reset_cache)
        cipher.block_size = 1
        return cipher</code></pre>
</details>
</dd>
<dt id="refinery.shell.chaskey"><code class="flex name class">
<span>class <span class="ident">chaskey</span></span>
<span>(</span><span>key, iv=b'', padding=None, mode=None, raw=False, rounds=12, swap=False, *, assoc_len=0, mac_len=0, segment_size=0, little_endian=False)</span>
</code></dt>
<dd>
<section class="desc"><p>This implements a block cipher based on the Chaskey algorithm. No subkeys are computed and the
default Chaskey operation is performed on all blocks. Notably, the Donut framework uses Chaskey
with 16 rounds and in CTR mode.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/crypto/cipher/chaskey.py#L103-L122" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class chaskey(StandardBlockCipherUnit, cipher=BlockCipherFactory(Chaskey)):
    &#34;&#34;&#34;
    This implements a block cipher based on the Chaskey algorithm. No subkeys are computed and the
    default Chaskey operation is performed on all blocks. Notably, the Donut framework uses Chaskey
    with 16 rounds and in CTR mode.
    &#34;&#34;&#34;
    def __init__(
        self, key, iv=b&#39;&#39;, padding=None, mode=None, raw=False,
        rounds: Arg.Number(&#39;-k&#39;, help=&#39;Number of rounds to use, the default is {default}&#39;) = _R,
        swap: Arg.Switch(&#39;-s&#39;, help=&#39;Use big endian byte order for all blocks.&#39;) = False,
        **more
    ):
        super().__init__(key, iv, padding=padding, mode=mode, raw=raw, rounds=rounds, swap=swap, **more)

    def _new_cipher(self, **optionals) -&gt; CipherInterface:
        return super()._new_cipher(
            swap=self.args.swap,
            rounds=self.args.rounds,
            **optionals
        )</code></pre>
</details>
</dd>
<dt id="refinery.shell.chop"><code class="flex name class">
<span>class <span class="ident">chop</span></span>
<span>(</span><span>size, step=None, truncate=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Reinterprets the input as a sequence of equally sized chunks and outputs this sequence.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/meta/chop.py#L7-L29" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class chop(Unit):
    &#34;&#34;&#34;
    Reinterprets the input as a sequence of equally sized chunks and outputs this sequence.
    &#34;&#34;&#34;

    def __init__(
        self,
        size: Arg.Number(&#39;size&#39;, help=&#39;Chop data into chunks of this size&#39;),
        step: Arg.Number(&#39;step&#39;, help=(
            &#39;Optionally specify a step size (which is equal to the size by default) which indicates the number of bytes by &#39;
            &#39;which the cursor will be increased after extracting a chunk.&#39;)) = None,
        truncate: Arg.Switch(&#39;-t&#39;, help=(
            &#39;Truncate possible excess bytes at the end of the input, by default they are appended as a single chunk.&#39;)) = False,
    ):
        return super().__init__(size=size, step=step, truncate=truncate)

    def process(self, data):
        view = memoryview(data)
        size = self.args.size
        step = self.args.step
        if size &lt; 1:
            raise ValueError(&#39;The chunk size has to be a positive integer value.&#39;)
        yield from splitchunks(view, size, step, self.args.truncate)</code></pre>
</details>
</dd>
<dt id="refinery.shell.clower"><code class="flex name class">
<span>class <span class="ident">clower</span></span>
</code></dt>
<dd>
<section class="desc"><p>Stands for "Convert to LOWER case"; The unit simply converts all latin alphabet chacters in the
input to lowercase.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/strings/clower.py#L6-L12" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class clower(Unit):
    &#34;&#34;&#34;
    Stands for &#34;Convert to LOWER case&#34;; The unit simply converts all latin alphabet chacters in the
    input to lowercase.
    &#34;&#34;&#34;
    def process(self, data):
        return data.lower()</code></pre>
</details>
</dd>
<dt id="refinery.shell.cm"><code class="flex name class">
<span>class <span class="ident">cm</span></span>
<span>(</span><span>invert=False, all=False, reset=False, size=False, ext=False, entropy=False, ic=False, magic=False, sha1=False, sha256=False, crc32=False, md5=False, hashes=False, *names)</span>
</code></dt>
<dd>
<section class="desc"><p>The Common Meta variables unit populates the set of meta variables of the current chunk with commonly
used metadata. The unit has no effect outside a frame.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/meta/cm.py#L11-L88" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class cm(Unit):
    &#34;&#34;&#34;
    The Common Meta variables unit populates the set of meta variables of the current chunk with commonly
    used metadata. The unit has no effect outside a frame.
    &#34;&#34;&#34;
    def __init__(
        self,
        invert  : Arg.Switch(&#39;-x&#39;, group=&#39;ALL&#39;, help=&#39;populate only options that have not been specified&#39;) = False,
        all     : Arg.Switch(&#39;-a&#39;, group=&#39;ALL&#39;, help=&#39;populate all options&#39;) = False,
        reset   : Arg.Switch(&#39;-r&#39;, help=&#39;discard all meta variables that were not explicitly specified&#39;) = False,
        size    : Arg.Switch(&#39;-S&#39;, help=&#39;size of the chunk&#39;) = False,
        ext     : Arg.Switch(&#39;-X&#39;, help=&#39;guess file extension&#39;) = False,
        entropy : Arg.Switch(&#39;-E&#39;, help=&#39;compute data entropy&#39;) = False,
        ic      : Arg.Switch(&#39;-C&#39;, help=&#39;compute the index of coincidence&#39;) = False,
        magic   : Arg.Switch(&#39;-M&#39;, help=&#39;compute file magic&#39;) = False,
        sha1    : Arg.Switch(&#39;-1&#39;, help=&#39;compute hash: SHA-1&#39;) = False,
        sha256  : Arg.Switch(&#39;-2&#39;, help=&#39;compute hash: SHA-256&#39;) = False,
        crc32   : Arg.Switch(&#39;-3&#39;, help=&#39;compute hash: CRC32&#39;) = False,
        md5     : Arg.Switch(&#39;-5&#39;, help=&#39;compute hash: MD5&#39;) = False,
        hashes  : Arg.Switch(&#39;-H&#39;, help=&#39;compute all common hashes&#39;) = False,
        *names  : Arg(metavar=&#39;name&#39;, help=(
            F&#39;A variable name that can include the common properties: {_COMMON_PROPERTIES_LIST}.&#39;
            R&#39; If none is given, the size variable is populated. For most of these, an optional &#39;
            R&#39;argument is available that can be used as a shorthand:&#39;))
    ):
        def stringify(name):
            if isinstance(name, str):
                return name
            return name.decode(self.codec)

        names = {stringify(name) for name in names}
        if hashes:
            md5 = sha256 = sha1 = crc32 = True
        if size:
            names.add(&#39;size&#39;)
        if ext:
            names.add(&#39;ext&#39;)
        if entropy:
            names.add(&#39;entropy&#39;)
        if ic:
            names.add(&#39;ic&#39;)
        if magic:
            names.add(&#39;magic&#39;)
        if sha1:
            names.add(&#39;sha1&#39;)
        if sha256:
            names.add(&#39;sha256&#39;)
        if crc32:
            names.add(&#39;crc32&#39;)
        if md5:
            names.add(&#39;md5&#39;)
        if not names and not reset:
            names.add(&#39;size&#39;)
        if all:
            if invert:
                raise ValueError(&#39;invert and all are both enabled, resulting in empty configuration.&#39;)
            names = set(LazyMetaOracle.derivations)
        elif invert:
            names = set(LazyMetaOracle.derivations) - names
        super().__init__(names=names, reset=reset)

    def process(self, data):
        return data

    def filter(self, chunks):
        names = self.args.names
        reset = self.args.reset
        for chunk in chunks:
            chunk: Chunk
            if not chunk.visible:
                yield chunk
                continue
            meta = metavars(chunk)
            if reset:
                chunk.meta.clear()
            for name in names:
                chunk[name] = meta[name]
            yield chunk</code></pre>
</details>
</dd>
<dt id="refinery.shell.couple"><code class="flex name class">
<span>class <span class="ident">couple</span></span>
<span>(</span><span>*commandline, buffer=False, noerror=False, timeout=0.0)</span>
</code></dt>
<dd>
<section class="desc"><p>Turns any command into a refinery unit. Data is processed by feeding it to the standard input of a process spawned from
the given command line, and then reading the standard output of that process as the result of the operation. The main
purpose of this unit is to allow using the syntax from <code><a title="refinery.lib.frame" href="lib/frame.html">refinery.lib.frame</a></code> with other command line tools. By default,
the <code><a title="refinery.couple" href="index.html#refinery.couple">couple</a></code> unit streams the output from the executed command as individual outputs, but the <code>buffer</code> option
can be set to buffer all output of a single execution. The format string expression <code>{}</code> or <code>{0}</code> can be used as one of
the arguments passed to the external command to represent the incoming data. In this case, the data will not be sent
to the standard input device of the new process.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/misc/couple.py#L11-L166" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class couple(Unit):
    &#34;&#34;&#34;
    Turns any command into a refinery unit. Data is processed by feeding it to the standard input of a process spawned from
    the given command line, and then reading the standard output of that process as the result of the operation. The main
    purpose of this unit is to allow using the syntax from `refinery.lib.frame` with other command line tools. By default,
    the `refinery.couple` unit streams the output from the executed command as individual outputs, but the `buffer` option
    can be set to buffer all output of a single execution. The format string expression `{}` or `{0}` can be used as one of
    the arguments passed to the external command to represent the incoming data. In this case, the data will not be sent
    to the standard input device of the new process.
    &#34;&#34;&#34;

    _JOIN_TIME = 0.1

    def __init__(
        self, *commandline : Arg(nargs=&#39;...&#39;, type=str, metavar=&#39;(all remaining)&#39;, help=(
            &#39;All remaining command line tokens form an arbitrary command line to be executed. Use format string syntax &#39;
            &#39;to insert meta variables and incoming data chunks.&#39;)),
        buffer: Arg.Switch(&#39;-b&#39;, help=&#39;Buffer the command output for one execution rather than streaming it.&#39;) = False,
        noerror: Arg(&#39;-e&#39;, help=&#39;do not merge stdin and stderr; stderr will only be output if -v is also specified.&#39;) = False,
        timeout: Arg(&#39;-t&#39;, metavar=&#39;T&#39;,
            help=&#39;Set an execution timeout as a floating point number in seconds, there is none by default.&#39;) = 0.0
    ):
        if not commandline:
            raise ValueError(&#39;you need to provide a command line.&#39;)
        super().__init__(commandline=commandline, noerror=noerror, buffer=buffer, timeout=timeout)

    def process(self, data):
        def shlexjoin():
            import shlex
            return &#39; &#39;.join(shlex.quote(cmd) for cmd in commandline)

        meta = metavars(data)
        meta.ghost = True
        used = set()
        commandline = [
            meta.format(cmd, self.codec, [data], None, False, used=used)
            for cmd in self.args.commandline
        ]

        if 0 in used:
            self.log_info(&#39;input used as command-line argument; sending no input to process stdin&#39;)
            data = None

        self.log_debug(shlexjoin)

        posix = &#39;posix&#39; in sys.builtin_module_names
        process = Popen(commandline,
            stdin=PIPE, stdout=PIPE, stderr=PIPE, shell=False, close_fds=posix)

        if self.args.buffer and not self.args.timeout:
            out, err = process.communicate(data)
            for line in err.splitlines():
                self.log_info(line)
            yield out
            return

        import io
        from threading import Thread, Event
        from queue import Queue, Empty
        from time import process_time, sleep

        start = 0
        result = None

        qerr = Queue()
        qout = Queue()
        done = Event()

        def adapter(stream, queue: Queue, event: Event):
            while not event.is_set():
                out = stream.read1()
                if out: queue.put(out)
                else: break
            stream.close()

        recvout = Thread(target=adapter, args=(process.stdout, qout, done), daemon=True)
        recverr = Thread(target=adapter, args=(process.stderr, qerr, done), daemon=True)

        recvout.start()
        recverr.start()

        if data:
            process.stdin.write(data)
        process.stdin.close()
        start = process_time()

        if self.args.buffer or self.args.timeout:
            result = io.BytesIO()

        def queue_read(q: Queue):
            try: return q.get_nowait()
            except Empty: return None

        errbuf = io.BytesIO()

        while True:
            out = queue_read(qout)
            err = None

            if self.args.noerror:
                err = queue_read(qerr)
            else:
                out = out or queue_read(qerr)

            if err and self.log_info():
                errbuf.write(err)
                errbuf.seek(0)
                lines = errbuf.readlines()
                errbuf.seek(0)
                errbuf.truncate()
                if lines:
                    if not (done.is_set() or lines[~0].endswith(B&#39;\n&#39;)):
                        errbuf.write(lines.pop())
                    for line in lines:
                        msg = line.rstrip(B&#39;\n&#39;)
                        if msg: self.log_info(msg)
            if out:
                if self.args.buffer or self.args.timeout:
                    result.write(out)
                if not self.args.buffer:
                    yield out

            if done.is_set():
                if recverr.is_alive():
                    self.log_warn(&#39;stderr receiver thread zombied&#39;)
                if recvout.is_alive():
                    self.log_warn(&#39;stdout receiver thread zombied&#39;)
                break
            elif not err and not out and process.poll() is not None:
                recverr.join(self._JOIN_TIME)
                recvout.join(self._JOIN_TIME)
                done.set()
            elif self.args.timeout:
                if process_time() - start &gt; self.args.timeout:
                    self.log_info(&#39;terminating process after timeout expired&#39;)
                    done.set()
                    process.terminate()
                    for wait in range(4):
                        if process.poll() is not None:
                            break
                        sleep(self._JOIN_TIME)
                    else:
                        self.log_warn(&#39;process termination may have failed&#39;)
                    recverr.join(self._JOIN_TIME)
                    recvout.join(self._JOIN_TIME)
                    if not len(result.getbuffer()):
                        result = RuntimeError(&#39;timeout reached, process had no output&#39;)
                    else:
                        result = RefineryPartialResult(
                            &#39;timeout reached, returning all collected output&#39;,
                            partial=result.getvalue())

        if isinstance(result, Exception):
            raise result
        elif self.args.buffer:
            yield result.getvalue()</code></pre>
</details>
</dd>
<dt id="refinery.shell.cp1252"><code class="flex name class">
<span>class <span class="ident">cp1252</span></span>
</code></dt>
<dd>
<section class="desc"><p>Encodes and decodes Windows CP 1252 (aka Latin1) encoded string data.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/encoding/cp1252.py#L6-L15" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class cp1252(Unit):
    &#34;&#34;&#34;
    Encodes and decodes Windows CP 1252 (aka Latin1) encoded string data.
    &#34;&#34;&#34;

    def process(self, data):
        return data.decode(self.codec).encode(&#39;cp1252&#39;)

    def reverse(self, data):
        return data.decode(&#39;cp1252&#39;).encode(self.codec)</code></pre>
</details>
</dd>
<dt id="refinery.shell.crc32"><code class="flex name class">
<span>class <span class="ident">crc32</span></span>
<span>(</span><span>text=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Returns the CRC32 Hash of the input data.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/crypto/hash/checksums.py#L12-L17" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class crc32(HashUnit):
    &#34;&#34;&#34;
    Returns the CRC32 Hash of the input data.
    &#34;&#34;&#34;
    def _algorithm(self, data: bytes) -&gt; bytes:
        return struct.pack(&#39;&gt;I&#39;, zlib.crc32(data))</code></pre>
</details>
</dd>
<dt id="refinery.shell.csb"><code class="flex name class">
<span>class <span class="ident">csb</span></span>
<span>(</span><span>format, utf16=True, ascii=True)</span>
</code></dt>
<dd>
<section class="desc"><p>Short for carve single buffer; carves the single largest buffer of a given format from the
input data and returns it.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/pattern/carve.py#L117-L129" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class csb(carve):
    &#34;&#34;&#34;
    Short for carve single buffer; carves the single largest buffer of a given format from the
    input data and returns it.
    &#34;&#34;&#34;
    def __init__(self, format, utf16=True, ascii=True):
        super().__init__(
            format,
            decode=False,
            single=True,
            utf16=utf16,
            ascii=ascii,
        )</code></pre>
</details>
</dd>
<dt id="refinery.shell.csd"><code class="flex name class">
<span>class <span class="ident">csd</span></span>
<span>(</span><span>format, utf16=True, ascii=True)</span>
</code></dt>
<dd>
<section class="desc"><p>Short for carve &amp; decode; carves the single largest buffer of a given format from the input
and decodes it with the appropriate decoder.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/pattern/carve.py#L102-L114" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class csd(carve):
    &#34;&#34;&#34;
    Short for carve &amp; decode; carves the single largest buffer of a given format from the input
    and decodes it with the appropriate decoder.
    &#34;&#34;&#34;
    def __init__(self, format, utf16=True, ascii=True):
        super().__init__(
            format,
            decode=True,
            single=True,
            utf16=utf16,
            ascii=ascii,
        )</code></pre>
</details>
</dd>
<dt id="refinery.shell.csv"><code class="flex name class">
<span>class <span class="ident">csv</span></span>
<span>(</span><span>quote=b'"', delim=b',')</span>
</code></dt>
<dd>
<section class="desc"><p>Extracts the rows of a CSV document with header and converts them into JSON chunks.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/formats/csv.py#L14-L90" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class csv(Unit):
    &#34;&#34;&#34;
    Extracts the rows of a CSV document with header and converts them into JSON chunks.
    &#34;&#34;&#34;
    def __init__(
        self,
        quote: Unit.Arg(&#39;-q&#39;, help=&#39;Specify the quote character, the default is a double quote.&#39;) = B&#39;&#34;&#39;,
        delim: Unit.Arg(&#39;-d&#39;, help=&#39;Specify the delimiter, the default is a single comma.&#39;) = B&#39;,&#39;
    ):
        super().__init__(quote=quote, delim=delim)

    def json_to_csv(self, table: dict):
        quote = self.args.quote.decode(self.codec)
        delim = self.args.delim.decode(self.codec)

        if not isinstance(table, list):
            raise ValueError(&#39;Input must be a JSON list.&#39;)

        out = MemoryFile()

        with io.TextIOWrapper(out, self.codec, newline=&#39;&#39;) as stream:
            writer = _csv.writer(stream, quotechar=quote, delimiter=delim, skipinitialspace=True)
            for row in table:
                if not isinstance(row, list):
                    break
                if not all(isinstance(item, str) for item in row):
                    break
                writer.writerow(row)
            else:
                return out.getvalue()

        keys = {}
        # A dictionary is used here over a set because dictionaries remember insertion order.
        # When feeding the unit a sequence of JSON objects, the user would likely expect the
        # column order in the resulting CSV to derive from the entry oder in the JSON data.

        for row in table:
            for key in row:
                if not isinstance(key, str):
                    continue
                keys[key] = None

        keys = list(keys)
        out = MemoryFile()

        with io.TextIOWrapper(out, self.codec, newline=&#39;&#39;) as stream:
            writer = _csv.writer(stream, quotechar=quote, delimiter=delim, skipinitialspace=True)
            writer.writerow(keys)
            for row in table:
                writer.writerow([str(row.get(key, &#39;&#39;)) for key in keys])
            return out.getvalue()

    def reverse(self, data: bytearray):
        try:
            table: List[Dict[str, Any]] = json.loads(data)
        except Exception:
            table: List[Dict[str, Any]] = [json.loads(line) for line in data.splitlines()]
        return self.json_to_csv(table)

    def process(self, data):
        quote = self.args.quote.decode(self.codec)
        delim = self.args.delim.decode(self.codec)

        def convert(field: str):
            if field.isdigit() and not field.startswith(&#39;0&#39;):
                return int(field)
            date = isodate(field)
            if date is not None:
                return date.isoformat(&#39; &#39;, &#39;seconds&#39;)
            return field

        with io.TextIOWrapper(MemoryFile(data), self.codec) as stream:
            rows = _csv.reader(stream, quotechar=quote, delimiter=delim, skipinitialspace=True)
            keys = next(rows)
            for row in rows:
                out = {key: convert(value) for key, value in zip(keys, row)}
                yield json.dumps(out, indent=4).encode(self.codec)</code></pre>
</details>
</dd>
<dt id="refinery.shell.cswap"><code class="flex name class">
<span>class <span class="ident">cswap</span></span>
</code></dt>
<dd>
<section class="desc"><p>Swap the case of the input string; all lowercase letters are turned into their uppercase
variant and vice-versa.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/strings/cswap.py#L6-L20" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class cswap(Unit):
    &#34;&#34;&#34;
    Swap the case of the input string; all lowercase letters are turned into their uppercase
    variant and vice-versa.
    &#34;&#34;&#34;
    def process(self, data: bytearray):
        lcase = bytes(range(B&#39;a&#39;[0], B&#39;z&#39;[0] + 1))
        ucase = bytes(range(B&#39;A&#39;[0], B&#39;Z&#39;[0] + 1))
        delta = lcase[0] - ucase[0]
        for k, letter in enumerate(data):
            if letter in ucase:
                data[k] += delta
            elif letter in lcase:
                data[k] -= delta
        return data</code></pre>
</details>
</dd>
<dt id="refinery.shell.cupper"><code class="flex name class">
<span>class <span class="ident">cupper</span></span>
</code></dt>
<dd>
<section class="desc"><p>Stands for "Convert to UPPER case"; The unit simply converts all latin alphabet chacters in the
input to uppercase.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/strings/cupper.py#L6-L12" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class cupper(Unit):
    &#34;&#34;&#34;
    Stands for &#34;Convert to UPPER case&#34;; The unit simply converts all latin alphabet chacters in the
    input to uppercase.
    &#34;&#34;&#34;
    def process(self, data):
        return data.upper()</code></pre>
</details>
</dd>
<dt id="refinery.shell.datefix"><code class="flex name class">
<span>class <span class="ident">datefix</span></span>
<span>(</span><span>format='%Y-%m-%d %H:%M:%S', dos=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Parses all kinds of date formats and unifies them into the same format.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/misc/datefix.py#L11-L126" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class datefix(Unit):
    &#34;&#34;&#34;
    Parses all kinds of date formats and unifies them into the same format.
    &#34;&#34;&#34;

    _FORMATS = [
        &#39;%B %dth %Y %H:%M:%S (UTC)&#39;,  # November 27th 2019 17:37:02 (UTC)
        &#39;%B %dnd %Y %H:%M:%S (UTC)&#39;,  # November 22nd 2019 17:37:02 (UTC)
        &#39;%B %dst %Y %H:%M:%S (UTC)&#39;,  # November 21st 2019 17:37:02 (UTC)
        &#39;%B %dth %Y, %H:%M:%S&#39;,       # November 27th 2019, 17:37:02
        &#39;%B %dnd %Y, %H:%M:%S&#39;,       # November 22nd 2019, 17:37:02
        &#39;%B %dst %Y, %H:%M:%S&#39;,       # November 21st 2019, 17:37:02
        &#39;%Y-%m-%dT%H:%M:%S&#39;,          # 2010-03-15T06:27:50
        &#39;%Y-%m-%d %H:%M:%S&#39;,          # iso (2010-03-15 06:27:50.000000)
        &#39;%Y-%m-%d %H:%M:%SZ%f&#39;,
        &#39;%Y-%m-%dT%H:%M:%S.%f&#39;,
        &#39;%Y-%m-%dT%H:%M:%SZ%f&#39;,
        &#39;%a %b %d %Y %H:%M:%S&#39;,       # Thu Apr 24 2014 12:32:21
        &#39;%m/%d/%Y %H:%M:%S&#39;,
        &#39;%m/%d/%Y, %H:%M:%S&#39;,
        &#39;%m/%d/%Y&#39;,
        &#39;%Y:%m:%d %H:%M:%S&#39;,          # ExifTool Output
    ]

    _TIMEZONE_REGEXES = [re_compile(p) for p in [
        R&#39;([+-])(\d{2})(\d{2})$&#39;,           # Thu Apr 24 2014 12:32:21 GMT-0700
        R&#39;([+-])(\d{2}):(\d{2})$&#39;,          # 2017:09:11 23:47:22+02:00
        R&#39;GMT([+-])(\d{2})(\d{2}) \(.+\)$&#39;  # Thu Apr 24 2014 12:32:21 GMT-0700 (PDT)
    ]]

    def __init__(
        self,
        format: Arg(help=&#39;Specify the output format as a strftime-like string, using ISO by default.&#39;) = &#39;%Y-%m-%d %H:%M:%S&#39;,
        dos: Arg(&#39;-d&#39;, help=&#39;Parse timestamps in DOS rather than Unix format.&#39;) = False
    ):
        super().__init__(format=format, dos=dos)

    @staticmethod
    def dostime(stamp: int) -&gt; datetime:
        &#34;&#34;&#34;
        Parses a given DOS timestamp into a datetime object.
        &#34;&#34;&#34;
        d, t = stamp &gt;&gt; 16, stamp &amp; 0xFFFF
        s = (t &amp; 0x1F) &lt;&lt; 1

        return datetime(
            year   = ((d &amp; 0xFE00) &gt;&gt; 0x9) + 1980,  # noqa
            month  = ((d &amp; 0x01E0) &gt;&gt; 0x5),         # noqa
            day    = ((d &amp; 0x001F) &gt;&gt; 0x0),         # noqa
            hour   = ((t &amp; 0xF800) &gt;&gt; 0xB),         # noqa
            minute = ((t &amp; 0x07E0) &gt;&gt; 0x5),         # noqa
            second = 59 if s == 60 else s,          # noqa
        )

    def _format(self, dt: datetime) -&gt; str:
        return dt.strftime(self.args.format)

    def _extract_timezone(self, data):
        for r in self._TIMEZONE_REGEXES:
            m = r.search(data)
            if not m:
                continue
            pm = m[1]
            td = timedelta(
                hours=int(m[2]), minutes=int(m[3]))
            if pm == &#39;-&#39;:
                td = -td
            return data[:-len(m[0])].strip(), td

        return data, None

    @linewise
    def process(self, data: str) -&gt; str:
        data = data.strip()

        # replace colons (i.e. for exiftool dates: 2017:01:01)
        if len(data) &gt; 10 and data[4] == &#39;:&#39; and data[7] == &#39;:&#39;:
            data = F&#39;{data[0:4]}-{data[5:7]}-{data[8:]}&#39;

        # strips Z at end (i.e. 20171022055144Z)
        if data.endswith(&#39;Z&#39;):
            data = data[:-1]

        if data.startswith(&#39;0x&#39;):
            try:
                data = str(int(data, 16))
            except Exception:
                pass

        # parses timestamps and dates without much format
        if data.isdigit():
            time_stamp = int(data)
            if len(data) &gt; 14:
                raise Exception(&#39;cannot parse all-numeric string as date: %s&#39; % data)
            elif len(data) == 14:
                # i.e. 20111020193727
                return self._format(datetime.strptime(data, &#39;%Y%m%d%H%M%S&#39;))
            elif len(data) == 13:
                # i.e. 1458016535000
                time_stamp //= 1000
                data = data[:-3]
            if self.args.dos:
                return self._format(self.dostime(time_stamp))
            else:
                return self._format(date_from_timestamp(time_stamp))

        data, time_delta = self._extract_timezone(data)

        for f in self._FORMATS:
            try:
                dt = datetime.strptime(data, f)
            except ValueError:
                continue
            return self._format(dt if time_delta is None else dt - time_delta)

        return data</code></pre>
</details>
</dd>
<dt id="refinery.shell.decompress"><code class="flex name class">
<span>class <span class="ident">decompress</span></span>
<span>(</span><span>prepend=True, tolerance=12, max_ratio=1.0, min_ratio=0.0001, strict_limits=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Attempts all available decompression units against the input and returns
the output of the first successful one. If none succeeds, the data is
returned unaltered. The process is heavily biased against LZNT1 decompression
due to a large tendency for LZNT1 false positives.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/compression/decompress.py#L67-L249" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class decompress(Unit):
    &#34;&#34;&#34;
    Attempts all available decompression units against the input and returns
    the output of the first successful one. If none succeeds, the data is
    returned unaltered. The process is heavily biased against LZNT1 decompression
    due to a large tendency for LZNT1 false positives.
    &#34;&#34;&#34;
    def __init__(
        self,
        prepend: Arg.Switch(&#39;-P&#39;, &#39;--no-prepend&#39;, off=True, help=(
            &#39;By default, if decompression fails, the unit attempts to prefix &#39;
            &#39;the data with all possible values of a single byte and decompress &#39;
            &#39;the result. This behavior can be disabled with this flag.&#39;)
        ) = True,
        tolerance: Arg.Number(&#39;-t&#39;, help=(
            &#39;Maximum number of bytes to strip from the beginning of the data; &#39;
            &#39;The default value is 12.&#39;)
        ) = 12,
        max_ratio: Arg(&#39;-m&#39;, metavar=&#39;R&#39;, help=(
            &#39;To determine whether a decompression algorithm was successful, the &#39;
            &#39;ratio of compressed size to decompressed size may at most be as large &#39;
            &#39;as this number, a floating point value R; default value is {default}.&#39;)
        ) = 1.0,
        min_ratio: Arg(&#39;-n&#39;, metavar=&#39;R&#39;, help=(
            &#39;Require that compression ratios must be at least as large as R. This &#39;
            &#39;is a &#34;too good to be true&#34; heuristic against algorithms like lznt1 &#39;
            &#39;that can produce false positives. The default is {default}.&#39;)
        ) = 0.0001,
        strict_limits: Arg(&#39;-l&#39;, action=&#39;store_true&#39;, help=(
            &#39;For recognized formats, i.e. when a magic signature is present, the &#39;
            &#39;above limits are disabled by default. Activate this flag to enforce &#39;
            &#39;them in every case.&#39;)
        ) = False

    ):
        if min_ratio &lt;= 0:
            raise ValueError(&#39;The compression factor must be nonnegative.&#39;)
        super().__init__(
            tolerance=tolerance,
            prepend=prepend,
            min_ratio=min_ratio,
            max_ratio=max_ratio,
            strict_limits=strict_limits,
        )
        self.engines: List[Unit] = [
            engine.assemble() for engine in [
                zstd, szdd, bz2, zl, lzf, lzma, lzw, jcalg, lzo, aplib, qlz, brotli, blz, lzjb, lz4, lznt1, nrv2e, nrv2d, nrv2b]
        ]
        for engine in self.engines:
            engine.log_detach()

    def process(self, data):

        data = memoryview(data)

        class Decompression(NamedTuple):
            engine: Unit
            rating: _R
            result: Optional[ByteString] = None
            cutoff: int = 0
            prefix: Optional[int] = None

            def __str__(self):
                status = self.rating.summary
                engine = self.engine.name
                prefix = self.prefix
                if prefix is not None:
                    prefix = F&#39;0x{prefix:02X}&#39;
                return F&#39;prefix={prefix}, cutoff=0x{self.cutoff:02X}, [{status}] engine={engine}&#39;

            def __len__(self):
                return len(self.result)

            @property
            def ratio(self):
                if not self.result:
                    return INF
                return len(data) / len(self)

            @property
            def unmodified(self):
                return self.prefix is None and self.cutoff == 0

            @property
            def method(self):
                return self.engine.name

        if self.args.prepend:
            buffer = bytearray(1 + len(data))
            buffer[1:] = data

        best_by_rating: Dict[_R, Decompression] = {}

        def best_current_rating():
            return max(best_by_rating, default=_R.InvalidData)

        def decompress(engine: Unit, cutoff: int = 0, prefix: Optional[int] = None, careful: bool = False):
            ingest = data[cutoff:]
            rating = _R.ValidData
            if cutoff == 0 and prefix is None and not careful:
                rating |= _R.NotMangled
            if prefix is not None:
                buffer[0] = prefix
                ingest = buffer
            is_handled = engine.handles(ingest)
            if is_handled is True:
                rating |= _R.KnownFormat
            if is_handled is False:
                return Decompression(engine, _R.InvalidData, None, cutoff, prefix)
            try:
                result = next(engine.act(ingest))
            except RefineryPartialResult as pr:
                rating |= _R.HadOutput
                result = pr.partial
            except Exception:
                result = None
            else:
                rating |= _R.Successful
            return Decompression(engine, rating, result, cutoff, prefix)

        def update(new: Decompression, discard_if_too_good=False):
            ratio = new.ratio
            if self.args.strict_limits or not new.rating &amp; _R.KnownFormat:
                if ratio &gt; self.args.max_ratio:
                    return
                if ratio &lt; self.args.min_ratio:
                    return
            best = best_by_rating.get(new.rating, None)
            prefix = new.prefix
            if prefix is not None:
                prefix = F&#39;0x{prefix:02X}&#39;
            if new.unmodified and best and not best.unmodified:
                threshold = 1
            else:
                threshold = 0.95
            if not best or len(new) &lt; len(best):
                q = 0
            else:
                q = len(best) / len(new)
            ratio *= 100
            brief = new.rating.brief
            if q &lt; threshold:
                if best and discard_if_too_good:
                    if q &lt; 0.5:
                        return
                    if new.rating &amp; _R.Successful != _R.Successful:
                        return
                self.log_info(lambda:
                    F&#39;[switch] [{brief}] [q={q:07.4f}] compression ratio {ratio:07.4f}% with: {new!s}&#39;)
                best_by_rating[new.rating] = new
            else:
                self.log_debug(lambda:
                    F&#39;[reject] [{brief}] [q={q:07.4f}] compression ratio {ratio:07.4f}% with: {new!s}&#39;)

        for engine in self.engines:
            self.log_debug(F&#39;attempting engine: {engine.name}&#39;)
            careful = isinstance(engine, (lznt1, lzf, lzjb))
            for t in range(self.args.tolerance + 1):
                if best_current_rating() &gt;= _R.Successful and careful and t &gt; 0:
                    break
                update(decompress(engine, t, None, careful), careful)
            if self.args.prepend and best_current_rating() &lt; _R.Successful:
                for p in range(0x100):
                    update(decompress(engine, 0, p, careful), careful)

        for r in sorted(best_by_rating, reverse=True):
            if dc := best_by_rating[r]:
                if not dc.rating &amp; _R.HadOutput:
                    continue
                self.log_info(F&#39;settling on {dc.method} decompression, cutoff={dc.cutoff} and prefix={dc.prefix}.&#39;)
                if dc.rating &amp; _R.NotMangled:
                    self.log_info(&#39;supporting evidence: no modifications to the buffer were necessary&#39;)
                if dc.rating &amp; _R.KnownFormat:
                    self.log_info(&#39;supporting evidence: found a known magic signature&#39;)
                if dc.rating &amp; _R.HadNoErrors:
                    self.log_info(&#39;supporting evidence: engine produced output without errors&#39;)
                elif dc.rating &amp; _R.HadOutput:
                    self.log_info(&#39;supporting evidence: there were errors, but the engine produced output&#39;)
                if not dc.rating &amp; _R.Successful:
                    self.log_info(&#39;the only decompression with result returned only a partial result.&#39;)
                return self.labelled(dc.result, method=dc.method)

        raise ValueError(&#39;no compression engine worked&#39;)</code></pre>
</details>
</dd>
<dt id="refinery.shell.dedup"><code class="flex name class">
<span>class <span class="ident">dedup</span></span>
<span>(</span><span>key=None, count=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Deduplicates a sequence of multiple inputs. The deduplication is limited to the current <code><a title="refinery.lib.frame" href="lib/frame.html">refinery.lib.frame</a></code>.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/meta/dedup.py#L11-L62" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class dedup(Unit):
    &#34;&#34;&#34;
    Deduplicates a sequence of multiple inputs. The deduplication is limited to the current `refinery.lib.frame`.
    &#34;&#34;&#34;
    def __init__(
        self,
        key: Arg(&#39;key&#39;, type=str, help=&#39;An optional meta variable expression to deduplicate.&#39;) = None,
        count: Arg.Switch(&#39;-c&#39;, help=&#39;Store the count of each deduplicated chunk.&#39;) = False
    ):
        super().__init__(key=key, count=count)

    def filter(self, chunks):
        keyvar = self.args.key

        if keyvar is not None:
            def key(chunk):
                v = PythonExpression.Evaluate(keyvar, metavars(chunk))
                if isbuffer(v):
                    v = md5(v).digest()
                return v
        else:
            def key(chunk):
                return md5(chunk).digest()

        if self.args.count:
            counts = {}
            buffer = {}
            hashes = None
        else:
            hashes = set()
            counts = None
            buffer = None

        for chunk in chunks:
            if not chunk.visible:
                yield chunk
                continue

            uid = key(chunk)

            if hashes is None:
                counts[uid] = counts.get(uid, 0) + 1
                buffer.setdefault(uid, chunk)
            elif uid in hashes:
                continue
            else:
                hashes.add(uid)
                yield chunk

        if hashes is None:
            for uid, chunk in buffer.items():
                yield self.labelled(chunk, count=counts[uid])</code></pre>
</details>
</dd>
<dt id="refinery.shell.defang"><code class="flex name class">
<span>class <span class="ident">defang</span></span>
<span>(</span><span>url_only=False, url_protocol=False, dot_only=False, quote_md=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Defangs all URL, domain and IPv4 address indicators in the input data by replacing the last dot
in the expression by <code>[.]</code>. For example, <code>127.0.0.1</code> will be replaced by <code>127.0.0[.]1</code>. For URL
indicators, the colon after the procol scheme is also wrapped in brackets.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/pattern/defang.py#L12-L131" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class defang(Unit):
    &#34;&#34;&#34;
    Defangs all URL, domain and IPv4 address indicators in the input data by replacing the last dot
    in the expression by `[.]`. For example, `127.0.0.1` will be replaced by `127.0.0[.]1`. For URL
    indicators, the colon after the procol scheme is also wrapped in brackets.
    &#34;&#34;&#34;

    _WHITELIST = [
        B&#39;wscript.shell&#39;,
    ]

    _PROTOCOL_ESCAPES = {
        B&#39;http&#39;: B&#39;hxxp&#39;,
        B&#39;https&#39;: B&#39;hxxps&#39;,
        B&#39;ftp&#39;: B&#39;fxp&#39;,
        B&#39;ftps&#39;: B&#39;fxps&#39;,
    }

    def __init__(
        self,
        url_only: Arg.Switch(&#39;-u&#39;, help=&#39;Only defang URLs, do not look for domains or IPs.&#39;) = False,
        url_protocol: Arg.Switch(&#39;-p&#39;, help=&#39;Escape the protocol in URLs.&#39;) = False,
        dot_only: Arg.Switch(&#39;-d&#39;, help=&#39;Do not escape the protocol colon in URLs.&#39;) = False,
        quote_md: Arg.Switch(&#39;-q&#39;, help=&#39;Wrap all indicators in backticks for markdown code.&#39;) = False
    ):
        self.superinit(super(), **vars())

    def _quote(self, word):
        return word if not self.args.quote_md else B&#39;`%s`&#39; % word

    def reverse(self, data: bytearray):
        def refang(hostname):
            return hostname[0].replace(B&#39;[.]&#39;, B&#39;.&#39;)
        data = defanged.hostname.sub(refang, data)
        data = data.replace(B&#39;[:]//&#39;, B&#39;://&#39;)
        data = data.replace(B&#39;[://]&#39;, B&#39;://&#39;)
        data = re.sub(B&#39;h.{3}?(s?)://&#39;, B&#39;http\\1://&#39;, data)
        data = re.sub(B&#39;fxp(s?)://&#39;, B&#39;ftp\\1://&#39;, data)
        return data

    def process(self, data):
        def replace_hostname(hostname: bytes, match=True):
            if match:
                return self._quote(replace_hostname(hostname[0], False))
            self.log_info(&#39;replace:&#39;, hostname)
            host = hostname
            user, atsgn, host = host.rpartition(B&#39;@&#39;)
            host, colon, port = host.rpartition(B&#39;:&#39;)
            host = host.lower()
            if not colon:
                host = port
                port = B&#39;&#39;
            if host in self._WHITELIST:
                return hostname
            host = re.split(R&#39;(?:\[\.\]|\.)&#39;, host.decode(&#39;latin1&#39;))
            if len(host) == 1:
                return hostname
            components = iter(reversed(host))
            defanged_parts = [next(components)]
            separator = &#39;[.]&#39;
            for part in components:
                defanged_parts.append(separator)
                defanged_parts.append(part)
                separator = &#39;[.]&#39; if part in tlds else &#39;.&#39;
            defanged_host = &#39;&#39;.join(reversed(defanged_parts)).encode(&#39;latin1&#39;)
            return user + atsgn + defanged_host + colon + port

        def replace_url(url: bytes):
            if not url:
                return url
            self.log_info(&#39;replace:&#39;, url)
            url = url.replace(B&#39;[:]//&#39;, B&#39;://&#39;, 1)
            url = url.replace(B&#39;[.]&#39;, B&#39;.&#39;)
            prefix = B&#39;tcp&#39;
            if url.startswith(B&#39;://&#39;):
                scheme = 0
            elif url.startswith(B&#39;//&#39;):
                scheme = 1
                prefix = prefix + B&#39;:&#39;
            else:
                scheme = 2
                prefix = B&#39;&#39;
            parsed = urlparse(prefix + url)
            operations = {
                name: self.process(getattr(parsed, name))
                for name in (&#39;path&#39;, &#39;params&#39;, &#39;query&#39;, &#39;fragment&#39;)
            }
            if self.args.url_protocol and parsed.scheme:
                operations.update(scheme=self._PROTOCOL_ESCAPES.get(parsed.scheme.lower(), scheme))
            if scheme &lt; 2:
                operations.update(scheme=B&#39;&#39;)
            operations.update(netloc=replace_hostname(parsed.netloc, False))
            url = urlunparse(parsed._replace(**operations))
            if scheme == 0:
                url = B&#39;:&#39; + url
            if not self.args.dot_only:
                url = url.replace(B&#39;://&#39;, B&#39;[:]//&#39;)
            return self._quote(url)

        urlsplit = defanged.url.split(data)
        step = defanged.url.value.groups + 1
        urlsplit[1::step] = [replace_url(t) for t in itertools.islice(iter(urlsplit), 1, None, step)]

        if not self.args.url_only:
            urlsplit[0::step] = [
                indicators.hostname.sub(replace_hostname, t)
                for t in itertools.islice(iter(urlsplit), 0, None, step)
            ]

        def fuse(urlsplit):
            txt = itertools.islice(iter(urlsplit), 0, None, step)
            url = itertools.islice(iter(urlsplit), 1, None, step)
            while True:
                try:
                    yield next(txt)
                    yield next(url)
                except StopIteration:
                    break

        return B&#39;&#39;.join(fuse(urlsplit))</code></pre>
</details>
</dd>
<dt id="refinery.shell.deob_js_arrays"><code class="flex name class">
<span>class <span class="ident">deob_js_arrays</span></span>
</code></dt>
<dd>
<section class="desc"><p>JavaScript deobfuscator to turn <code>["Z", "t", "s", "e"][0]</code> into <code>"Z"</code>.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/obfuscation/js/arrays.py#L9-L29" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class deob_js_arrays(Deobfuscator):
    &#34;&#34;&#34;
    JavaScript deobfuscator to turn `[&#34;Z&#34;, &#34;t&#34;, &#34;s&#34;, &#34;e&#34;][0]` into `&#34;Z&#34;`.
    &#34;&#34;&#34;

    def deobfuscate(self, data):

        def litpick(match):
            try:
                array = match[1]
                index = int(match[2])
                lpick = array.split(&#39;,&#39;)[index].strip()
                self.log_debug(lambda: F&#39;{lpick} = {match[0]}&#39;)
            except (TypeError, IndexError):
                lpick = match[0]
            return lpick

        p = R&#39;\s{{0,5}}&#39;.join([
            &#39;\\[&#39;, &#39;((?:{i}|{s})&#39;, &#39;(?:,&#39;, &#39;(?:{i}|{s})&#39;, &#39;)*)&#39;, &#39;\\]&#39;, &#39;\\[&#39;, &#39;({i})&#39;, &#39;\\]&#39;
        ]).format(i=formats.integer, s=formats.string)
        return re.sub(p, litpick, data)</code></pre>
</details>
</dd>
<dt id="refinery.shell.deob_js_getattr"><code class="flex name class">
<span>class <span class="ident">deob_js_getattr</span></span>
</code></dt>
<dd>
<section class="desc"><p>JavaScript deobfuscator to turn <code>WScript["CreateObject"]</code> into <code>WScript.CreateObject</code>.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/obfuscation/js/getattr.py#L9-L20" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class deob_js_getattr(Deobfuscator):
    &#34;&#34;&#34;
    JavaScript deobfuscator to turn `WScript[&#34;CreateObject&#34;]` into `WScript.CreateObject`.
    &#34;&#34;&#34;

    def deobfuscate(self, data):
        def dottify(match):
            name = match[2][1:-1]
            if name.isidentifier():
                return F&#39;{match[1]}.{name}&#39;
            return match[0]
        return re.sub(FR&#39;(\w+)\[({formats.string})\]&#39;, dottify, data)</code></pre>
</details>
</dd>
<dt id="refinery.shell.deob_js_tuples"><code class="flex name class">
<span>class <span class="ident">deob_js_tuples</span></span>
</code></dt>
<dd>
<section class="desc"><p>JavaScript deobfuscator to turn <code>("Z", "t", "s", "e")</code> into <code>"e"</code>.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/obfuscation/js/tuples.py#L9-L28" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class deob_js_tuples(Deobfuscator):
    &#34;&#34;&#34;
    JavaScript deobfuscator to turn `(&#34;Z&#34;, &#34;t&#34;, &#34;s&#34;, &#34;e&#34;)` into `&#34;e&#34;`.
    &#34;&#34;&#34;

    def deobfuscate(self, data):

        def litpick(match):
            try:
                array = match[1]
                lpick = array.split(&#39;,&#39;)[-1].strip()
                self.log_debug(lambda: F&#39;{lpick} = {match[0]}&#39;)
            except (TypeError, IndexError):
                lpick = match[0]
            return lpick

        p = R&#39;\s{{0,5}}&#39;.join([
            &#39;\\(&#39;, &#39;((?:{i}|{s})&#39;, &#39;(?:,&#39;, &#39;(?:{i}|{s})&#39;, &#39;)*)&#39;, &#39;\\)&#39;
        ]).format(i=formats.integer, s=formats.string)
        return re.sub(p, litpick, data)</code></pre>
</details>
</dd>
<dt id="refinery.shell.deob_ps1"><code class="flex name class">
<span>class <span class="ident">deob_ps1</span></span>
<span>(</span><span>timeout=100)</span>
</code></dt>
<dd>
<section class="desc"></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/obfuscation/ps1/all.py#L27-L53" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class deob_ps1(IterativeDeobfuscator):

    _SUBUNITS: List[Type[Deobfuscator]] = [
        deob_ps1_escape,
        deob_ps1_cases,
        deob_ps1_brackets,
        deob_ps1_format,
        deob_ps1_typecast,
        deob_ps1_stringreplace,
        deob_ps1_b64convert,
        deob_ps1_encodings,
        deob_ps1_concat,
        deob_ps1_invoke,
        deob_ps1_uncurly
    ]

    def deobfuscate(self, data):
        units = [u() for u in self._SUBUNITS]
        for u in units:
            u.log_level = self.log_level
        for unit in units:
            self.log_debug(lambda: F&#39;invoking {unit.name}&#39;)
            checkpoint = hash(data)
            data = unit.deobfuscate(data)
            if checkpoint != hash(data) and not self.log_debug(&#39;data has changed.&#39;):
                self.log_info(F&#39;used {unit.name}&#39;)
        return re.sub(R&#39;[\r\n]+&#39;, &#39;\n&#39;, data)</code></pre>
</details>
</dd>
<dt id="refinery.shell.deob_ps1_b64convert"><code class="flex name class">
<span>class <span class="ident">deob_ps1_b64convert</span></span>
</code></dt>
<dd>
<section class="desc"></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/obfuscation/ps1/b64convert.py#L13-L35" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class deob_ps1_b64convert(Deobfuscator):

    _SENTINEL = re.compile(&#39;\\s*&#39;.join(
        (re.escape(&#39;[System.Convert]::FromBase64String&#39;), &#39;\\(&#39;, &#39;({s})&#39;, &#39;\\)&#39;)
    ).format(s=formats.ps1str), flags=re.IGNORECASE)

    def deobfuscate(self, data):
        strlit = Ps1StringLiterals(data)

        def replacer(match: re.Match[str]):
            if strlit.get_container(match.start()):
                return match[0]
            try:
                string, = string_unquote(match[1])
            except ValueError:
                return match[0]
            try:
                bytes = base64.b64decode(string)
            except Exception:
                return match[0]
            return &#39;@({})&#39;.format(&#39;,&#39;.join(F&#39;0x{b:02X}&#39; for b in bytes))

        return self._SENTINEL.sub(replacer, data)</code></pre>
</details>
</dd>
<dt id="refinery.shell.deob_ps1_brackets"><code class="flex name class">
<span>class <span class="ident">deob_ps1_brackets</span></span>
</code></dt>
<dd>
<section class="desc"><p>PowerShell deobfuscation that removes superfluous brackets around constant
literals, i.e. <code>("{0}{2}{1}")</code> is transformed to <code>"{0}{2}{1}"</code>. Currently,
only integer and string constants are supported.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/obfuscation/ps1/brackets.py#L10-L38" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class deob_ps1_brackets(Deobfuscator):
    &#34;&#34;&#34;
    PowerShell deobfuscation that removes superfluous brackets around constant
    literals, i.e. `(&#34;{0}{2}{1}&#34;)` is transformed to `&#34;{0}{2}{1}&#34;`. Currently,
    only integer and string constants are supported.
    &#34;&#34;&#34;
    _SENTINEL = re.compile(
        RF&#39;&#39;&#39;(?&lt;![\w&#34;&#39;]{{2}})&#39;&#39;&#39;  # this may be a function call
        RF&#39;&#39;&#39;(\-\w+)?&#39;&#39;&#39;  # not a function call but an argument
        RF&#39;&#39;&#39;\(\s*({formats.integer}|{formats.ps1str})\s*(\S)&#39;&#39;&#39;,
        flags=re.IGNORECASE
    )

    def deobfuscate(self, data):
        strlit = Ps1StringLiterals(data)
        repeat = True

        @strlit.outside
        def replacement(match):
            nonlocal repeat
            if match[3] == &#39;)&#39;:
                repeat = True
                return (match[1] or &#39;&#39;) + match[2]

        while repeat:
            repeat = False
            data = self._SENTINEL.sub(replacement, data)

        return data</code></pre>
</details>
</dd>
<dt id="refinery.shell.deob_ps1_cases"><code class="flex name class">
<span>class <span class="ident">deob_ps1_cases</span></span>
</code></dt>
<dd>
<section class="desc"></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/obfuscation/ps1/cases.py#L9-L45" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class deob_ps1_cases(Deobfuscator):
    _NAMES = [
        &#39;-BXor&#39;,
        &#39;-Exec Bypass&#39;,
        &#39;-NoLogo&#39;,
        &#39;-NonInter&#39;,
        &#39;-Replace&#39;,
        &#39;-Windows Hidden&#39;,
        &#39;.Invoke&#39;,
        &#39;Assembly&#39;,
        &#39;Byte&#39;,
        &#39;Char&#39;,
        &#39;ChildItem&#39;,
        &#39;CreateThread&#39;,
        &#39;Get-Variable&#39;,
        &#39;GetType&#39;,
        &#39;IntPtr&#39;,
        &#39;Invoke-Expression&#39;,
        &#39;Invoke&#39;,
        &#39;Length&#39;,
        &#39;Net.WebClient&#39;,
        &#39;PowerShell&#39;,
        &#39;PSVersionTable&#39;,
        &#39;Set-Item&#39;,
        &#39;Set-Variable&#39;,
        &#39;Start-Sleep&#39;,
        &#39;ToString&#39;,
        &#39;Type&#39;,
        &#39;Value&#39;,
        &#39;Void&#39;,
    ]

    @outside(formats.ps1str)
    def deobfuscate(self, data):
        for name in self._NAMES:
            data = re.sub(RF&#39;\b{re.escape(name)}\b&#39;, name, data, flags=re.IGNORECASE)
        return data</code></pre>
</details>
</dd>
<dt id="refinery.shell.deob_ps1_concat"><code class="flex name class">
<span>class <span class="ident">deob_ps1_concat</span></span>
<span>(</span><span>timeout=100)</span>
</code></dt>
<dd>
<section class="desc"></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/obfuscation/ps1/concat.py#L9-L42" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class deob_ps1_concat(IterativeDeobfuscator):
    _SENTINEL = re.compile(R&#39;&#39;&#39;[&#39;&#34;]\s*[+&amp;]\s*[&#39;&#34;]&#39;&#39;&#39;)

    def deobfuscate(self, data):

        def concat(data):
            strlit = Ps1StringLiterals(data)
            repeat = True
            while repeat:
                for match in self._SENTINEL.finditer(data):
                    a, b = match.span()
                    a = strlit.get_container(a)
                    if a is None:
                        continue
                    b = strlit.get_container(b)
                    if b is None or b != a + 1:
                        continue
                    a = strlit.ranges[a]
                    b = strlit.ranges[b]
                    stra = data[slice(*a)]
                    strb = data[slice(*b)]
                    parts = list(string_unquote(stra))
                    it = iter(string_unquote(strb))
                    parts[~0] += next(it)
                    parts.extend(it)
                    yield data[:a[0]] + string_quote(parts)
                    data = data[b[1]:]
                    strlit.update(data)
                    break
                else:
                    repeat = False
            yield data

        return &#39;&#39;.join(concat(data))</code></pre>
</details>
</dd>
<dt id="refinery.shell.deob_ps1_encodings"><code class="flex name class">
<span>class <span class="ident">deob_ps1_encodings</span></span>
</code></dt>
<dd>
<section class="desc"></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/obfuscation/ps1/encodings.py#L13-L45" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class deob_ps1_encodings(Deobfuscator):

    _SENTINEL = re.compile(&#39;\\s*&#39;.join(
        (re.escape(&#39;[System.Text.Encoding]::&#39;) + &#39;(\\w+)\\.GetString&#39;, &#39;\\(&#39;, &#39;@\\(&#39;, &#39;({a})&#39;, &#39;\\)&#39;, &#39;\\)&#39;)
    ).format(a=formats.intarray), flags=re.IGNORECASE)

    def deobfuscate(self, data):
        strlit = Ps1StringLiterals(data)

        def replacer(match: re.Match[str]):
            if strlit.get_container(match.start()):
                return match[0]
            try:
                bytes = bytearray(int(x.strip(), 0) for x in match[2].split(&#39;,&#39;))
            except Exception:
                return match[0]
            encoding = {
                &#39;ASCII&#39;: &#39;ascii&#39;,
                &#39;BigEndianUnicode&#39;: &#39;utf-16be&#39;,
                &#39;Default&#39;: &#39;latin1&#39;,
                &#39;Unicode&#39;: &#39;utf-16le&#39;,
            }.get(match[1], match[1])
            try:
                codecs.lookup(encoding)
            except LookupError:
                encoding = &#39;utf8&#39;
            try:
                string = bytes.decode(encoding)
            except Exception:
                return match[0]
            return string_quote(string)

        return self._SENTINEL.sub(replacer, data)</code></pre>
</details>
</dd>
<dt id="refinery.shell.deob_ps1_escape"><code class="flex name class">
<span>class <span class="ident">deob_ps1_escape</span></span>
</code></dt>
<dd>
<section class="desc"></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/obfuscation/ps1/escape.py#L9-L15" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class deob_ps1_escape(Deobfuscator):

    def deobfuscate(self, data):
        strlit = Ps1StringLiterals(data)
        @strlit.outside
        def repl(m): return m[1]
        return re.sub(R&#39;&#39;&#39;`([^0abfnrtv`#&#39;&#34;\$])&#39;&#39;&#39;, repl, data)</code></pre>
</details>
</dd>
<dt id="refinery.shell.deob_ps1_format"><code class="flex name class">
<span>class <span class="ident">deob_ps1_format</span></span>
</code></dt>
<dd>
<section class="desc"><p>PowerShell deobfuscation for the following "format string"-based technique:</p>
<ul>
<li><code>"{0}{2}{1}"-f 'signa','ures','t'</code></li>
<li><code>"{0}na{2}{1}"-f 'sig','ures','t'</code></li>
</ul></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/obfuscation/ps1/format.py#L11-L79" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class deob_ps1_format(Deobfuscator):
    &#34;&#34;&#34;
    PowerShell deobfuscation for the following &#34;format string&#34;-based technique:

    - `&#34;{0}{2}{1}&#34;-f &#39;signa&#39;,&#39;ures&#39;,&#39;t&#39;`
    - `&#34;{0}na{2}{1}&#34;-f &#39;sig&#39;,&#39;ures&#39;,&#39;t&#39;`
    &#34;&#34;&#34;

    def deobfuscate(self, data):

        repeat = True

        while repeat:

            repeat = False

            for string in re.finditer(str(formats.ps1str), data):
                argmatch = re.search(R&#39;^\s*-[fF]\s*((?:{s},\s*)*{s})&#39;.format(s=formats.ps1str), data[string.end():])
                if not argmatch:
                    continue

                def dbgmsg():
                    sample = string[0]
                    if len(sample) &gt; 33:
                        sample = F&#34;{sample[1:30]}...{sample[0]}&#34;
                    return F&#39;found match at {string.start()}: {sample}&#39;

                self.log_debug(dbgmsg)

                args = re.split(F&#39;({formats.ps1str})&#39;, argmatch[1])
                args = [list(string_unquote(a.strip())) for a in args[1::2]]

                def formatter(string):
                    buffer = []
                    for k, part in enumerate(re.split(R&#39;(\{\d+\})&#39;, string)):
                        if k % 2 == 0:
                            if part:
                                buffer.append(part)
                            continue
                        try:
                            index = int(part[1:-1])
                            arg = args[index]
                        except IndexError as IE:
                            raise IndexError(F&#39;only found {len(args)} arguments and format sequence {index}, aborting.&#39;) from IE

                        it = iter(arg)
                        buffer.append(next(it))

                        if len(arg) &gt; 1:
                            yield &#39;&#39;.join(buffer)
                            buffer = []
                            for last, part in lookahead(it):
                                if last:
                                    buffer.append(part)
                                    break
                                yield part

                    yield &#39;&#39;.join(buffer)

                try:
                    result = string_apply(string[0], formatter)
                except IndexError:
                    continue

                data = data[:string.start()] + result + data[argmatch.end() + string.end():]
                repeat = True
                break

        return data</code></pre>
</details>
</dd>
<dt id="refinery.shell.deob_ps1_invoke"><code class="flex name class">
<span>class <span class="ident">deob_ps1_invoke</span></span>
</code></dt>
<dd>
<section class="desc"></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/obfuscation/ps1/invoke.py#L9-L40" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class deob_ps1_invoke(Deobfuscator):
    def deobfuscate(self, data):
        strlit = Ps1StringLiterals(data)

        @strlit.outside
        def invrepl1(m): return m[1] + m[3]

        data = re.sub(
            R&#39;&#39;&#39;(\.|::)&#39;&#39;&#39;                    # preceeded by dot or namespace delimiter
            R&#39;&#39;&#39;([&#39;&#34;])(\w{1,200})\2&#39;&#39;&#39;        # quoted string (actually a method name)
            R&#39;&#39;&#39;(?=[\s\(\.\,\;\+\-])&#39;&#39;&#39;,      # only if followed by certain characters
            invrepl1, data                    # remove quotes around symbol
        )

        @strlit.outside
        def invrepl2(m): return m[1] + &#39;(&#39;

        data = re.sub(
            &#39;\\s{0,5}&#39;.join([
                &#39;[.&amp;]&#39;, &#39;(\\(&#39;,               # sourcing operator
                &#39;(?:gcm|get-command)&#39;, &#39;)?&#39;,  # potentially a get-command
                &#39;([\&#39;&#34;])([-a-z]{1,100})\\2&#39;   # string enclosing a command
                &#39;(?(1)\\s{0,5}\\)|)&#39;,         # closing bracket for get-command
            ]), &#39;\\3&#39;, data, flags=re.IGNORECASE
        )
        data = re.sub(
            R&#39;&#39;&#39;(\w{1,200})\.Invoke\s*\(&#39;&#39;&#39;,
            invrepl2, data,
            flags=re.IGNORECASE
        )

        return data</code></pre>
</details>
</dd>
<dt id="refinery.shell.deob_ps1_secstr"><code class="flex name class">
<span>class <span class="ident">deob_ps1_secstr</span></span>
<span>(</span><span>*a)</span>
</code></dt>
<dd>
<section class="desc"></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/obfuscation/ps1/securestring.py#L11-L69" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class deob_ps1_secstr(Deobfuscator):
    def __init__(self, *a, **kw):
        super().__init__(*a, **kw)

        self._pack = pack()
        self._secstr = secstr()

        self._pattern = re.compile(
            R&#39;\s{{0,20}}&#39;.join([
                R&#39;&#39;&#39;([&#39;&#34;])({b})\1&#39;&#39;&#39;,
                R&#39;\|&#39;, R&#39;\.?&#39;, R&#39;&amp;?&#39;,
                R&#39;&#39;&#39;([&#39;&#34;]?)ConvertTo-SecureString\3&#39;&#39;&#39;,
                R&#39;-ke?y?&#39;,
                R&#39;&#39;&#39;(\(?)({a}|{i}\s{{0,20}}\.\.\s{{0,20}}{i})&#39;&#39;&#39;,
                R&#39;((?:\)\s{{0,20}}){{0,10}})?&#39;
            ]).format(
                b=formats.b64,
                a=formats.intarray,
                i=formats.integer
            ),
            flags=re.IGNORECASE | re.DOTALL
        )

    def _decrypt_block(self, data, match):
        if &#39;..&#39; in match[5]:
            a, b = [int(x.strip(), 0) for x in match[5].split(&#39;..&#39;)]
            key = range(min(a, b), max(a, b) + 1)
            if a &gt; b:
                key = reversed(key)
            self._secstr.args.key = bytes(bytearray(key))
        else:
            self._secstr.args.key = self._pack(match[5].encode(self.codec))
        decoded = self._secstr(match[2].encode(self.codec))
        decoded = decoded.decode(self.codec)
        result = F&#39;\n\n{decoded}\n\n&#39;
        brackets = match[6].count(&#39;)&#39;)
        start = match.start()
        if match[4]:
            brackets -= 1
        if brackets &lt;= 0:
            if brackets &lt; 0:
                result += &#39;)&#39;
            return start, result
        while brackets:
            start -= 1
            if data[start] == &#39;(&#39;:
                brackets -= 1
            if data[start] == &#39;)&#39;:
                brackets += 1
        return start, result

    def deobfuscate(self, data):
        while True:
            match = self._pattern.search(data)
            if not match:
                break
            start, result = self._decrypt_block(data, match)
            data = data[:start] + result + data[match.end():]
        return data</code></pre>
</details>
</dd>
<dt id="refinery.shell.deob_ps1_stringreplace"><code class="flex name class">
<span>class <span class="ident">deob_ps1_stringreplace</span></span>
</code></dt>
<dd>
<section class="desc"></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/obfuscation/ps1/stringreplace.py#L11-L83" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class deob_ps1_stringreplace(Deobfuscator):

    _SENTINEL = re.compile((
        R&#39;(?i)[\&#39;&#34;]\s*&#39;               # end of haystack string
        R&#39;(-c|-i|-|\.)replace&#39;        # the replace call
        R&#39;([\(\s]*)({s})([\)\s]*),&#39;   # needle (with brackets)
        R&#39;([\(\s]*)({s})([\)\s]*)&#39;    # insert (with brackets)
    ).format(s=formats.ps1str), flags=re.IGNORECASE)

    def deobfuscate(self, data):
        repeat = True
        strlit = Ps1StringLiterals(data)

        while repeat:
            repeat = False
            needle = None

            for match in self._SENTINEL.finditer(data):
                k = strlit.get_container(match.start())
                if k is None:
                    continue
                offset, end = strlit.ranges[k]
                if match.start() != end - 1:
                    continue
                string = data[offset:end]
                pf, bl1, needle, bl2, br1, insert, br2 = match.groups()
                end = match.end()
                case = &#39;&#39; if pf[0] in &#39;.c&#39; else &#39;(?i)&#39;
                bl = bl1.count(&#39;(&#39;) - bl2.count(&#39;)&#39;)
                br = br2.count(&#39;)&#39;) - br1.count(&#39;(&#39;)
                if pf[0] == &#39;.&#39;:
                    bl -= 1
                    br -= 1
                if bl != 0 or br &lt; 0:
                    continue
                needle = list(string_unquote(needle))
                if len(needle) &gt; 1:
                    continue

                needle = needle[0]
                head, *body = string_unquote(insert)

                self.log_info(&#39;replacing&#39;, needle, &#39;by&#39;, insert)

                if not body:
                    def perform_replacement(string):
                        return re.sub(F&#39;{case}{re.escape(needle)}&#39;, lambda _: head, string)
                else:
                    *body, tail = body
                    def perform_replacement(string): # noqa
                        parts = re.split(F&#39;{case}{re.escape(needle)}&#39;, string)
                        if len(parts) == 1:
                            yield string
                            return
                        it = iter(parts)
                        yield next(it) + head
                        yield from body
                        for last, part in lookahead(it):
                            if last:
                                yield tail + part
                            else:
                                yield tail + part + head
                                yield from body

                replaced = string_apply(string, perform_replacement) + (br * &#39;)&#39;)
                strlit.ranges[k] = offset, offset + len(replaced) - br
                strlit.ranges[k + 1: k + 3] = []
                strlit.shift(len(replaced) + offset - end, k + 1)
                data = data[:offset] + replaced + data[end:]
                repeat = True
                break

        return data</code></pre>
</details>
</dd>
<dt id="refinery.shell.deob_ps1_typecast"><code class="flex name class">
<span>class <span class="ident">deob_ps1_typecast</span></span>
</code></dt>
<dd>
<section class="desc"><p>Replaces sequences like [Char]120 to their string representation, in this
case the string "x".</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/obfuscation/ps1/typecast.py#L11-L68" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class deob_ps1_typecast(Deobfuscator):
    &#34;&#34;&#34;
    Replaces sequences like [Char]120 to their string representation, in this
    case the string &#34;x&#34;.
    &#34;&#34;&#34;

    def deobfuscate(self, data):
        strlit = Ps1StringLiterals(data)

        @strlit.outside
        def strip_typecast(m): return m[1]

        data = re.sub(
            FR&#39;\[(?:string|char\[\])\]\s*({formats.ps1str!s})&#39;,
            strip_typecast,
            data,
            flags=re.IGNORECASE
        )

        @strlit.outside
        def char_literal(match):
            c = chr(int(match[1].lower(), 0))
            if c == &#34;&#39;&#34;:
                return &#39;&#39;&#39;&#34;&#39;&#34;&#39;&#39;&#39;
            return F&#34;&#39;{c}&#39;&#34;

        data = re.sub(
            R&#39;\[char\]\s*0*(0x[0-9a-f]+|\d+)&#39;,
            char_literal,
            data,
            flags=re.IGNORECASE
        )

        def char_array(match):
            result = bytes(int(x, 0) for x in match[1].split(&#39;,&#39;))
            try:
                result = result.decode(&#39;ascii&#39;)
                if not all(x in string.printable or x.isspace() for x in result):
                    raise ValueError
            except ValueError:
                return match[0]
            else:
                return string_quote(result)

        data = re.sub(
            R&#39;\s*&#39;.join([
                R&#39;\[char\[\]\]&#39;,
                R&#39;\((&#39;,
                R&#39;(?:\s*(?:0x[0-9a-f]+|\d+)\s*,)+&#39;,
                R&#39;(?:0x[0-9a-f]+|\d+)&#39;,
                R&#39;)\)&#39;
            ]),
            char_array,
            data,
            flags=re.IGNORECASE
        )

        return data</code></pre>
</details>
</dd>
<dt id="refinery.shell.deob_ps1_uncurly"><code class="flex name class">
<span>class <span class="ident">deob_ps1_uncurly</span></span>
</code></dt>
<dd>
<section class="desc"><p>PowerShell deobfuscation that removes superfluous curly braces around variable
names that do not require it, i.e. <code>${variable}</code> is transformed to just <code>$variable</code>.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/obfuscation/ps1/uncurly.py#L9-L21" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class deob_ps1_uncurly(Deobfuscator):
    &#34;&#34;&#34;
    PowerShell deobfuscation that removes superfluous curly braces around variable
    names that do not require it, i.e. `${variable}` is transformed to just `$variable`.
    &#34;&#34;&#34;

    _SENTINEL = re.compile(R&#39;\$\{(\w+)\}&#39;)

    def deobfuscate(self, data):
        strlit = Ps1StringLiterals(data)
        @strlit.outside
        def strip(m): return F&#39;${m[1]}&#39;
        return self._SENTINEL.sub(strip, data)</code></pre>
</details>
</dd>
<dt id="refinery.shell.deob_vba"><code class="flex name class">
<span>class <span class="ident">deob_vba</span></span>
<span>(</span><span>timeout=100)</span>
</code></dt>
<dd>
<section class="desc"></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/obfuscation/vba/all.py#L25-L49" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class deob_vba(IterativeDeobfuscator):

    _SUBUNITS: List[Type[Deobfuscator]] = [
        deob_vba_comments,
        deob_vba_brackets,
        deob_vba_char_function,
        deob_vba_concat,
        deob_vba_arithmetic,
        deob_vba_constants,
        deob_vba_dummy_variables,
        deob_vba_stringreplace,
        deob_vba_stringreverse,
    ]

    def deobfuscate(self, data):
        units = [u() for u in self._SUBUNITS]
        for u in units:
            u.log_level = self.log_level
        for unit in units:
            self.log_debug(lambda: F&#39;invoking {unit.name}&#39;)
            checkpoint = hash(data)
            data = unit.deobfuscate(data)
            if checkpoint != hash(data) and not self.log_debug(&#39;data has changed.&#39;):
                self.log_info(F&#39;used {unit.name}&#39;)
        return re.sub(R&#39;[\r\n]+&#39;, &#39;\n&#39;, data)</code></pre>
</details>
</dd>
<dt id="refinery.shell.deob_vba_arithmetic"><code class="flex name class">
<span>class <span class="ident">deob_vba_arithmetic</span></span>
</code></dt>
<dd>
<section class="desc"></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/obfuscation/vba/arithmetic.py#L16-L90" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class deob_vba_arithmetic(Deobfuscator):
    def deobfuscate(self, data):
        strings = StringLiterals(formats.vbastr, data)

        def vba_int_eval(match: re.Match[str]) -&gt; str:
            s = match[0].lower()
            if not s.startswith(&#39;&amp;&#39;):
                return s
            t, s = s[1], s[2:].rstrip(&#39;&amp;&#39;)
            if t == &#39;h&#39;:
                return str(int(s, 16))
            if t == &#39;b&#39;:
                return str(int(s, 2))
            if t == &#39;o&#39;:
                return str(int(s, 8))

        @strings.outside
        def evaluate(match: re.Match[str]):
            expression = match[0]
            expression = expression.strip()
            if not any(c.isdigit() for c in expression):
                return expression
            expression = re.sub(str(formats.vbaint), vba_int_eval, expression)
            brackets = 0
            positions = []
            ok = True
            head = tail = rest = &#39;&#39;
            for end, character in enumerate(expression):
                if character == &#39;(&#39;:
                    brackets += 1
                    positions.append(end)
                    continue
                if character == &#39;)&#39;:
                    brackets -= 1
                    if brackets &lt; 0:
                        expression, tail = expression[:end], expression[end:]
                        break
                    else:
                        positions.pop()
                    if brackets == 0 and expression[0] == &#39;(&#39;:
                        expression, rest = expression[:end + 1], expression[end + 1:]
                        break
            if expression.isdigit():
                return match[0]
            if brackets &gt; 0:
                pos = positions[~0] + 1
                head = expression[:pos]
                expression = expression[pos:]
            try:
                result = str(_cautious_vba_eval(expression + rest))
            except Exception:
                ok = False
            else:
                rest = &#39;&#39;
            if not ok and rest:
                try:
                    result = str(_cautious_vba_eval(expression))
                except Exception:
                    expression += rest
                else:
                    ok = True
            if not ok:
                result = expression
                self.log_info(F&#39;error trying to parse arithmetic expression at offset {match.start()}: ({expression})&#39;)
            else:
                if expression.startswith(&#39;(&#39;) and expression.endswith(&#39;)&#39;):
                    result = F&#39;({result})&#39;
            if tail:
                tail = self.deobfuscate(tail)
            return F&#39;{head}{result}{rest}{tail}&#39;

        pattern = re.compile(R&#39;(?:{i}|{f}|[-+(])(?:[^\S\r\n]{{0,20}}(?:{i}|{f}|[-%|&amp;~&lt;&gt;()+/*^]))+&#39;.format(
            i=str(formats.vbaint), f=str(formats.float)))

        return pattern.sub(evaluate, data)</code></pre>
</details>
</dd>
<dt id="refinery.shell.deob_vba_brackets"><code class="flex name class">
<span>class <span class="ident">deob_vba_brackets</span></span>
</code></dt>
<dd>
<section class="desc"></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/obfuscation/vba/brackets.py#L9-L31" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class deob_vba_brackets(Deobfuscator):
    _SENTINEL = re.compile(
        RF&#39;&#39;&#39;(?&lt;![\w&#34;&#39;]{{2}})&#39;&#39;&#39;  # this may be a function call
        RF&#39;&#39;&#39;\(\s*({formats.vbaint}|{formats.vbastr}|{formats.float})\s*(\S)&#39;&#39;&#39;,
        flags=re.IGNORECASE
    )

    def deobfuscate(self, data):
        strlit = StringLiterals(formats.vbastr, data)
        repeat = True

        @strlit.outside
        def replacement(match):
            nonlocal repeat
            if match[2] == &#39;)&#39;:
                repeat = True
                return match[1]

        while repeat:
            repeat = False
            data = self._SENTINEL.sub(replacement, data)

        return data</code></pre>
</details>
</dd>
<dt id="refinery.shell.deob_vba_char_function"><code class="flex name class">
<span>class <span class="ident">deob_vba_char_function</span></span>
</code></dt>
<dd>
<section class="desc"></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/obfuscation/vba/char.py#L11-L30" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class deob_vba_char_function(Deobfuscator):
    def deobfuscate(self, data):
        strings = StringLiterals(formats.vbastr, data)

        @strings.outside
        def evaluate_char_function(match: re.Match[str]):
            try:
                c = chr(int(match[1]))
            except ValueError:
                return match[0]
            if c == &#39;&#34;&#39;:
                return &#39;&#34;&#34;&#34;&#34;&#39;
            if c == &#39;\\&#39;:
                return &#39;&#34;\\&#34;&#39;
            c = repr(c)[1:-1]
            if len(c) &gt; 1:
                return match[0]
            return &#39;&#34;{}&#34;&#39;.format(c)

        return re.sub(R&#39;(?i)\bchrw?\s*\(\s*(\d+)\s*\)&#39;, evaluate_char_function, data)</code></pre>
</details>
</dd>
<dt id="refinery.shell.deob_vba_chr_literals"><code class="flex name class">
<span>class <span class="ident">deob_vba_chr_literals</span></span>
</code></dt>
<dd>
<section class="desc"></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/obfuscation/vba/vba.py#L8-L17" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class deob_vba_chr_literals(Unit):
    def process(self, data):
        def _chr(m):
            code = int(m[1], 0)
            if code == 34:
                return B&#39;&#34;&#34;&#34;&#34;&#39;
            return B&#39;&#34;%s&#34;&#39; % chr(code).encode(&#39;unicode_escape&#39;)
        data = re.sub(BR&#39;Chr\((\d+x?\d+)\)&#39;, _chr, data, flags=re.IGNORECASE)
        data = re.sub(BR&#39;&#34;\s*\&amp;\s*&#34;&#39;, B&#39;&#39;, data)
        return data</code></pre>
</details>
</dd>
<dt id="refinery.shell.deob_vba_comments"><code class="flex name class">
<span>class <span class="ident">deob_vba_comments</span></span>
</code></dt>
<dd>
<section class="desc"></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/obfuscation/vba/comments.py#L8-L10" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class deob_vba_comments(Deobfuscator):
    def deobfuscate(self, data):
        return re.sub(R&#34;(?im)^\s{0,20}(?:&#39;|rem\b|dim\b).*(?:\Z|$\n\r?)&#34;, &#39;&#39;, data)</code></pre>
</details>
</dd>
<dt id="refinery.shell.deob_vba_concat"><code class="flex name class">
<span>class <span class="ident">deob_vba_concat</span></span>
<span>(</span><span>timeout=100)</span>
</code></dt>
<dd>
<section class="desc"></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/obfuscation/vba/concat.py#L10-L37" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class deob_vba_concat(IterativeDeobfuscator):
    _SENTINEL = re.compile(R&#39;&#39;&#39;&#34;\s*(\++|&amp;)\s*&#34;&#39;&#39;&#39;)

    def deobfuscate(self, data):

        def concat(data):
            strlit = StringLiterals(formats.vbastr, data)
            repeat = True
            while repeat:
                for match in self._SENTINEL.finditer(data):
                    a, b = match.span()
                    a = strlit.get_container(a)
                    if a is None:
                        continue
                    b = strlit.get_container(b)
                    if b is None or b != a + 1:
                        continue
                    _, a = strlit.ranges[a]
                    b, c = strlit.ranges[b]
                    yield data[:a - 1] + data[b + 1:c]
                    data = data[c:]
                    strlit.update(data)
                    break
                else:
                    repeat = False
            yield data

        return &#39;&#39;.join(concat(data))</code></pre>
</details>
</dd>
<dt id="refinery.shell.deob_vba_constants"><code class="flex name class">
<span>class <span class="ident">deob_vba_constants</span></span>
</code></dt>
<dd>
<section class="desc"></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/obfuscation/vba/constants.py#L9-L41" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class deob_vba_constants(Deobfuscator):
    def deobfuscate(self, data):
        codelines = data.splitlines(keepends=True)
        constants = {}
        constline = {}
        variables = set()
        for k, line in enumerate(codelines):
            match = re.match(R&#39;(?im)^\s*(?:sub|function)\s*(\w+)&#39;, line)
            if match:
                variables.add(match[1])
                continue
            match = re.match(
                R&#39;(?im)^(?:\s*const)?\s*(\w+)\s*=\s*({i}|{s})\s*(?:\&#39;|rem|$)&#39;.format(
                    s=formats.ps1str,
                    i=formats.integer
                ), line)
            if match is None or match[1] in variables:
                pass
            elif match[2] != constants.get(match[1], match[2]):
                self.log_debug(F&#39;del {match[1]}&#39;)
                del constants[match[1]]
                del constline[match[1]]
                variables.add(match[1])
            else:
                self.log_debug(F&#39;add {match[1]} = {match[2]}&#39;)
                constants[match[1]] = match[2]
                constline[match[1]] = k
        codelines = [line for k, line in enumerate(codelines) if k not in constline.values()]
        data = &#39;&#39;.join(codelines)
        for name, value in constants.items():
            data = re.sub(RF&#39;\b{re.escape(name)!s}\b&#39;, lambda _: value, data)

        return data</code></pre>
</details>
</dd>
<dt id="refinery.shell.deob_vba_dummy_variables"><code class="flex name class">
<span>class <span class="ident">deob_vba_dummy_variables</span></span>
</code></dt>
<dd>
<section class="desc"></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/obfuscation/vba/dummies.py#L10-L57" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class deob_vba_dummy_variables(Deobfuscator):
    def deobfuscate(self, data):
        lines = data.splitlines(keepends=False)
        names = collections.defaultdict(list)

        def might_be_used_in(name, line):
            # avoid finding the name within a string literal
            line = &#39;&#34;&#34;&#39;.join(re.split(str(formats.ps1str), line))
            line = re.split(RF&#39;\b{name}\b&#39;, line)
            try:
                L, R = line
            except ValueError:
                return False
            L = L.strip().lower()
            if L.startswith(&#34;&#39;&#34;) or L.startswith(&#39;rem&#39;):
                return False
            R = R.strip().lower()
            if R.startswith(&#39;=&#39;) and &#39;if&#39; not in L:
                return False
            if L.startswith(&#39;dim&#39;):
                return False
            return True

        pattern = re.compile(
            R&#39;(?i)^\s{0,8}(?:const\s{1,8})?(\w+)\s{1,8}=\s{1,8}.*$&#39;
        )

        for k, line in enumerate(lines):
            try:
                name = pattern.match(line)[1]
            except (AttributeError, TypeError):
                continue
            if re.search(r&#39;\w+\(&#39;, line):
                # might be a function call
                continue
            names[name].append(k)

        for line in lines:
            while True:
                for name in names:
                    if might_be_used_in(name, line):
                        del names[name]
                        break
                else:
                    break

        return &#39;\n&#39;.join(line for k, line in enumerate(lines) if not any(
            k in rows for rows in names.values()))</code></pre>
</details>
</dd>
<dt id="refinery.shell.deob_vba_stringreplace"><code class="flex name class">
<span>class <span class="ident">deob_vba_stringreplace</span></span>
</code></dt>
<dd>
<section class="desc"></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/obfuscation/vba/stringreplace.py#L11-L32" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class deob_vba_stringreplace(Deobfuscator):

    _SENTINEL = re.compile((
        R&#39;(?i)\bReplace\s*\(&#39;  # the replace call
        R&#39;\s*({s}),&#39;           # haystack (with brackets)
        R&#39;\s*({s}),&#39;           # needle (with brackets)
        R&#39;\s*({s})\s*\)&#39;       # insert (with brackets)
    ).format(s=formats.vbastr), flags=re.IGNORECASE)

    def deobfuscate(self, data):
        strlit = StringLiterals(formats.vbastr, data)

        @strlit.outside
        def replacement(match: re.Match[str]):
            return string_quote(
                string_unquote(match[1]).replace(
                    string_unquote(match[2]),
                    string_unquote(match[3])
                )
            )

        return self._SENTINEL.sub(replacement, data)</code></pre>
</details>
</dd>
<dt id="refinery.shell.deob_vba_stringreverse"><code class="flex name class">
<span>class <span class="ident">deob_vba_stringreverse</span></span>
</code></dt>
<dd>
<section class="desc"></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/obfuscation/vba/stringreverse.py#L11-L25" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class deob_vba_stringreverse(Deobfuscator):

    _SENTINEL = re.compile((
        R&#39;(?i)\bStrReverse\s*\(&#39;  # the reverse call
        R&#39;\s*({s})\s*\)&#39;          # string
    ).format(s=formats.vbastr), flags=re.IGNORECASE)

    def deobfuscate(self, data):
        strlit = StringLiterals(formats.vbastr, data)

        @strlit.outside
        def replacement(match: re.Match[str]):
            return string_quote(&#39;&#39;.join(reversed(string_unquote(match[1]))))

        return self._SENTINEL.sub(replacement, data)</code></pre>
</details>
</dd>
<dt id="refinery.shell.des"><code class="flex name class">
<span>class <span class="ident">des</span></span>
<span>(</span><span>key, iv=b'', *, padding=None, mode=None, raw=False, little_endian=False, segment_size=0, mac_len=0, assoc_len=0)</span>
</code></dt>
<dd>
<section class="desc"><p>DES encryption and decryption.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/crypto/cipher/des.py#L9-L13" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class des(StandardBlockCipherUnit, cipher=PyCryptoFactoryWrapper(DES)):
    &#34;&#34;&#34;
    DES encryption and decryption.
    &#34;&#34;&#34;
    pass</code></pre>
</details>
</dd>
<dt id="refinery.shell.des3"><code class="flex name class">
<span>class <span class="ident">des3</span></span>
<span>(</span><span>key, iv=b'', *, padding=None, mode=None, raw=False, little_endian=False, segment_size=0, mac_len=0, assoc_len=0)</span>
</code></dt>
<dd>
<section class="desc"><p>3-DES encryption and decryption.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/crypto/cipher/des3.py#L9-L13" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class des3(StandardBlockCipherUnit, cipher=PyCryptoFactoryWrapper(DES3)):
    &#34;&#34;&#34;
    3-DES encryption and decryption.
    &#34;&#34;&#34;
    pass</code></pre>
</details>
</dd>
<dt id="refinery.shell.deskd"><code class="flex name class">
<span>class <span class="ident">deskd</span></span>
<span>(</span><span>size=8)</span>
</code></dt>
<dd>
<section class="desc"><p>Stands for "DES Key Derivation". It implements the same functionality as <code>DES_string_to_key</code> in OpenSSL. It
converts a string to an 8 byte DES key with odd byte parity, per FIPS specification. This is not a modern
key derivation function.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/crypto/keyderive/deskd.py#L47-L84" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class deskd(KeyDerivation):
    &#34;&#34;&#34;
    Stands for &#34;DES Key Derivation&#34;. It implements the same functionality as `DES_string_to_key` in OpenSSL. It
    converts a string to an 8 byte DES key with odd byte parity, per FIPS specification. This is not a modern
    key derivation function.
    &#34;&#34;&#34;
    def __init__(self, size: Arg(help=&#39;The number of bytes to generate, default is the maximum of 8.&#39;) = 8):
        super().__init__(size=size, salt=None)

    def process(self, password):
        from Cryptodome.Cipher import DES
        from Cryptodome.Util.strxor import strxor

        key = bytearray(8)

        for i, j in enumerate(password):
            if ((i % 16) &lt; 8):
                key[i % 8] ^= (j &lt;&lt; 1) &amp; 0xFF
            else:
                j = (((j &lt;&lt; 4) &amp; 0xf0) | ((j &gt;&gt; 4) &amp; 0x0f))
                j = (((j &lt;&lt; 2) &amp; 0xcc) | ((j &gt;&gt; 2) &amp; 0x33))
                j = (((j &lt;&lt; 1) &amp; 0xaa) | ((j &gt;&gt; 1) &amp; 0x55))
                key[7 - (i % 8)] ^= j

        des_set_odd_parity(key)

        if password:
            n = len(password)
            password = password.ljust(n + 7 - ((n - 1) % 8), b&#39;\0&#39;)
            des = DES.new(key, DES.MODE_ECB)
            for k in range(0, n, 8):
                key[:] = des.encrypt(strxor(password[k:k + 8], key))
            des_set_odd_parity(key)

        if self.args.size &gt; 8:
            raise RefineryPartialResult(&#39;can provide at most 8 bytes.&#39;, partial=key)

        return key[:self.args.size]</code></pre>
</details>
</dd>
<dt id="refinery.shell.dexstr"><code class="flex name class">
<span>class <span class="ident">dexstr</span></span>
</code></dt>
<dd>
<section class="desc"><p>Extract strings from DEX (Dalvik Executable) files.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/formats/dexstr.py#L7-L14" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class dexstr(Unit):
    &#34;&#34;&#34;
    Extract strings from DEX (Dalvik Executable) files.
    &#34;&#34;&#34;
    def process(self, data):
        dex = DexFile(data)
        for string in dex.read_strings():
            yield string.encode(self.codec)</code></pre>
</details>
</dd>
<dt id="refinery.shell.dnarrays"><code class="flex name class">
<span>class <span class="ident">dnarrays</span></span>
</code></dt>
<dd>
<section class="desc"><p>Extracts arrays of strings or integers that are encoded in the .NET binary as IL opcodes.
The data is exported as JSON.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/formats/pe/dotnet/dnarrays.py#L15-L155" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class dnarrays(Unit):
    &#34;&#34;&#34;
    Extracts arrays of strings or integers that are encoded in the .NET binary as IL opcodes.
    The data is exported as JSON.
    &#34;&#34;&#34;
    @staticmethod
    def _read_int(reader: StructReader):
        value = reader.read_byte() - 0x16
        if value &lt; 0:
            raise ValueError
        elif value &lt;= 8:
            return value
        elif value == 9:
            return reader.read_byte()
        elif value == 10:
            return reader.u32()
        else:
            raise ValueError

    @staticmethod
    def _read_str(reader: StructReader, header: DotNetHeader):
        if reader.read_byte() != 0x72:
            raise ValueError
        token: int = reader.read_integer(24)
        value: str = header.meta.Streams.US[token]
        if reader.read_byte() != 0x70:
            raise ValueError
        return value

    _STACK_ARRAY_PATTERN_STR = re.compile(
        BR&#39;&#39;&#39;(?x)
        (?: [\x16-\x1E]|\x1F.|\x20.{4} ) # load array length
        (?:  \x8D...\x01               ) # newarr System.String
        (?:
        (?:  \x25                      ) # dup
        (?: [\x16-\x1E]|\x1F.|\x20.{4} ) # load integer index
        (?:  \x72...\x70               ) # load the string
        (?:  \xA2                      ) # stelem.ref
        ){4,}
        &#39;&#39;&#39;, flags=re.DOTALL)

    def _str_arrays(self, data: ByteStr, header: DotNetHeader, tables: NetMetaDataTables):
        for match in self._STACK_ARRAY_PATTERN_STR.finditer(data):
            reader = StructReader(match[0])
            result: list[str] = []
            size = self._read_int(reader)
            if reader.read_byte() != 0x8D:
                raise RuntimeError
            stt = reader.read_integer(24)
            if reader.read_byte() != 0x01:
                raise RuntimeError
            if stt &lt; 1 or tables.TypeRef[stt - 1].TypeName != &#39;String&#39;:
                continue
            self.log_info(F&#39;str array pattern at 0x{match.start():X}, size {size}&#39;)
            for k in range(size):
                if reader.read_byte() != 0x25:
                    raise RuntimeError
                if self._read_int(reader) != k:
                    break
                result.append(self._read_str(reader, header))
                if reader.read_byte() != 0xA2:
                    raise RuntimeError
            else:
                yield match.start(), result

    _STACK_ARRAY_PATTERN_INT = re.compile(
        BR&#39;&#39;&#39;(?x)
        (    \x12.|\xFE\x0D..          ) # load array variable
        (?: [\x16-\x1E]|\x1F.|\x20.{4} ) # push integer value
        (?:  \x52                      ) # store value into array
        (?:
        (?:  \1                        ) # load same array variable
        (?: [\x16-\x1E]|\x1F.|\x20.{4} ) # load integer index
        (?:  \x58                      ) # add; compute offset
        (?: [\x16-\x1E]|\x1F.|\x20.{4} ) # push integer value
        (?:  \x52                      ) # store value into array
        ){4,}
        &#39;&#39;&#39;, flags=re.DOTALL)

    def _int_arrays(self, data: ByteStr, header: DotNetHeader, tables: NetMetaDataTables):
        for match in self._STACK_ARRAY_PATTERN_INT.finditer(data):
            self.log_info(F&#39;int array pattern at 0x{match.start():X}&#39;)
            reader = StructReader(match[0])
            result: list[int] = []
            opc, = reader.peek(1)
            skip = {0x12: 2, 0xFE: 4}[opc]
            reader.seekrel(skip)
            for index in itertools.count(1):
                result.append(self._read_int(reader))
                assert reader.read_byte() == 0x52
                if reader.eof:
                    yield match.start(), result
                    break
                reader.seekrel(skip)
                if self._read_int(reader) != index:
                    self.log_info(&#39;index inconsistency; aborting&#39;)
                    break
                assert reader.read_byte() == 0x58

    def process(self, data):
        def printable(name: str):
            return name.replace(&#39;.&#39;, &#39;&#39;).isidentifier()

        @functools.lru_cache(maxsize=None)
        def method(offset: int):
            rva = header.pe.get_rva_from_offset(offset)
            method = min(tables.MethodDef, key=lambda m: (m.RVA &gt; rva, rva - m.RVA))
            index = tables.MethodDef.index(method)
            method_name = method.Name
            if not printable(method_name):
                method_name = F&#39;method_{method.RVA:08X}&#39;
            for k, (methods, tr) in enumerate(zip(methods_of_type, tables.TypeDef)):
                if index in methods:
                    namespace = tr.TypeNamespace
                    type_name = tr.TypeName
                    if printable(type_name):
                        spec = F&#39;type{k}::{method_name}&#39;
                    else:
                        spec = F&#39;{type_name}::{method_name}&#39;
                    if printable(namespace):
                        spec = F&#39;{namespace}::{spec}&#39;
                    return spec
            return method_name

        header = DotNetHeader(data)
        tables = header.meta.Streams.Tables

        methods_of_type = [tr.MethodList.Index - 1 for tr in tables.TypeDef]
        methods_of_type.append(len(tables.MethodDef))
        methods_of_type = [range(*methods_of_type[k:k + 2]) for k in range(len(methods_of_type) - 1)]

        arrays = dict(itertools.chain(
            self._int_arrays(data, header, tables),
            self._str_arrays(data, header, tables),
        ))
        result = collections.defaultdict(list)
        for offset in sorted(arrays):
            result[method(offset)].append(arrays[offset])

        result = {m: {F&#39;v{k}&#39;: v for k, v in enumerate(t, 1)} for m, t in result.items()}
        return json.dumps(result, indent=4).encode(self.codec)</code></pre>
</details>
</dd>
<dt id="refinery.shell.dnblob"><code class="flex name class">
<span>class <span class="ident">dnblob</span></span>
</code></dt>
<dd>
<section class="desc"><p>Extracts all blobs defined in the <code>#Blob</code> stream of .NET executables.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/formats/pe/dotnet/dnblob.py#L7-L14" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class dnblob(Unit):
    &#34;&#34;&#34;
    Extracts all blobs defined in the `#Blob` stream of .NET executables.
    &#34;&#34;&#34;
    def process(self, data):
        header = DotNetHeader(data, parse_resources=False)
        for blob in header.meta.Streams.Blob.values():
            yield blob</code></pre>
</details>
</dd>
<dt id="refinery.shell.dncfx"><code class="flex name class">
<span>class <span class="ident">dncfx</span></span>
</code></dt>
<dd>
<section class="desc"><p>Extracts the encrypted strings from ConfuserX protected .NET execuctables.
Each decrypted string is returned as a single output.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/formats/pe/dotnet/dncfx.py#L14-L109" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class dncfx(Unit):
    &#34;&#34;&#34;
    Extracts the encrypted strings from ConfuserX protected .NET execuctables.
    Each decrypted string is returned as a single output.
    &#34;&#34;&#34;
    _PATTERN_ARRAY_INIT = (
        BR&#39;(\x1F.|\x20....)&#39;      # load size of a chunk
        BR&#39;\x8D.\x00\x00\x01&#39;     # create a UInt32 array
        BR&#39;\x25&#39;                  # dup
        BR&#39;\xD0%s\x04&#39;            # ldtoken: RVA of array data
        BR&#39;\x28.\x00\x00.&#39;        # call to InitializeArray
    )

    def process(self, data):
        header = DotNetHeader(data, parse_resources=False)
        decompressor = lzma()

        class IntegerAssignment:
            def __init__(self, match):
                self.offset = match.start()
                self.value, = struct.unpack(&#39;&lt;I&#39;, match[1])

        def get_size(match):
            ins = match[1]
            fmt = &#39;&lt;B&#39; if ins[0] == 0x1F else &#39;&lt;I&#39;
            result, = struct.unpack(fmt, ins[-struct.calcsize(fmt):])
            return result

        potential_seeds = [
            IntegerAssignment(m)
            for m in re.finditer(br&#39;\x20(....)&#39;, data, re.DOTALL)
        ]

        for entry in header.meta.RVAs:
            offset = header.pe.get_offset_from_rva(entry.RVA)
            index = struct.pack(&#39;&lt;I&#39;, entry.Field.Index)
            strings_found = 0
            for match in re.finditer(self._PATTERN_ARRAY_INIT % re.escape(index[:3]), data, flags=re.DOTALL):
                ms = match.start()

                def sortkey(t):
                    weight = abs(t.offset - ms)
                    if t.offset &lt; ms:
                        # this weights assignments after the array initialization down, but still
                        # prefers them over assignments that are further away than 2kb
                        weight += 2000
                    return weight

                size = get_size(match)

                if size % 0x10 or size &gt; 10000:
                    continue

                self.log_debug(F&#39;found RVA {entry.Field.Index} initialized with length {size}.&#39;)
                potential_seeds.sort(key=sortkey)

                for seed in potential_seeds[1:400]:
                    # the first potential_seed will always be the assignment of the size variable
                    ciphertext = data[offset:offset + size * 4]
                    key = self._xs64star(seed.value)
                    key = chunks.pack(key, 4) + ciphertext[:-0x40]
                    decrypted = strxor(key, ciphertext)
                    try:
                        decompressed = decompressor(decrypted)
                    except Exception as e:
                        self.log_debug(
                            F&#39;decompression failed for seed {seed.value:08X} at offset {seed.offset:08X}: {e}&#39;)
                        continue
                    else:
                        self.log_info(
                            F&#39;decompression worked for seed {seed.value:08X} at offset {seed.offset:08X}.&#39;)
                    if len(decompressed) &lt; 0x100:
                        continue
                    for string in self._extract_strings(decompressed):
                        strings_found += 1
                        yield string
                    if strings_found &gt; 10:
                        break

    def _xs64star(self, state):
        for i in range(16):
            state ^= (state &gt;&gt; 12) &amp; 0xFFFFFFFF
            state ^= (state &lt;&lt; 25) &amp; 0xFFFFFFFF
            state ^= (state &gt;&gt; 27) &amp; 0xFFFFFFFF
            yield state &amp; 0xFFFFFFFF

    def _extract_strings(self, blob):
        reader = StreamReader(blob)
        while reader.tell() &lt; len(blob):
            try:
                size = reader.expect(UInt32)
                string = reader.expect(StringPrimitive, size=size, codec=&#39;UTF8&#39;, align=4)
            except ParserEOF:
                return
            if string:
                yield string.encode(self.codec)</code></pre>
</details>
</dd>
<dt id="refinery.shell.dnds"><code class="flex name class">
<span>class <span class="ident">dnds</span></span>
<span>(</span><span>dereference=True, encode=None, digest=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Stands for "DotNet DeSerialize": Expects data that has been serialized using the .NET class
"BinaryFormatter". The output is a representation of the deserialized data in JSON format.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/formats/pe/dotnet/dnds.py#L7-L34" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class dnds(JSONEncoderUnit):
    &#34;&#34;&#34;
    Stands for &#34;DotNet DeSerialize&#34;: Expects data that has been serialized using the .NET class
    &#34;BinaryFormatter&#34;. The output is a representation of the deserialized data in JSON format.
    &#34;&#34;&#34;

    def __init__(
        self, dereference: Arg.Switch(&#39;-r&#39;, &#39;--keep-references&#39;, off=True,
            help=&#39;Do not resolve Object references in serialized data.&#39;) = True,
        encode=None, digest=None
    ):
        super().__init__(encode=encode, digest=digest, dereference=dereference)

    def process(self, data):
        self.log_debug(&#39;initializing parser, will fail on malformed stream&#39;)
        bf = BinaryFormatterParser(
            data,
            keep_meta=True,
            dereference=self.args.dereference,
            ignore_errors=not self.log_debug(),
        )

        return self.to_json([
            {
                &#39;Type&#39;: repr(record),
                &#39;Data&#39;: record
            } for record in bf
        ])</code></pre>
</details>
</dd>
<dt id="refinery.shell.dnfields"><code class="flex name class">
<span>class <span class="ident">dnfields</span></span>
<span>(</span><span>*paths, list=False, join_path=False, drop_path=False, fuzzy=0, exact=False, regex=False, path=b'path')</span>
</code></dt>
<dd>
<section class="desc"><p>This unit can extract data from constant field variables in classes of .NET
executables. Since the .NET header stores only the offset and not the size of
constant fields, heuristics are used to search for opcode sequences that load
the data and additional heuristics are used to guess the size of the data
type.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/formats/pe/dotnet/dnfields.py#L19-L152" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class dnfields(PathExtractorUnit):
    &#34;&#34;&#34;
    This unit can extract data from constant field variables in classes of .NET
    executables. Since the .NET header stores only the offset and not the size of
    constant fields, heuristics are used to search for opcode sequences that load
    the data and additional heuristics are used to guess the size of the data
    type.
    &#34;&#34;&#34;
    _SIZEMAP = {
        &#39;^s?byte$&#39;       : 1,
        &#39;^s?char$&#39;       : 2,
        &#39;^[us]?int.?16$&#39; : 2,
        &#39;^[us]?int.?32$&#39; : 4,
        &#39;^[us]?int.?64$&#39; : 8,
    }

    def _guess_field_info(self, tables, data, t) -&gt; FieldInfo:
        pattern = (
            BR&#39;(\x20....|\x1F.)&#39;                # ldc.i4  count
            BR&#39;\x8D(...)([\x01\x02])&#39;           # newarr  col|row
            BR&#39;\x25&#39;                            # dup
            BR&#39;\xD0\x%02x\x%02x\x%02x\x04&#39;      # ldtoken t
            BR&#39;(?:.{0,12}?&#39;                      # ...
            BR&#39;\x80(...)\x04)?&#39; % (             # stsfld variable
                (t &gt;&gt; 0x00) &amp; 0xFF,
                (t &gt;&gt; 0x08) &amp; 0xFF,
                (t &gt;&gt; 0x10) &amp; 0xFF
            )
        )
        for match in re.finditer(pattern, data, flags=re.DOTALL):
            count, j, r, name = match.groups()
            count, j, r = struct.unpack(&#39;&lt;LLB&#39;, B&#39;%s%s\0%s&#39; % (count[1:].ljust(4, B&#39;\0&#39;), j, r))
            if name:
                try:
                    name = struct.unpack(&#39;&lt;L&#39;, B&#39;%s\0&#39; % name)
                    name = name[0]
                    name = tables[4][name - 1].Name
                except Exception as E:
                    self.log_info(F&#39;attempt to parse field name failed: {E!s}&#39;)
                    name = None
            element = tables[r][j - 1]
            for pattern, size in self._SIZEMAP.items():
                if re.match(pattern, element.TypeName, flags=re.IGNORECASE):
                    return FieldInfo(element.TypeName, count, size, name)

    def unpack(self, data):
        header = DotNetHeader(data, parse_resources=False)
        tables = header.meta.Streams.Tables
        fields = tables.FieldRVA
        if not fields:
            return
        iwidth = len(str(len(fields)))
        rwidth = max(len(F&#39;{field.RVA:X}&#39;) for field in fields)
        rwidth = max(rwidth, 4)
        remaining_field_indices = set(range(len(tables.Field)))

        for k, rv in enumerate(fields):
            _index = rv.Field.Index
            field = tables.Field[_index - 1]
            remaining_field_indices.discard(_index - 1)
            fname = field.Name
            ftype = None
            if len(field.Signature) == 2:
                # Crude signature parser for non-array case. Reference:
                # https://www.codeproject.com/Articles/42649/NET-File-Format-Signatures-Under-the-Hood-Part-1
                # https://www.codeproject.com/Articles/42655/NET-file-format-Signatures-under-the-hood-Part-2
                guess = {
                    0x03: FieldInfo(&#39;Char&#39;,   1, 1, None),  # noqa
                    0x04: FieldInfo(&#39;SByte&#39;,  1, 1, None),  # noqa
                    0x05: FieldInfo(&#39;Byte&#39;,   1, 1, None),  # noqa
                    0x06: FieldInfo(&#39;Int16&#39;,  1, 2, None),  # noqa
                    0x07: FieldInfo(&#39;UInt16&#39;, 1, 2, None),  # noqa
                    0x08: FieldInfo(&#39;Int32&#39;,  1, 4, None),  # noqa
                    0x09: FieldInfo(&#39;UInt32&#39;, 1, 4, None),  # noqa
                    0x0A: FieldInfo(&#39;Int64&#39;,  1, 8, None),  # noqa
                    0x0B: FieldInfo(&#39;UInt64&#39;, 1, 8, None),  # noqa
                    0x0C: FieldInfo(&#39;Single&#39;, 1, 4, None),  # noqa
                    0x0D: FieldInfo(&#39;Double&#39;, 1, 8, None),  # noqa
                }.get(field.Signature[1], None)
            else:
                guess = self._guess_field_info(tables, data, _index)
            if guess is None:
                self.log_debug(lambda: F&#39;field {k:0{iwidth}d} with signature {field.Signature.hex()}: unable to guess type information&#39;)
                continue
            totalsize = guess.count * guess.size
            if guess.name is not None:
                fname = guess.name
            if not fname.isprintable():
                fname = F&#39;F{rv.RVA:0{rwidth}X}&#39;
            ext = ftype = guess.type.lower()
            if guess.count &gt; 1:
                ftype += F&#39;[{guess.count}]&#39;
            self.log_info(
                F&#39;field {k:0{iwidth}d}; token 0x{_index:06X}; RVA 0x{rv.RVA:04X}; count {guess.count}; type {guess.type}; name {fname}&#39;)
            offset = header.pe.get_offset_from_rva(rv.RVA)
            yield UnpackResult(
                F&#39;{fname}.{ext}&#39;,
                lambda t=offset, s=totalsize: data[t:t + s],
                name=fname,
                type=ftype,
            )

        for _index in remaining_field_indices:
            field = tables.Field[_index]
            index = _index + 1
            name = field.Name
            if field.Flags.HasFieldRVA:
                self.log_warn(F&#39;field {name} has RVA flag set, but no RVA was found&#39;)
            token = index.to_bytes(3, &#39;little&#39;)
            values = set()
            for match in re.finditer((
                BR&#39;\x72(?P&lt;token&gt;...)\x70&#39;          # ldstr
                BR&#39;(?:\x6F(?P&lt;function&gt;...)\x0A)?&#39;  # call GetBytes
                BR&#39;\x80%s\x04&#39;                      # stsfld
            ) % re.escape(token), data, re.DOTALL):
                md = match.groupdict()
                fn_token = md.get(&#39;function&#39;)
                fn_index = fn_token and int.from_bytes(fn_token, &#39;little&#39;) or None
                if fn_index is not None:
                    fn_name = tables.MemberRef[fn_index].Name
                    if fn_name != &#39;GetBytes&#39;:
                        self.log_info(F&#39;skipping string assignment passing through call to {fn_name}&#39;)
                        continue
                k = int.from_bytes(md[&#39;token&#39;], &#39;little&#39;)
                values.add(header.meta.Streams.US[k].encode(self.codec))
            if not values:
                continue
            if len(values) == 1:
                yield UnpackResult(
                    F&#39;{name}.str&#39;,
                    next(iter(values)),
                    name=name,
                    type=&#39;string&#39;
                )</code></pre>
</details>
</dd>
<dt id="refinery.shell.dnhdr"><code class="flex name class">
<span>class <span class="ident">dnhdr</span></span>
<span>(</span><span>resources=False, encode=None, digest=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Expects data that has been formatted with the <code>BinaryFormatter</code> class. The
output is a representation of the deserialized data in JSON format.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/formats/pe/dotnet/dnhdr.py#L7-L29" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class dnhdr(JSONEncoderUnit):
    &#34;&#34;&#34;
    Expects data that has been formatted with the `BinaryFormatter` class. The
    output is a representation of the deserialized data in JSON format.
    &#34;&#34;&#34;
    def __init__(
        self,
        resources: Arg.Switch(&#39;-r&#39;, &#39;--resources&#39;, help=&#39;Also parse .NET resources.&#39;) = False,
        encode=None, digest=None
    ):
        super().__init__(encode=encode, digest=digest, resources=resources)

    def process(self, data):
        dn = DotNetHeader(data, parse_resources=self.args.resources)
        dn = {
            &#39;Head&#39;: dn.head,
            &#39;Meta&#39;: dn.meta
        }

        if self.args.resources:
            dn[&#39;RSRC&#39;] = dn.resources

        return self.to_json(dn)</code></pre>
</details>
</dd>
<dt id="refinery.shell.dnmr"><code class="flex name class">
<span>class <span class="ident">dnmr</span></span>
<span>(</span><span>*paths, list=False, join_path=False, drop_path=False, exact=False, fuzzy=0, regex=False, path=b'name', raw=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Extracts subfiles from .NET managed resources.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/formats/pe/dotnet/dnmr.py#L9-L45" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class dnmr(PathExtractorUnit):
    &#34;&#34;&#34;
    Extracts subfiles from .NET managed resources.
    &#34;&#34;&#34;
    def __init__(
        self, *paths, list=False, join_path=False, drop_path=False, exact=False, fuzzy=0, regex=False, path=b&#39;name&#39;,
        raw: Arg.Switch(&#39;-w&#39;, help=&#39;Do not deserialize the managed resource entry data.&#39;) = False
    ):
        super().__init__(
            *paths,
            list=list,
            join_path=join_path,
            drop_path=drop_path,
            path=path,
            raw=raw,
            fuzzy=fuzzy,
            exact=exact,
            regex=regex,
        )

    def unpack(self, data):
        try:
            managed = NetStructuredResources(data)
        except NoManagedResource:
            managed = None
        if not managed:
            raise RefineryPartialResult(&#39;no managed resources found&#39;, partial=data)
        for entry in managed:
            if entry.Error:
                self.log_warn(F&#39;entry {entry.Name} carried error message: {entry.Error}&#39;)
            data = entry.Data
            if not self.args.raw:
                if isinstance(entry.Value, str):
                    data = entry.Value.encode(&#39;utf-16le&#39;)
                elif isbuffer(entry.Value):
                    data = entry.Value
            yield UnpackResult(entry.Name, data)</code></pre>
</details>
</dd>
<dt id="refinery.shell.dnrc"><code class="flex name class">
<span>class <span class="ident">dnrc</span></span>
<span>(</span><span>*paths, list=False, join_path=False, drop_path=False, fuzzy=0, exact=False, regex=False, path=b'path')</span>
</code></dt>
<dd>
<section class="desc"><p>Extracts all .NET resources whose name matches any of the given patterns
and outputs them. Use the <code><a title="refinery.units.formats.pe.dotnet.dnmr" href="units/formats/pe/dotnet/dnmr.html">refinery.units.formats.pe.dotnet.dnmr</a></code> unit to
extract subfiles from managed .NET resources.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/formats/pe/dotnet/dnrc.py#L7-L22" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class dnrc(PathExtractorUnit):
    &#34;&#34;&#34;
    Extracts all .NET resources whose name matches any of the given patterns
    and outputs them. Use the `refinery.units.formats.pe.dotnet.dnmr` unit to
    extract subfiles from managed .NET resources.
    &#34;&#34;&#34;
    def unpack(self, data):
        header = DotNetHeader(data)

        if not header.resources:
            if self.args.list:
                return
            raise ValueError(&#39;This file contains no resources.&#39;)

        for resource in header.resources:
            yield UnpackResult(resource.Name, resource.Data)</code></pre>
</details>
</dd>
<dt id="refinery.shell.dnsdomain"><code class="flex name class">
<span>class <span class="ident">dnsdomain</span></span>
<span>(</span><span>min=1, max=None, len=None, stripspace=False, duplicates=False, longest=False, take=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Extracts domain names in the format as they appear in DNS requests. This
can be used as a quick and dirty way to extract domains from PCAP files,
for example.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/pattern/dnsdomain.py#L13-L43" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class dnsdomain(PatternExtractorBase):
    &#34;&#34;&#34;
    Extracts domain names in the format as they appear in DNS requests. This
    can be used as a quick and dirty way to extract domains from PCAP files,
    for example.
    &#34;&#34;&#34;

    _DOMAIN_CHARACTERS = (
        B&#39;ABCDEFGHIJKLMNOPQRSTUVWXYZ&#39;
        B&#39;abcdefghijklmnopqrstuvwxyz&#39;
        B&#39;0123456789-_&#39;
    )

    _DOMAIN_PATTERN = BR&#39;(?:%s){1,20}(?:%s)\b&#39; % (_lps(0xFF), _lps(25))

    def process(self, data):

        def transform(match):
            match = bytearray(match[0])
            pos = 0
            while pos &lt; len(match):
                length = match[pos]
                match[pos] = 0x2E
                if len(match) &lt; length + pos:
                    return None
                if any(x not in self._DOMAIN_CHARACTERS for x in match[pos + 1 : pos + length]):
                    return None
                pos += 1 + length
            return match[1:]

        yield from self.matches_filtered(memoryview(data), self._DOMAIN_PATTERN, transform)</code></pre>
</details>
</dd>
<dt id="refinery.shell.dnsfx"><code class="flex name class">
<span>class <span class="ident">dnsfx</span></span>
<span>(</span><span>*paths, list=False, join_path=False, drop_path=False, fuzzy=0, exact=False, regex=False, path=b'path')</span>
</code></dt>
<dd>
<section class="desc"><p>Extracts files from .NET single file applications.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/formats/pe/dotnet/dnsfx.py#L16-L85" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class dnsfx(PathExtractorUnit):
    &#34;&#34;&#34;
    Extracts files from .NET single file applications.
    &#34;&#34;&#34;
    _SIGNATURE = bytes([
        # 32 bytes represent the bundle signature: SHA-256 for &#39;.net core bundle&#39;
        0x8b, 0x12, 0x02, 0xb9, 0x6a, 0x61, 0x20, 0x38,
        0x72, 0x7b, 0x93, 0x02, 0x14, 0xd7, 0xa0, 0x32,
        0x13, 0xf5, 0xb9, 0xe6, 0xef, 0xae, 0x33, 0x18,
        0xee, 0x3b, 0x2d, 0xce, 0x24, 0xb3, 0x6a, 0xae
    ])

    def unpack(self, data):
        reader = StreamReader(data)
        reader.seek(self._find_bundle_manifest_offset(data))

        major_version = reader.expect(UInt32)
        minor_version = reader.expect(UInt32)
        self.log_info(F&#39;version {major_version}.{minor_version}&#39;)

        count = reader.expect(UInt32)
        bhash = reader.expect(StringPrimitive)
        self.log_info(F&#39;bundle {bhash} contains {count} files&#39;)

        if major_version &gt;= 2:
            reader.expect(UInt64) # depsOffset
            reader.expect(UInt64) # depsSize
            reader.expect(UInt64) # runtimeConfigOffset
            reader.expect(UInt64) # runtimeConfigSize
            reader.expect(UInt64) # flags

        for _ in range(count):
            try:
                offset = reader.expect(UInt64)
                size = reader.expect(UInt64)
                compressed_size = 0
                if major_version &gt;= 6:
                    compressed_size = reader.expect(UInt64)
                type = reader.expect(Byte)
                path = reader.expect(StringPrimitive)

                def _logmsg():
                    _log = F&#39;read item at offset 0x{offset:08X}, type 0x{type:02X}, size {SizeInt(size)!r}&#39;
                    if compressed_size:
                        return F&#39;{_log}, compressed to size {SizeInt(compressed_size)!r}&#39;
                    return F&#39;{_log}, uncompressed&#39;

                self.log_debug(_logmsg)

                with reader.checkpoint():
                    reader.seek(offset)
                    if compressed_size:
                        item_data = reader.read(compressed_size) | zl | bytearray
                    else:
                        item_data = reader.read(size)

                yield UnpackResult(path, item_data)
            except ParserEOF:
                self.log_warn(&#39;unexpected EOF while parsing bundle, terminating&#39;)
                break

    def _find_bundle_manifest_offset(self, data: bytearray) -&gt; int:
        bundle_sig_offset = data.find(self._SIGNATURE, 0)
        if bundle_sig_offset &lt; 0:
            raise ValueError(&#39;Cannot find valid Bundle Manifest offset. Is this a .NET Bundle?&#39;)
        return int.from_bytes(data[bundle_sig_offset - 8:bundle_sig_offset], &#39;little&#39;)

    @classmethod
    def handles(self, data: bytearray):
        return self._SIGNATURE in data</code></pre>
</details>
</dd>
<dt id="refinery.shell.dnstr"><code class="flex name class">
<span>class <span class="ident">dnstr</span></span>
<span>(</span><span>user=True, meta=True)</span>
</code></dt>
<dd>
<section class="desc"><p>Extracts all strings defined in the <code>#Strings</code> and <code>#US</code> streams of .NET
executables.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/formats/pe/dotnet/dnstr.py#L7-L29" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class dnstr(Unit):
    &#34;&#34;&#34;
    Extracts all strings defined in the `#Strings` and `#US` streams of .NET
    executables.
    &#34;&#34;&#34;

    def __init__(
        self,
        user: Arg.Switch(&#39;-m&#39;, &#39;--meta&#39;, off=True, group=&#39;HEAP&#39;, help=&#39;Only extract from #Strings.&#39;) = True,
        meta: Arg.Switch(&#39;-u&#39;, &#39;--user&#39;, off=True, group=&#39;HEAP&#39;, help=&#39;Only extract from #US.&#39;) = True,
    ):
        if not meta and not user:
            raise ValueError(&#39;Either ascii or utf16 strings must be enabled.&#39;)
        super().__init__(meta=meta, user=user)

    def process(self, data):
        header = DotNetHeader(data, parse_resources=False)
        if self.args.meta:
            for string in header.meta.Streams.Strings.values():
                yield string.encode(self.codec)
        if self.args.user:
            for string in header.meta.Streams.US.values():
                yield string.encode(self.codec)</code></pre>
</details>
</dd>
<dt id="refinery.shell.doctxt"><code class="flex name class">
<span>class <span class="ident">doctxt</span></span>
</code></dt>
<dd>
<section class="desc"><p>Extracts the text body from Word documents.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/formats/office/doctxt.py#L20-L155" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class doctxt(Unit):
    &#34;&#34;&#34;
    Extracts the text body from Word documents.
    &#34;&#34;&#34;

    @Unit.Requires(&#39;olefile&#39;, &#39;formats&#39;, &#39;office&#39;, &#39;extended&#39;)
    def _olefile():
        import olefile
        return olefile

    def process(self, data: bytearray):
        extractors: Dict[str, Callable[[bytearray], str]] = OrderedDict(
            doc=self._extract_ole,
            docx=self._extract_docx,
            odt=self._extract_odt,
        )
        if data.startswith(B&#39;PK&#39;):
            self.log_debug(&#39;document contains zip file signature, likely a odt or docx file&#39;)
            extractors.move_to_end(&#39;doc&#39;)
            if &#39;opendocument&#39; in str(data | xtzip(&#39;mimetype&#39;)):
                self.log_debug(&#39;odt signature detected&#39;)
                extractors.move_to_end(&#39;odt&#39;, last=False)
        for filetype, extractor in extractors.items():
            self.log_debug(F&#39;trying to extract as {filetype}&#39;)
            try:
                result = extractor(data)
            except ImportError:
                raise
            except Exception as error:
                self.log_info(F&#39;failed extractring as {filetype}: {error!s}&#39;)
            else:
                return result.encode(self.codec)
        raise ValueError(&#39;All extractors failed, the input data is not recognized as any known document format.&#39;)

    def _extract_docx(self, data: Chunk) -&gt; str:
        NAMESPACE = &#39;{http://schemas.openxmlformats.org/wordprocessingml/2006/main}&#39;
        PARAGRAPH = F&#39;{NAMESPACE}p&#39;
        TEXT = F&#39;{NAMESPACE}t&#39;
        chunk = data | xtzip(&#39;word/document.xml&#39;) | bytearray
        if not chunk:
            raise ValueError(&#39;No document.xml file found.&#39;)
        root: Element = XML(chunk)
        with StringIO() as output:
            for index, paragraph in enumerate(root.iter(PARAGRAPH)):
                if index &gt; 0:
                    output.write(&#39;\n&#39;)
                for node in paragraph.iter(TEXT):
                    if node.text:
                        output.write(node.text)
            return output.getvalue()

    def _extract_odt(self, data: bytes):
        def _extract_text(node: Element):
            NAMESPACE = &#39;{urn:oasis:names:tc:opendocument:xmlns:text:1.0}&#39;
            PARAGRAPH = F&#39;{NAMESPACE}p&#39;
            SPAN = F&#39;{NAMESPACE}span&#39;
            SPACE = F&#39;{NAMESPACE}s&#39;
            with StringIO() as res:
                for element in node:
                    tag = element.tag
                    text = element.text or &#39;&#39;
                    tail = element.tail or &#39;&#39;
                    if tag in [PARAGRAPH, SPAN]:
                        res.write(text)
                    elif tag == SPACE:
                        res.write(&#39; &#39;)
                    else:
                        self.log_debug(F&#39;unknown tag: {tag}&#39;)
                    res.write(_extract_text(element))
                    res.write(tail)
                    if tag == PARAGRAPH:
                        res.write(&#39;\n&#39;)
                return res.getvalue()

        NAMESPACE = &#39;{urn:oasis:names:tc:opendocument:xmlns:office:1.0}&#39;
        BODY = F&#39;{NAMESPACE}body&#39;
        TEXT = F&#39;{NAMESPACE}text&#39;
        for part in xtzip().unpack(data):
            if part.path != &#39;content.xml&#39;:
                continue
            xml_content: bytes = part.get_data()
            root: Element = XML(xml_content)
            body: Element = root.find(BODY)
            text: Element = body.find(TEXT)
            return _extract_text(text)
        else:
            raise ValueError(&#39;found no text&#39;)

    def _extract_ole(self, data: bytearray) -&gt; str:
        stream = MemoryFile(data)
        with self._olefile.OleFileIO(stream) as ole:
            doc = ole.openstream(&#39;WordDocument&#39;).read()
            with StructReader(doc) as reader:
                table_name = F&#39;{(doc[11] &gt;&gt; 1) &amp; 1}Table&#39;
                reader.seek(0x1A2)
                offset = reader.u32()
                length = reader.u32()
            with StructReader(ole.openstream(table_name).read()) as reader:
                reader.seek(offset)
                table = reader.read(length)
            piece_table = self._load_piece_table(table)
            return self._get_text(doc, piece_table)

    def _load_piece_table(self, table: bytes) -&gt; bytes:
        with StructReader(table) as reader:
            while not reader.eof:
                entry_type = reader.read_byte()
                if entry_type == 1:
                    reader.seekrel(reader.read_byte())
                    continue
                if entry_type == 2:
                    length = reader.u32()
                    return reader.read(length)
                raise NotImplementedError(F&#39;Unsupported table entry type value 0x{entry_type:X}.&#39;)

    def _get_text(self, doc: bytes, piece_table: bytes) -&gt; str:
        piece_count: int = 1 + (len(piece_table) - 4) // 12
        with StringIO() as text:
            with StructReader(piece_table) as reader:
                character_positions = [reader.u32() for _ in range(piece_count)]
                for i in range(piece_count - 1):
                    cp_start = character_positions[i]
                    cp_end = character_positions[i + 1]
                    fc_value = reader.read_struct(&#39;xxLxx&#39;, unwrap=True)
                    is_ansi = bool((fc_value &gt;&gt; 30) &amp; 1)
                    fc = fc_value &amp; 0xBFFFFFFF
                    cb = cp_end - cp_start
                    if is_ansi:
                        encoding = &#39;cp1252&#39;
                        fc = fc // 2
                    else:
                        encoding = &#39;utf16&#39;
                        cb *= 2
                    raw = doc[fc : fc + cb]
                    text.write(raw.decode(encoding).replace(&#39;\r&#39;, &#39;\n&#39;))
            return text.getvalue()</code></pre>
</details>
</dd>
<dt id="refinery.shell.drp"><code class="flex name class">
<span>class <span class="ident">drp</span></span>
<span>(</span><span>consecutive=False, align=False, min=1, max=, len=None, all=False, threshold=20, weight=0, buffer=1024, chug=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Detect Repeating Patterns - detects the most prevalent repeating byte pattern
in a chunk of data. The unit computes a suffix tree which may require a lot of
memory for large buffers.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/misc/drp.py#L25-L187" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class drp(Unit):
    &#34;&#34;&#34;
    Detect Repeating Patterns - detects the most prevalent repeating byte pattern
    in a chunk of data. The unit computes a suffix tree which may require a lot of
    memory for large buffers.
    &#34;&#34;&#34;
    def __init__(
        self,
        consecutive: Arg.Switch(&#39;-c&#39;, help=&#39;Assume that the repeating pattern is consecutive when observable.&#39;) = False,
        align: Arg.Switch(&#39;-d&#39;, help=&#39;Assume that the pattern occurs at offsets that are multiples of its length.&#39;) = False,
        min: Arg.Number(&#39;-n&#39;, help=&#39;Minimum size of the pattern to search for. Default is {default}.&#39;) = 1,
        max: Arg.Number(&#39;-N&#39;, help=&#39;Maximum size of the pattern to search for. Default is {default}.&#39;) = INF,
        len: Arg.Number(&#39;-l&#39;, help=&#39;Set the exact size of the pattern. This is equivalent to --min=N --max=N.&#39;) = None,
        all: Arg.Switch(&#39;-a&#39;, help=&#39;Produce one output for each repeating pattern that was detected.&#39;) = False,
        threshold: Arg.Number(&#39;-t&#39;, help=&#39;Patterns must match this performance threshold in percent, lest they be discarded.&#39;) = 20,
        weight: Arg.Number(&#39;-w&#39;, help=&#39;Specifies how much longer patterns are favored over small ones. Default is {default}.&#39;) = 0,
        buffer: Arg.Number(&#39;-b&#39;, group=&#39;BFR&#39;, help=&#39;Maximum number of bytes to inspect at once. The default is {default}.&#39;) = 1024,
        chug  : Arg.Switch(&#39;-g&#39;, group=&#39;BFR&#39;, help=&#39;Compute the prefix tree for the entire buffer instead of chunking it.&#39;) = False
    ):
        if len is not None:
            min = max = len
        super().__init__(
            min=min,
            max=max,
            all=all,
            consecutive=consecutive,
            align=align,
            weight=weight,
            buffer=buffer,
            chug=chug,
            threshold=threshold
        )

    def _get_patterns(self, data):
        with stackdepth(len(data)):
            tree = SuffixTree(data)
        min_size = self.args.min
        max_size = self.args.max
        patterns = set()
        cursor = 0
        while cursor &lt; len(data):
            node = tree.root
            rest = data[cursor:]
            remaining = len(rest)
            length = 0
            offset = None
            while node.children and length &lt; remaining:
                for child in node.children.values():
                    if tree.data[child.start] == rest[length]:
                        node = child
                        break
                if node.start &gt;= cursor:
                    break
                offset = node.start - length
                length = node.end + 1 - offset
            if offset is None:
                cursor += 1
                continue
            length = min(remaining, length)
            if max_size &gt;= length &gt;= min_size:
                pattern = rest[:length].tobytes()
                patterns.add(pattern)
            cursor += length
        del tree
        return patterns

    @staticmethod
    def _consecutive_count(data, pattern):
        length = len(pattern)
        if length == 1:
            return data.count(pattern)
        view = memoryview(data)
        return max(sum(1 for i in range(k, len(view), length) if view[i:i + length] == pattern)
            for k in range(len(pattern)))

    @staticmethod
    def _truncate_pattern(pattern):
        offset = 0
        for byte in pattern[1:]:
            if byte == pattern[offset]:
                offset += 1
            else:
                offset = 0
        if offset &gt; 0:
            pattern = pattern[:-offset]
        return pattern

    def process(self, data: bytearray):
        if len(data) &lt;= 1:
            yield data
            return

        memview = memoryview(data)
        weight = 1 + (self.args.weight / 10)

        if self.args.chug:
            patterns = self._get_patterns(memview)
        else:
            patterns = set()
            chunksize = self.args.buffer
            for k in range(0, len(memview), chunksize):
                patterns |= self._get_patterns(memview[k:k + chunksize])
        if not patterns:
            raise RefineryPartialResult(&#39;no repeating sequences found&#39;, data)

        self.log_debug(&#39;removing duplicate pattern detections&#39;)
        duplicates = set()
        maxlen = max(len(p) for p in patterns)
        for pattern in sorted(patterns, key=len):
            for k in range(2, maxlen // len(pattern) + 1):
                repeated = pattern * k
                if repeated in patterns:
                    duplicates.add(repeated)
        patterns -= duplicates

        self.log_debug(F&#39;counting coverage of {len(patterns)} patterns&#39;)
        pattern_count = {p: data.count(p) for p in patterns}
        pattern_performance = dict(pattern_count)

        for consecutive in (False, True):
            if consecutive:
                self.log_debug(F&#39;re-counting coverage of {len(patterns)} patterns&#39;)
                patterns = {self._truncate_pattern(p) for p in patterns}
                pattern_performance = {p: self._consecutive_count(data, p) for p in patterns}

            self.log_debug(&#39;evaluating pattern performance&#39;)
            for pattern, count in pattern_performance.items():
                pattern_performance[pattern] = count * (len(pattern) ** weight)
            best_performance = max(pattern_performance.values())
            for pattern, performance in pattern_performance.items():
                pattern_performance[pattern] = performance / best_performance

            self.log_debug(&#39;removing patterns below performance threshold&#39;)
            threshold = self.args.threshold
            patterns = {p for p in patterns if pattern_performance[p] * 100 &gt;= threshold}
            pattern_count = {p: data.count(p) for p in patterns}

            if not self.args.consecutive:
                break

        if self.args.all:
            for pattern in sorted(patterns, key=pattern_performance.get, reverse=True):
                yield self.labelled(pattern, count=pattern_count[pattern])
            return

        best_patterns = [p for p in patterns if pattern_performance[p] == 1.0]

        if len(best_patterns) &gt; 1:
            self.log_warn(&#39;could not determine unique best repeating pattern, returning the first of these:&#39;)
            for k, pattern in enumerate(best_patterns):
                self.log_warn(F&#39;{k:02d}.: {pattern.hex()}&#39;)

        result = best_patterns[0]

        if self.args.align:
            def rotated(pattern):
                for k in range(len(pattern)):
                    yield pattern[k:] + pattern[:k]
            rotations = {k % len(result): r for k, r in (
                (data.find(r), r) for r in rotated(result)) if k &gt;= 0}
            result = rotations[min(rotations)]

        yield result</code></pre>
</details>
</dd>
<dt id="refinery.shell.dsjava"><code class="flex name class">
<span>class <span class="ident">dsjava</span></span>
</code></dt>
<dd>
<section class="desc"><p>Deserialize Java serialized data and re-serialize as JSON.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/formats/java/deserialize.py#L65-L76" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class dsjava(Unit):
    &#34;&#34;&#34;
    Deserialize Java serialized data and re-serialize as JSON.
    &#34;&#34;&#34;
    @Unit.Requires(&#39;javaobj-py3&gt;=0.4.0.1&#39;, &#39;formats&#39;)
    def _javaobj():
        import javaobj.v2
        return javaobj.v2

    def process(self, data):
        with JavaEncoder as encoder:
            return encoder.dumps(self._javaobj.loads(data)).encode(self.codec)</code></pre>
</details>
</dd>
<dt id="refinery.shell.dsphp"><code class="flex name class">
<span>class <span class="ident">dsphp</span></span>
</code></dt>
<dd>
<section class="desc"><p>Deserialize PHP serialized data and re-serialize as JSON.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/formats/deserialize_php.py#L7-L41" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class dsphp(Unit):
    &#34;&#34;&#34;
    Deserialize PHP serialized data and re-serialize as JSON.
    &#34;&#34;&#34;
    @Unit.Requires(&#39;phpserialize&#39;, &#39;formats&#39;)
    def _php():
        import phpserialize
        return phpserialize

    def reverse(self, data):
        return self._php.dumps(json.loads(data))

    def process(self, data):
        phpobject = self._php.phpobject

        class encoder(json.JSONEncoder):
            def default(self, obj):
                try:
                    return super().default(obj)
                except TypeError:
                    pass
                if isinstance(obj, bytes) or isinstance(obj, bytearray):
                    return obj.decode(&#39;utf8&#39;)
                if isinstance(obj, phpobject):
                    return obj._asdict()

        return json.dumps(
            self._php.loads(
                data,
                object_hook=phpobject,
                decode_strings=True
            ),
            indent=4,
            cls=encoder
        ).encode(self.codec)</code></pre>
</details>
</dd>
<dt id="refinery.shell.dump"><code class="flex name class">
<span>class <span class="ident">dump</span></span>
<span>(</span><span>*files, tee=False, stream=False, plain=False, force=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Dump incoming data to files on disk. It is possible to specify filenames with format fields.
Any metadata field on an incoming chunk is available. Additionally, any field that can be
populated by the <code><a title="refinery.cm" href="index.html#refinery.cm">cm</a></code> unit is also available. These include the following:</p>
<pre><code>{ext}    : Automatically guessed file extension
{crc32}  : CRC32 checksum of the data
{index}  : Index of the data in the input stream, starting at 0
{size}   : Size of the data in bytes
{md5}    : MD5 hash of the data
{sha1}   : SHA1 hash of the data
{sha256} : SHA-256 hash of the data
{path}   : Associated path; defaults to {sha256} if none is given.
</code></pre>
<p>When not using formatted file names, the unit ingests as many incoming inputs as filenames were
specified on the command line. Unless connected to a terminal, the remaining inputs will be
forwarded on STDOUT. The <code>-t</code> or <code>--tee</code> switch can be used to forward all inputs, under all
circumstances, regardless of whether or not they have been processed.</p>
<p>If no file is specified, all ingested inputs are concatenated and written to the clipboard. This
will only succeed when the data can successfully be encoded.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/sinks/dump.py#L14-L210" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class dump(Unit):
    &#34;&#34;&#34;
    Dump incoming data to files on disk. It is possible to specify filenames with format fields.
    Any metadata field on an incoming chunk is available. Additionally, any field that can be
    populated by the `refinery.cm` unit is also available. These include the following:

        {ext}    : Automatically guessed file extension
        {crc32}  : CRC32 checksum of the data
        {index}  : Index of the data in the input stream, starting at 0
        {size}   : Size of the data in bytes
        {md5}    : MD5 hash of the data
        {sha1}   : SHA1 hash of the data
        {sha256} : SHA-256 hash of the data
        {path}   : Associated path; defaults to {sha256} if none is given.

    When not using formatted file names, the unit ingests as many incoming inputs as filenames were
    specified on the command line. Unless connected to a terminal, the remaining inputs will be
    forwarded on STDOUT. The `-t` or `--tee` switch can be used to forward all inputs, under all
    circumstances, regardless of whether or not they have been processed.

    If no file is specified, all ingested inputs are concatenated and written to the clipboard. This
    will only succeed when the data can successfully be encoded.
    &#34;&#34;&#34;

    def __init__(
        self, *files: Arg(metavar=&#39;file&#39;, type=str, help=&#39;Optionally formatted filename.&#39;),
        tee    : Arg.Switch(&#39;-t&#39;, help=&#39;Forward all inputs to STDOUT.&#39;) = False,
        stream : Arg.Switch(&#39;-s&#39;, help=&#39;Dump all incoming data to the same file.&#39;) = False,
        plain  : Arg.Switch(&#39;-p&#39;, help=&#39;Never apply any formatting to file names.&#39;) = False,
        force  : Arg.Switch(&#39;-f&#39;, help=&#39;Remove files if necessary to create dump path.&#39;) = False,
    ):
        if stream and len(files) != 1:
            raise ValueError(&#39;Can only use exactly one file in stream mode.&#39;)
        super().__init__(files=files, tee=tee, stream=stream, force=force)
        self.stream = None
        self._formatted = not plain and any(self._has_format(f) for f in files)
        self._reset()

    @staticmethod
    def _has_format(filename):
        if not isinstance(filename, str):
            return False
        formatter = Formatter()
        return any(
            any(t.isalnum() for t in fields)
            for _, fields, *__ in formatter.parse(filename) if fields
        )

    def _reset(self):
        self.exhausted = False
        self.paths = cycle(self.args.files) if self._formatted else iter(self.args.files)
        self._close()

    @property
    def _clipcopy(self):
        return not self.args.files

    def _components(self, path):
        def _reversed_components(path):
            while True:
                path, component = os.path.split(path)
                if not component:
                    break
                yield component
            yield path
        components = list(_reversed_components(path))
        components.reverse()
        return components

    def _open(self, path, unc=False):
        if hasattr(path, &#39;close&#39;):
            return path
        path = os.path.abspath(path)
        base = os.path.dirname(path)
        if not unc:
            self.log_info(&#39;opening:&#39;, path)
        try:
            os.makedirs(base, exist_ok=True)
        except FileExistsError:
            self.log_info(&#39;existed:&#39;, path)
            part, components = &#39;&#39;, self._components(path)
            while components:
                component, *components = components
                part = os.path.join(part, component)
                if os.path.exists(part) and os.path.isfile(part):
                    if self.args.force:
                        os.unlink(part)
                        return self._open(path, unc)
                    break
            raise RefineryCriticalException(F&#39;Unable to dump to {path} because {part} is a file.&#39;)
        except FileNotFoundError:
            if unc or os.name != &#39;nt&#39;:
                raise
            path = F&#39;\\\\?\\{path}&#39;
            return self._open(path, unc=True)
        except OSError as e:
            if not self.log_info():
                self.log_warn(&#39;opening:&#39;, path)
            self.log_warn(&#39;errored:&#39;, e.args[1])
            return open(os.devnull, &#39;wb&#39;)
        else:
            mode = &#39;ab&#39; if self.args.stream else &#39;wb&#39;
            return open(path, mode)

    def _close(self, final=False):
        if not self.stream:
            return
        self.stream.flush()
        if self.args.stream and not final:
            return
        if self._clipcopy:
            if os.name == &#39;nt&#39;:
                from refinery.lib.winclip import ClipBoard, CF
                try:
                    img = self._image.open(self.stream)
                    with io.BytesIO() as out:
                        img.save(out, &#39;BMP&#39;)
                except Exception:
                    with ClipBoard(CF.TEXT) as cpb:
                        cpb.copy(self.stream.getvalue())
                else:
                    with ClipBoard(CF.DIB) as cpb:
                        out.seek(14, io.SEEK_SET)
                        cpb.copy(out.read())
            else:
                data = self.stream.getvalue()
                data = data.decode(self.codec, errors=&#39;backslashreplace&#39;)
                self._pyperclip.copy(data)
        self.stream.close()
        self.stream = None

    @Unit.Requires(&#39;pyperclip&#39;)
    def _pyperclip():
        import pyperclip
        return pyperclip

    @Unit.Requires(&#39;Pillow&#39;, &#39;formats&#39;)
    def _image():
        from PIL import Image
        return Image

    def process(self, data: bytes):
        forward_input_data = self.args.tee
        if self._clipcopy:
            self.stream.write(data)
        elif not self.exhausted:
            if not self.stream:
                # This should happen only when the unit is called from Python code
                # rather than via the command line.
                try:
                    path = next(self.paths)
                except StopIteration:
                    raise RefineryCriticalException(&#39;the list of filenames was exhausted.&#39;)
                else:
                    with self._open(path) as stream:
                        stream.write(data)
            else:
                self.stream.write(data)
                self.log_debug(F&#39;wrote 0x{len(data):08X} bytes&#39;)
                self._close()
        else:
            forward_input_data = forward_input_data or not self.isatty
            if not forward_input_data:
                size = metavars(data).size
                self.log_warn(F&#39;discarding unprocessed chunk of size {size!s}.&#39;)
        if forward_input_data:
            yield data

    def filter(self, chunks):
        if self.exhausted:
            self._reset()

        nostream = not self.args.stream
        clipcopy = self._clipcopy

        if clipcopy:
            self.stream = io.BytesIO()

        for index, chunk in enumerate(chunks, 0):
            if not chunk.visible:
                continue
            if not clipcopy and not self.exhausted and (nostream or not self.stream):
                try:
                    path = next(self.paths)
                except StopIteration:
                    self.exhausted = True
                else:
                    if self._has_format(path):
                        meta = metavars(chunk)
                        meta.ghost = True
                        meta.update_index(index)
                        path = meta.format_str(path, self.codec, [chunk])
                    self.stream = self._open(path)
            yield chunk

        self._close(final=True)
        self.exhausted = True</code></pre>
</details>
</dd>
<dt id="refinery.shell.eat"><code class="flex name class">
<span>class <span class="ident">eat</span></span>
<span>(</span><span>name)</span>
</code></dt>
<dd>
<section class="desc"><p>Consume a meta variable and replace the contents of the current chunk with it. If the variable
contains a string, it is encoded with the default codec. If the variable cannot be converted to
a byte string, the data is lost and an empty chunk is returned.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/meta/eat.py#L8-L41" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class eat(Unit):
    &#34;&#34;&#34;
    Consume a meta variable and replace the contents of the current chunk with it. If the variable
    contains a string, it is encoded with the default codec. If the variable cannot be converted to
    a byte string, the data is lost and an empty chunk is returned.
    &#34;&#34;&#34;
    def __init__(
        self,
        name: Arg(help=&#39;The name of the variable to be used.&#39;, type=str),
    ):
        super().__init__(name=check_variable_name(name))

    def process(self, data: Chunk):
        def invalid_type():
            return F&#39;variable {name} is of type &#34;{type}&#34;, unable to convert to byte string - data is lost&#39;
        name = self.args.name
        meta = metavars(data)
        data = meta.pop(name)
        type = data.__class__.__name__
        if isinstance(data, int):
            self.log_info(F&#39;variable {name} is an integer, converting to string.&#39;)
            data = str(data).encode(self.codec)
        if isinstance(data, str):
            self.log_info(F&#39;variable {name} is a string, encoding as {self.codec}&#39;)
            data = data.encode(self.codec)
        elif not isbuffer(data):
            try:
                wrapped = bytearray(data)
            except Exception:
                self.log_warn(invalid_type())
                data = None
            else:
                data = wrapped
        return data</code></pre>
</details>
</dd>
<dt id="refinery.shell.ef"><code class="flex name class">
<span>class <span class="ident">ef</span></span>
<span>(</span><span>*filenames, list=False, meta=False, size=None, read=0, wild=False, tame=False, symlinks=False, linewise=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Short for "emit file". The unit reads files from disk and outputs them individually. Has the ability to
read large files in chunks.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/meta/ef.py#L23-L264" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class ef(Unit):
    &#34;&#34;&#34;
    Short for &#34;emit file&#34;. The unit reads files from disk and outputs them individually. Has the ability to
    read large files in chunks.
    &#34;&#34;&#34;

    def __init__(self,
        *filenames: Arg(metavar=&#39;FILEMASK&#39;, nargs=&#39;+&#39;, type=str, help=(
            &#39;A list of file masks. Each matching file will be read from disk and &#39;
            &#39;emitted. The file masks can include format string expressions which &#39;
            &#39;will be substituted from the current meta variables. The masks can &#39;
            &#39;use wild-card expressions, but this feature is disabled by default on &#39;
            &#39;Posix platforms, where it has to be enabled explicitly using the -w &#39;
            &#39;switch. On Windows, the feature is enabled by default and can be &#39;
            &#39;disabled using the -t switch.&#39;
        )),
        list: Arg.Switch(&#39;-l&#39;, help=&#39;Only lists files with metadata.&#39;) = False,
        meta: Arg.Switch(&#39;-m&#39;, help=(
            &#39;Adds the atime, mtime, ctime, and size metadata variables.&#39;
        )) = False,
        size: Arg.Bounds(&#39;-s&#39;, range=True, help=(
            &#39;If specified, only files are read whose size is in the given range.&#39;)) = None,
        read: Arg.Number(&#39;-r&#39;, help=(
            &#39;If specified, files will be read in chunks of size N and each &#39;
            &#39;chunk is emitted as one element in the output list.&#39;
        )) = 0,
        wild: Arg.Switch(&#39;-w&#39;, group=&#39;W&#39;, help=&#39;Force use of wildcard patterns in file masks.&#39;) = False,
        tame: Arg.Switch(&#39;-t&#39;, group=&#39;W&#39;, help=&#39;Disable wildcard patterns in file masks.&#39;) = False,
        symlinks: Arg.Switch(&#39;-y&#39;, help=&#39;Follow symbolic links and junctions, these are ignored by default.&#39;) = False,
        linewise: Arg.Switch(&#39;-i&#39;, help=(
            &#39;Read the file linewise. By default, one line is read at a time. &#39;
            &#39;In line mode, the --read argument can be used to read the given &#39;
            &#39;number of lines in each chunk.&#39;
        )) = False
    ):
        if wild and tame:
            raise ValueError(&#39;Cannot be both wild and tame!&#39;)
        super().__init__(
            size=size,
            read=read,
            list=list,
            meta=meta,
            wild=wild,
            tame=tame,
            symlinks=symlinks,
            linewise=linewise,
            filenames=filenames
        )

    def _read_chunks(self, fd):
        while True:
            buffer = fd.read(self.args.read)
            if not buffer:
                break
            yield buffer

    def _read_lines(self, fd):
        count = self.args.read or 1
        if count == 1:
            while True:
                buffer = fd.readline()
                if not buffer:
                    break
                yield buffer
            return
        with MemoryFile() as out:
            while True:
                for _ in range(count):
                    buffer = fd.readline()
                    if not buffer:
                        break
                    out.write(buffer)
                if not out.tell():
                    break
                yield out.getvalue()
                out.seek(0)
                out.truncate()

    def _absolute_path(self, path_string: str):
        path = Path(path_string).absolute()
        if os.name == &#39;nt&#39; and not path.parts[0].startswith(&#39;\\\\?\\&#39;):
            # The pathlib glob method will simply fail mid-traversal if it attempts to descend into
            # a folder or to a file whose path exceeds MAX_PATH on Windows. As a workaround, we use
            # UNC paths throughout and truncate to relative paths after enumeration.
            path = Path(F&#39;\\\\?\\{path!s}&#39;)
        return path

    def _glob(self, pattern: str) -&gt; Iterable[Path]:
        if pattern.endswith(&#39;**&#39;):
            pattern += &#39;/*&#39;
        wildcard = re.search(R&#39;[\[\?\*]&#39;, pattern)
        if wildcard is None:
            yield self._absolute_path(pattern)
            return
        k = wildcard.start()
        base, pattern = pattern[:k], pattern[k:]
        path = self._absolute_path(base or &#39;.&#39;)
        last = path.parts[-1]
        if base.endswith(last):
            # /base/something.*
            pattern = F&#39;{last}{pattern}&#39;
            path = path.parent

        scandir = os.scandir

        class EmptyIterator:
            def __enter__(self): return self
            def __exit__(self, *_, **__): pass
            def __next__(self): raise StopIteration
            def __iter__(self): return self

        if sys.version_info &gt;= (3, 12):
            def islink(path):
                return os.path.islink(path) or os.path.isjunction(path)
        else:
            def islink(path):
                try:
                    return bool(os.readlink(path))
                except OSError:
                    return False

        paths_scanned = set()

        def _patched_scandir(path):
            if islink(path):
                if not self.args.symlinks:
                    return EmptyIterator()
                try:
                    rp = os.path.realpath(path, strict=True)
                except OSError:
                    return EmptyIterator()
                if rp in paths_scanned:
                    self.log_warn(F&#39;file system loop at: {path!s}&#39;)
                    return EmptyIterator()
                paths_scanned.add(rp)
                path = rp
            try:
                return scandir(path)
            except Exception as e:
                ignore = _ERROR_IGNORES.get(os.name, set())
                if not any(p.lower() in ignore for p in Path(path).parts):
                    self.log_warn(F&#39;error calling scandir, {exception_to_string(e)}: {path}&#39;)
                return EmptyIterator()

        try:
            os.scandir = _patched_scandir
            for match in path.glob(pattern):
                yield match
        finally:
            os.scandir = scandir

    def process(self, data):
        meta = metavars(data)
        size = self.args.size
        size = size and range(size.start, size.stop, size.step)
        meta.ghost = True
        wild = (os.name == &#39;nt&#39; or self.args.wild) and not self.args.tame
        root = self._absolute_path(&#39;.&#39;)
        paths = self._glob if wild else lambda mask: [self._absolute_path(mask)]
        do_meta = self.args.meta
        do_stat = size or do_meta

        class SkipErrors:
            unit = self

            def __init__(self):
                self._history: Set[type] = set()
                self._message: Dict[type, Optional[str]] = {
                    ValueError: (
                        None
                    ), PermissionError: (
                        &#39;access error while scanning: {}&#39;
                    ), OSError: (
                        &#39;system error while scanning: {}&#39;
                    ), FileNotFoundError: (
                        &#39;file unexpectedly not found: {}&#39;
                    ), Exception: (
                        &#39;unknown error while reading: {}&#39;
                    ),
                }
                self.path = None

            def reset(self, path):
                self._history.clear()
                self.path = path
                return self

            def __enter__(self):
                return self

            def __exit__(self, et, ev, trace):
                if et is None:
                    return False
                for t, msg in self._message.items():
                    if issubclass(et, t):
                        if t not in self._history:
                            self._history.add(t)
                            if msg is not None:
                                self.unit.log_info(msg.format(self.path))
                        return True
                else:
                    return False

        for mask in self.args.filenames:
            mask = meta.format_str(mask, self.codec, [data])
            self.log_debug(&#39;scanning for mask:&#39;, mask)
            kwargs = dict()
            skip_errors = SkipErrors()
            for path in paths(mask):
                skip_errors.reset(path)
                filesize = None
                with skip_errors:
                    path = path.relative_to(root)
                with skip_errors:
                    if wild and not path.is_file():
                        continue
                with skip_errors:
                    if do_stat:
                        stat = path.stat()
                        filesize = stat.st_size
                    if do_meta:
                        kwargs.update(
                            fsize=filesize,
                            atime=datetime.fromtimestamp(stat.st_atime).isoformat(&#39; &#39;, &#39;seconds&#39;),
                            ctime=datetime.fromtimestamp(stat.st_ctime).isoformat(&#39; &#39;, &#39;seconds&#39;),
                            mtime=datetime.fromtimestamp(stat.st_mtime).isoformat(&#39; &#39;, &#39;seconds&#39;)
                        )
                if size is not None and filesize not in size:
                    continue
                with skip_errors:
                    if self.args.list:
                        yield self.labelled(str(path).encode(self.codec), **kwargs)
                        continue
                    with path.open(&#39;rb&#39;) as stream:
                        if self.args.linewise:
                            yield from self._read_lines(stream)
                        elif self.args.read:
                            yield from self._read_chunks(stream)
                        else:
                            data = stream.read()
                            self.log_info(lambda: F&#39;reading: {path!s} ({len(data)} bytes)&#39;)
                            yield self.labelled(data, path=path.as_posix(), **kwargs)</code></pre>
</details>
</dd>
<dt id="refinery.shell.emit"><code class="flex name class">
<span>class <span class="ident">emit</span></span>
<span>(</span><span>*data)</span>
</code></dt>
<dd>
<section class="desc"></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/meta/emit.py#L12-L39" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class emit(Unit):

    def __init__(self, *data: Arg(help=(
        &#39;Data to be emitted. If no argument is specified, data is retrieved from &#39;
        &#39;the clipboard. Multiple arguments are output in framed format.&#39;
    ))):
        super().__init__(data=data)

    @Unit.Requires(&#39;pyperclip&#39;)
    def _pyperclip():
        import pyperclip
        return pyperclip

    def process(self, data):
        if self.args.data:
            yield from self.args.data
            return
        if os.name == &#39;nt&#39;:
            from refinery.lib.winclip import get_any_data
            mode, data = get_any_data()
            if mode is not None:
                self.log_info(F&#39;retrieved clipboard data in {mode.name} format&#39;)
            yield data
        else:
            data = self._pyperclip.paste()
            if not data:
                return
            yield data.encode(self.codec, &#39;replace&#39;)</code></pre>
</details>
</dd>
<dt id="refinery.shell.esc"><code class="flex name class">
<span>class <span class="ident">esc</span></span>
<span>(</span><span>hex=False, unicode=False, greedy=False, unquoted=False, quoted=False, bare=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Encodes and decodes common ASCII escape sequences.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/encoding/esc.py#L8-L107" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class esc(Unit):
    &#34;&#34;&#34;
    Encodes and decodes common ASCII escape sequences.
    &#34;&#34;&#34;
    _ESCAPE = {
        0x00: BR&#39;\0&#39;,
        0x07: BR&#39;\a&#39;,
        0x08: BR&#39;\b&#39;,
        0x0C: BR&#39;\f&#39;,
        0x0A: BR&#39;\n&#39;,
        0x0D: BR&#39;\r&#39;,
        0x09: BR&#39;\t&#39;,
        0x0B: BR&#39;\v&#39;,
        0x5C: BR&#39;\\&#39;,
        0x27: BR&#39;\&#39;&#39;,
        0x22: BR&#39;\&#34;&#39;
    }
    _UNESCAPE = {
        BR&#39;0&#39;: B&#39;\x00&#39;,
        BR&#39;a&#39;: B&#39;\x07&#39;,
        BR&#39;b&#39;: B&#39;\x08&#39;,
        BR&#39;f&#39;: B&#39;\x0C&#39;,
        BR&#39;n&#39;: B&#39;\x0A&#39;,
        BR&#39;r&#39;: B&#39;\x0D&#39;,
        BR&#39;t&#39;: B&#39;\x09&#39;,
        BR&#39;v&#39;: B&#39;\x0B&#39;,
        B&#39;\\&#39;: B&#39;\x5C&#39;,
        BR&#34;&#39;&#34;: B&#39;\x27&#39;,
        BR&#39;&#34;&#39;: B&#39;\x22&#39;
    }

    def __init__(self,
        hex     : Arg.Switch(&#39;-x&#39;, help=&#39;Hex encode everything, do not use C escape sequences.&#39;) = False,
        unicode : Arg.Switch(&#39;-u&#39;, help=&#39;Use unicode escape sequences and UTF-8 encoding.&#39;) = False,
        greedy  : Arg.Switch(&#39;-g&#39;, help=&#39;Replace \\x by x and \\u by u when not followed by two or four hex digits, respectively.&#39;) = False,
        unquoted: Arg.Switch(&#39;-p&#39;, group=&#39;Q&#39;, help=&#39;Never remove enclosing quotes.&#39;) = False,
        quoted  : Arg.Switch(&#39;-q&#39;, group=&#39;Q&#39;, help=&#39;Remove enclosing quotes while decoding and add them for encoding.&#39;) = False,
        bare    : Arg.Switch(&#39;-b&#39;, help=&#39;Do not escape quote characters.&#39;) = False,
    ) -&gt; Unit: pass  # noqa

    def process(self, data):
        data = memoryview(data)

        if self.args.quoted:
            quote = data[0]
            if data[-1] != quote:
                self.log_info(&#39;string is not correctly quoted&#39;)
            else:
                data = data[1:-1]
        elif not self.args.unquoted:
            quote = data[:1]
            strip = data[1:-1]
            if data[-1:] == quote and not re.search(br&#39;(?&lt;!\\)&#39; + re.escape(quote), strip):
                self.log_info(&#39;removing automatically detected quotes&#39;)
                data = strip

        def unescape(match):
            c = match[1]
            if len(c) &gt; 1:
                if c[0] == 0x75:
                    # unicode
                    upper = int(c[1:3], 16)
                    lower = int(c[3:5], 16)
                    if self.args.unicode:
                        return bytes((lower, upper)).decode(&#39;utf-16le&#39;).encode(self.codec)
                    return bytes((lower,))
                elif c[0] == 0x78:
                    # hexadecimal
                    return bytes((int(c[1:3], 16),))
                else:
                    # octal escape sequence
                    return bytes((int(c, 8) &amp; 0xFF,))
            elif c in B&#39;ux&#39;:
                return c if self.args.greedy else match[0]
            return self._UNESCAPE.get(c, c)
        data = re.sub(
            RB&#39;\\(u[a-fA-F0-9]{4}|x[a-fA-F0-9]{1,2}|[0-7]{3}|.)&#39;, unescape, data)
        return data

    def reverse(self, data):
        if self.args.unicode:
            string = data.decode(self.codec).encode(&#39;UNICODE_ESCAPE&#39;)
        else:
            if not self.args.hex:
                def escape(match):
                    c = match[0][0]
                    return self._ESCAPE.get(c, RB&#39;\x%02x&#39; % c)
                pattern = RB&#39;[\x00-\x1F\x22\x27\x5C\x7F-\xFF]&#39;
                if self.args.bare:
                    pattern = RB&#39;[\x00-\x1F\x5C\x7F-\xFF]&#39;
                string = re.sub(pattern, escape, data)
            else:
                string = bytearray(4 * len(data))
                for k in range(len(data)):
                    a = k * 4
                    b = k * 4 + 4
                    string[a:b] = RB&#39;\x%02x&#39; % data[k]
        if self.args.quoted:
            string = B&#39;&#34;%s&#34;&#39; % string
        return string</code></pre>
</details>
</dd>
<dt id="refinery.shell.evtx"><code class="flex name class">
<span>class <span class="ident">evtx</span></span>
<span>(</span><span>raw=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Extracts data from Windows Event Log files (EVTX). Each extracted log entry is returned as a single
output chunk in XML format.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/formats/evtx.py#L7-L26" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class evtx(Unit):
    &#34;&#34;&#34;
    Extracts data from Windows Event Log files (EVTX). Each extracted log entry is returned as a single
    output chunk in XML format.
    &#34;&#34;&#34;

    def __init__(self, raw: Unit.Arg.Switch(&#39;-r&#39;, help=&#39;Extract raw event data rather than XML.&#39;) = False):
        super().__init__(raw=raw)

    @Unit.Requires(&#39;python-evtx&#39;, &#39;formats&#39;)
    def _evtx():
        from Evtx.Evtx import Evtx
        return Evtx

    def process(self, data):
        with VirtualFileSystem() as vfs:
            raw = self.args.raw
            with self._evtx(vfs.new(data)) as log:
                for record in log.records():
                    yield record.data() if raw else record.xml().encode(self.codec)</code></pre>
</details>
</dd>
<dt id="refinery.shell.fernet"><code class="flex name class">
<span>class <span class="ident">fernet</span></span>
<span>(</span><span>key)</span>
</code></dt>
<dd>
<section class="desc"><p>Decrypt Fernet messages.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/crypto/cipher/fernet.py#L13-L51" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class fernet(Unit):
    &#34;&#34;&#34;
    Decrypt Fernet messages.
    &#34;&#34;&#34;
    def __init__(self, key: Arg(help=&#39;A fernet key, either in base64 or raw binary.&#39;)):
        super().__init__(key=key)

    def _b64(self, data):
        try:
            return data | b64(urlsafe=True) | bytearray
        except Exception:
            return data

    def process(self, data):
        fk = self._b64(self.args.key)
        if len(fk) != 32:
            raise ValueError(F&#39;The given Fernet key has length {len(fk)}, expected 32 bytes.&#39;)
        signing_key = fk[:16]
        encryption_key = fk[16:]
        decoded = self._b64(data)
        reader = StructReader(memoryview(decoded), bigendian=True)
        signed_data = reader.peek(reader.remaining_bytes - 32)
        version = reader.u8()
        timestamp = datetime.fromtimestamp(reader.u64())
        iv = reader.read(16)
        if version != 0x80:
            self.log_warn(F&#39;The Fernet version is 0x{version:02X}, the only documented one is 0x80.&#39;)
        ciphertext = reader.read(reader.remaining_bytes - 32)
        if len(ciphertext) % 16 != 0:
            raise ValueError(&#39;The encoded ciphertext is not 16-byte block aligned.&#39;)
        signature = reader.read(32)
        hmac = HMAC.new(signing_key, digestmod=SHA256)
        hmac.update(signed_data)
        if hmac.digest() != signature:
            self.log_warn(&#39;HMAC verification failed; the message has been tampered with.&#39;)
            self.log_info(F&#39;computed signature: {hmac.hexdigest().upper()}&#39;)
            self.log_info(F&#39;provided signature: {signature.hex().upper()}&#39;)
        plaintext = ciphertext | aes(mode=&#39;cbc&#39;, iv=iv, key=encryption_key) | bytearray
        return self.labelled(plaintext, timestamp=timestamp.isoformat(&#39; &#39;, &#39;seconds&#39;))</code></pre>
</details>
</dd>
<dt id="refinery.shell.gost"><code class="flex name class">
<span>class <span class="ident">gost</span></span>
<span>(</span><span>key, iv=b'', padding=None, mode=None, raw=False, swap=False, sbox=SBOX.R34, *, assoc_len=0, mac_len=0, segment_size=0, little_endian=False)</span>
</code></dt>
<dd>
<section class="desc"><p>GOST encryption and decryption.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/crypto/cipher/gost.py#L108-L128" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class gost(StandardBlockCipherUnit, cipher=BlockCipherFactory(GOST)):
    &#34;&#34;&#34;
    GOST encryption and decryption.
    &#34;&#34;&#34;
    def __init__(
        self, key, iv=B&#39;&#39;, padding=None, mode=None, raw=False,
        swap: Arg.Switch(&#39;-s&#39;, help=&#39;Decode blocks as big endian rather than little endian.&#39;) = False,
        sbox: Arg.Option(&#39;-x&#39;, choices=SBOX, help=(
            &#39;Choose an SBOX. The default is {default}, which corresponds to the R-34.12.2015 standard. &#39;
            &#39;The other option is CBR, which is the SBOX used by the Central Bank of Russia.&#39;
        )) = SBOX.R34, **more
    ):
        sbox = Arg.AsOption(sbox, SBOX)
        super().__init__(key, iv, padding=padding, mode=mode, raw=raw, swap=swap, sbox=sbox, **more)

    def _new_cipher(self, **optionals) -&gt; CipherInterface:
        return super()._new_cipher(
            swap=self.args.swap,
            sbox=self.args.sbox,
            **optionals
        )</code></pre>
</details>
</dd>
<dt id="refinery.shell.group"><code class="flex name class">
<span>class <span class="ident">group</span></span>
<span>(</span><span>size)</span>
</code></dt>
<dd>
<section class="desc"><p>Group incoming chunks into frames of the given size.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/meta/group.py#L8-L29" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class group(Unit):
    &#34;&#34;&#34;
    Group incoming chunks into frames of the given size.
    &#34;&#34;&#34;
    def __init__(self, size: Arg.Number(help=&#39;Size of each group; must be at least 2.&#39;, bound=(2, None))):
        super().__init__(size=size)

    def process(self, data: Chunk):
        if not data.temp:
            return
        yield data
        yield from islice(data.temp, 0, self.args.size - 1)

    def filter(self, chunks):
        it = iter(chunks)
        while True:
            try:
                head: Chunk = next(it)
            except StopIteration:
                return
            head.temp = it
            yield head</code></pre>
</details>
</dd>
<dt id="refinery.shell.groupby"><code class="flex name class">
<span>class <span class="ident">groupby</span></span>
<span>(</span><span>name)</span>
</code></dt>
<dd>
<section class="desc"><p>Group incoming chunks by the contents of a meta variable. Note that the unit
blocks and cannot stream any output until the input frame is consumed: It has
to read every input chunk to make sure that all groupings are complete.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/meta/groupby.py#L10-L34" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class groupby(Unit):
    &#34;&#34;&#34;
    Group incoming chunks by the contents of a meta variable. Note that the unit
    blocks and cannot stream any output until the input frame is consumed: It has
    to read every input chunk to make sure that all groupings are complete.
    &#34;&#34;&#34;
    def __init__(self, name: Arg(type=str, help=&#39;name of the meta variable&#39;)):
        super().__init__(name=check_variable_name(name))

    def process(self, data):
        yield from data.temp

    def filter(self, chunks: Iterable[Chunk]) -&gt; Generator[Chunk, None, None]:
        name = self.args.name
        members = defaultdict(list)
        for chunk in chunks:
            try:
                value = chunk.meta[name]
            except KeyError:
                value = None
            members[value].append(chunk)
        for chunklist in members.values():
            dummy = chunklist[0]
            dummy.temp = chunklist
            yield dummy</code></pre>
</details>
</dd>
<dt id="refinery.shell.hc128"><code class="flex name class">
<span>class <span class="ident">hc128</span></span>
<span>(</span><span>key, discard=0, stateful=False)</span>
</code></dt>
<dd>
<section class="desc"><p>HC-128 encryption and decryption.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/crypto/cipher/hc128.py#L77-L84" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class hc128(StreamCipherUnit):
    &#34;&#34;&#34;
    HC-128 encryption and decryption.
    &#34;&#34;&#34;
    key_size = {32}

    def keystream(self) -&gt; Iterable[int]:
        return HC128(self.args.key)</code></pre>
</details>
</dd>
<dt id="refinery.shell.hc256"><code class="flex name class">
<span>class <span class="ident">hc256</span></span>
<span>(</span><span>key, iv=b'\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00', discard=0, stateful=False)</span>
</code></dt>
<dd>
<section class="desc"><p>HC-256 encryption and decryption.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/crypto/cipher/hc256.py#L80-L96" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class hc256(StreamCipherUnit):
    &#34;&#34;&#34;
    HC-256 encryption and decryption.
    &#34;&#34;&#34;
    key_size = {32}

    def __init__(
        self, key,
        iv: Arg(help=&#39;An initialization vector; the default is a sequence of 32 zero bytes.&#39;) = bytes(32),
        discard=0, stateful=False,
    ):
        super().__init__(key=key, iv=iv, stateful=stateful, discard=discard)
        self._keystream = None

    def keystream(self) -&gt; Iterable[int]:
        for num in HC256(self.args.key, self.args.iv):
            yield from num.to_bytes(4, &#39;little&#39;)</code></pre>
</details>
</dd>
<dt id="refinery.shell.hex"><code class="flex name class">
<span>class <span class="ident">hex</span></span>
</code></dt>
<dd>
<section class="desc"><p>Hex-decodes and encodes binary data. Non-hex characters are removed from
the input. For decoding, any odd trailing hex digits are stripped as two
hex digits are required to represent a byte.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/encoding/hex.py#L6-L29" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class hex(Unit):
    &#34;&#34;&#34;
    Hex-decodes and encodes binary data. Non-hex characters are removed from
    the input. For decoding, any odd trailing hex digits are stripped as two
    hex digits are required to represent a byte.
    &#34;&#34;&#34;

    def reverse(self, data):
        import base64
        return base64.b16encode(data)

    def process(self, data):
        import re
        import base64
        data = re.sub(B&#39;[^A-Fa-f0-9]+&#39;, B&#39;&#39;, data)
        if len(data) % 2:
            data = data[:-1]
        return base64.b16decode(data, casefold=True)

    @classmethod
    def handles(self, data: bytearray):
        from refinery.lib.patterns import formats
        if formats.spaced_hex.fullmatch(data):
            return True</code></pre>
</details>
</dd>
<dt id="refinery.shell.hexload"><code class="flex name class">
<span>class <span class="ident">hexload</span></span>
<span>(</span><span>blocks=1, dense=False, expand=False, narrow=False, width=0)</span>
</code></dt>
<dd>
<section class="desc"><p>Convert hex dumps back to the original data and vice versa. All options of this unit apply
to its reverse operation where binary data is converted to a readable hexdump format.
The default mode of the unit expects the input data to contain a readable hexdump and
converts it back to binary.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/formats/hexload.py#L17-L113" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class hexload(HexViewer):
    &#34;&#34;&#34;
    Convert hex dumps back to the original data and vice versa. All options of this unit apply
    to its reverse operation where binary data is converted to a readable hexdump format.
    The default mode of the unit expects the input data to contain a readable hexdump and
    converts it back to binary.
    &#34;&#34;&#34;
    @regex
    class _ENCODED_BYTES:
        R&#34;&#34;&#34;
        (?ix)(?:^|(?&lt;=\s))                      # encoded byte patches must be prefixed by white space
        (?:
            (?:                                 # separated chunks of hex data
                [a-f0-9]{2}                     # hexadecimal chunk; single byte (two hexadecimal letters)
                \s{1,2}                         # encoded byte followed by whitespace
                (?:                             # at least one more encoded byte
                    [a-f0-9]{2}                 # followed by more encoded bytes
                    (?:\s{1,2}[a-f0-9]{2})*     # unless it was just a single byte
                )?
            )
            | (?:[a-f0-9]{4}\s{1,2}             # 2-byte chunks
              (?:[a-f0-9]{4}
              (?:\s{1,2}[a-f0-9]{4})*)?)
            | (?:[a-f0-9]{8}\s{1,2}             # 4-byte chunks
              (?:[a-f0-9]{8}
              (?:\s{1,2}[a-f0-9]{8})*)?)
            | (?:(?:[a-f0-9]{2})+)              # continuous line of hexadecimal characters
        )(?=\s|$)                               # terminated by a whitespace or line end
        &#34;&#34;&#34;

    def __init__(self, blocks=1, dense=False, expand=False, narrow=False, width=0):
        super().__init__(blocks=blocks, dense=dense, expand=expand, narrow=narrow, width=width)
        self._hexline_pattern = re.compile(F&#39;{make_hexline_pattern(1)}(?:[\r\n]|$)&#39;, flags=re.MULTILINE)

    def process(self, data: bytearray):
        lines = data.decode(self.codec).splitlines(keepends=False)

        if not lines:
            return None

        decoded_bytes = bytearray()
        encoded_byte_matches: List[Dict[int, int]] = []

        for line in lines:
            matches: Dict[int, int] = {}
            encoded_byte_matches.append(matches)
            for match in self._ENCODED_BYTES.finditer(line):
                a, b = match.span()
                matches[a] = b - a

        it = iter(encoded_byte_matches)
        offsets = set(next(it).keys())
        for matches in it:
            offsets.intersection_update(matches.keys())
        if not offsets:
            raise ValueError(&#39;unable to determine the position of the hex bytes in this dump&#39;)
        lengths: Dict[int, List[int]] = {offset: [] for offset in offsets}
        del offsets
        for matches in encoded_byte_matches:
            for offset in lengths:
                lengths[offset].append(matches[offset])
        for offset in lengths:
            lengths[offset].sort()
        midpoint = len(encoded_byte_matches) // 2
        offset, length = max(((offset, lengths[offset][midpoint]) for offset in lengths),
            key=operator.itemgetter(1))
        end = offset + length
        del lengths
        for k, line in enumerate(lines, 1):
            encoded_line = line[offset:end]
            onlyhex = re.search(r&#39;^[\sA-Fa-f0-9]+&#39;, encoded_line)
            if not onlyhex:
                self.log_warn(F&#39;ignoring line without hexadecimal data: {line}&#39;)
                continue
            if onlyhex.group(0) != encoded_line:
                if k != len(lines):
                    self.log_warn(F&#39;ignoring line with mismatching hex data length: {line}&#39;)
                    continue
                encoded_line = onlyhex.group(0)
            self.log_debug(F&#39;decoding: {encoded_line.strip()}&#39;)
            decoded_line = bytes.fromhex(encoded_line)
            decoded_bytes.extend(decoded_line)
            txt = line[end:]
            txt_stripped = re.sub(&#39;\\s+&#39;, &#39;&#39;, txt)
            if not txt_stripped:
                continue
            if len(decoded_line) not in range(len(txt_stripped), len(txt) + 1):
                self.log_warn(F&#39;preview size {len(txt_stripped)} does not match decoding: {line}&#39;)
        if decoded_bytes:
            yield decoded_bytes

    def reverse(self, data):
        metrics = self._get_metrics(len(data))
        if not self.args.width:
            metrics.fit_to_width(allow_increase=True)
        for line in self.hexdump(data, metrics):
            yield line.encode(self.codec)</code></pre>
</details>
</dd>
<dt id="refinery.shell.HKDF"><code class="flex name class">
<span>class <span class="ident">HKDF</span></span>
<span>(</span><span>size, salt, hash='SHA512')</span>
</code></dt>
<dd>
<section class="desc"><p>HKDF Key derivation</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/crypto/keyderive/hkdf.py#L6-L14" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class HKDF(KeyDerivation):
    &#34;&#34;&#34;HKDF Key derivation&#34;&#34;&#34;

    def __init__(self, size, salt, hash=&#39;SHA512&#39;):
        super().__init__(size=size, salt=salt, hash=hash)

    def process(self, data):
        from Cryptodome.Protocol.KDF import HKDF
        return HKDF(data, self.args.size, self.args.salt, self.hash)</code></pre>
</details>
</dd>
<dt id="refinery.shell.hmac"><code class="flex name class">
<span>class <span class="ident">hmac</span></span>
<span>(</span><span>salt, hash='SHA1', size=None)</span>
</code></dt>
<dd>
<section class="desc"><p>HMAC based key derivation</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/crypto/keyderive/hmac.py#L6-L16" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class hmac(KeyDerivation):
    &#34;&#34;&#34;
    HMAC based key derivation
    &#34;&#34;&#34;

    def __init__(self, salt, hash=&#39;SHA1&#39;, size=None):
        super().__init__(salt=salt, size=size, hash=hash)

    def process(self, data):
        from Cryptodome.Hash import HMAC
        return HMAC.new(data, self.args.salt, digestmod=self.hash).digest()</code></pre>
</details>
</dd>
<dt id="refinery.shell.htmlesc"><code class="flex name class">
<span>class <span class="ident">htmlesc</span></span>
</code></dt>
<dd>
<section class="desc"><p>Encodes and decodes HTML entities.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/encoding/htmlesc.py#L9-L20" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class htmlesc(Unit):
    &#34;&#34;&#34;
    Encodes and decodes HTML entities.
    &#34;&#34;&#34;

    @unicoded
    def process(self, data: str) -&gt; str:
        return html_entities.unescape(data)

    @unicoded
    def reverse(self, data: str) -&gt; str:
        return html_entities.escape(data)</code></pre>
</details>
</dd>
<dt id="refinery.shell.httprequest"><code class="flex name class">
<span>class <span class="ident">httprequest</span></span>
</code></dt>
<dd>
<section class="desc"><p>Parses HTTP request data, as you would obtain from a packet dump. The unit extracts
POST data in any format; each uploaded file is emitted as a separate chunk.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/formats/httprequest.py#L50-L107" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class httprequest(Unit):
    &#34;&#34;&#34;
    Parses HTTP request data, as you would obtain from a packet dump. The unit extracts
    POST data in any format; each uploaded file is emitted as a separate chunk.
    &#34;&#34;&#34;
    def process(self, data):
        def header(line: bytes):
            name, colon, data = line.decode(&#39;utf8&#39;).partition(&#39;:&#39;)
            if colon:
                yield (name.strip().lower(), data.strip())

        head, _, body = data.partition(b&#39;\r\n\r\n&#39;)
        request, *headers = head.splitlines(False)
        headers = dict(t for line in headers for t in header(line))
        method, path, _, *rest = request.split()

        mode = _Fmt.RawBody

        if rest:
            self.log_warn(&#39;unexpected rest data while parsing HTTP request:&#39;, rest)

        if method == b&#39;GET&#39; and not body:
            mode = _Fmt.UrlEncode
            body = path.partition(B&#39;?&#39;)[1]
        if method == b&#39;POST&#39; and (ct := headers.get(&#39;content-type&#39;, None)):
            ct, _ = _parse_header(ct)
            mode = _Fmt(ct)

        def chunks(upload: Dict[Union[str, bytes], List[bytes]]):
            for key, values in upload.items():
                if not isinstance(key, str):
                    key = key.decode(&#39;utf8&#39;)
                for value in values:
                    yield self.labelled(value, name=key)

        if mode is _Fmt.RawBody:
            yield body
            return
        if mode is _Fmt.Multipart:
            _, _, message_data = data.partition(b&#39;\n&#39;)
            msg = BytesParser().parsebytes(message_data)
            for part in msg.walk():
                payloads = part.get_payload(decode=True)
                if not isinstance(payloads, list):
                    payloads = [payloads]
                for payload in payloads:
                    if not isbuffer(payload):
                        continue
                    if name := part.get_filename():
                        payload = self.labelled(payload, name=name)
                    yield payload

        if mode is _Fmt.UrlEncode:
            yield from chunks(parse_qs(body, keep_blank_values=1))

    @classmethod
    def handles(self, data: bytearray) -&gt; bool | None:
        return data.startswith(B&#39;POST &#39;) or data.startswith(B&#39;GET &#39;)</code></pre>
</details>
</dd>
<dt id="refinery.shell.httpresponse"><code class="flex name class">
<span>class <span class="ident">httpresponse</span></span>
</code></dt>
<dd>
<section class="desc"><p>Parses HTTP response text, as you would obtain from a packet dump. This can be
useful if chunked or compressed transfer encoding was used.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/formats/httpresponse.py#L14-L28" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class httpresponse(Unit):
    &#34;&#34;&#34;
    Parses HTTP response text, as you would obtain from a packet dump. This can be
    useful if chunked or compressed transfer encoding was used.
    &#34;&#34;&#34;
    def process(self, data):
        with SockWrapper(data) as mock:
            mock.seek(0)
            parser = HTTPResponse(mock)
            parser.begin()
            try:
                return parser.read()
            except IncompleteRead as incomplete:
                msg = F&#39;incomplete read: {len(incomplete.partial)} bytes processed, {incomplete.expected} more expected&#39;
                raise RefineryPartialResult(msg, incomplete.partial) from incomplete</code></pre>
</details>
</dd>
<dt id="refinery.shell.iemap"><code class="flex name class">
<span>class <span class="ident">iemap</span></span>
<span>(</span><span>legend=False, background=False, block_char='#', *label)</span>
</code></dt>
<dd>
<section class="desc"><p>The information entropy map displays a colored bar on the terminal visualizing the file's local
entropy from beginning to end.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/sinks/iemap.py#L15-L157" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class iemap(Unit):
    &#34;&#34;&#34;
    The information entropy map displays a colored bar on the terminal visualizing the file&#39;s local
    entropy from beginning to end.
    &#34;&#34;&#34;
    def __init__(
        self,
        legend: Unit.Arg.Switch(&#39;-l&#39;, help=&#39;Show entropy color legend.&#39;) = False,
        background: Unit.Arg.Switch(&#39;-b&#39;, help=&#39;Generate the bar by coloring the background.&#39;) = False,
        block_char: Unit.Arg(&#39;-c&#39;, &#39;--block-char&#39;, type=str, metavar=&#39;C&#39;,
            help=&#39;Character used for filling the bar, default is {default}&#39;) = &#39;#&#39;,
        *label: Unit.Arg(type=str, metavar=&#39;label-part&#39;, help=(
            &#39;The remaining command line specifies a format string expression that will be printed &#39;
            &#39;over the heat map display of each processed chunk.&#39;
        ))
    ):
        super().__init__(label=&#39; &#39;.join(label), background=background, legend=legend, block_char=block_char)

    @Unit.Requires(&#39;colorama&#39;, &#39;display&#39;, &#39;default&#39;, &#39;extended&#39;)
    def _colorama():
        import colorama
        return colorama

    def process(self, data):
        from sys import stderr
        from os import name as os_name
        colorama = self._colorama
        colorama.init(autoreset=False, convert=(os_name == &#39;nt&#39;))

        nobg = not self.args.background
        meta = metavars(data)

        label = meta.format_str(self.args.label, self.codec, [data])
        if label:
            if not label.endswith(&#39; &#39;):
                label = F&#39;{label} &#39;
            if not label.startswith(&#39; &#39;):
                label = F&#39; {label}&#39;

        bgmap = [
            colorama.Back.BLACK,
            colorama.Back.WHITE,
            colorama.Back.YELLOW,
            colorama.Back.CYAN,
            colorama.Back.BLUE,
            colorama.Back.GREEN,
            colorama.Back.LIGHTRED_EX,
            colorama.Back.MAGENTA,
        ]
        fgmap = [
            colorama.Fore.LIGHTBLACK_EX,
            colorama.Fore.LIGHTWHITE_EX,
            colorama.Fore.LIGHTYELLOW_EX,
            colorama.Fore.LIGHTCYAN_EX,
            colorama.Fore.LIGHTBLUE_EX,
            colorama.Fore.LIGHTGREEN_EX,
            colorama.Fore.LIGHTRED_EX,
            colorama.Fore.LIGHTMAGENTA_EX,
        ]

        _reset = colorama.Back.BLACK + colorama.Fore.WHITE + colorama.Style.RESET_ALL

        clrmap = fgmap if nobg else bgmap
        header = &#39;[&#39;
        header_length = 1
        footer_length = 4 + 7

        if self.args.legend:
            header = &#39;[{1}{0}] {2}&#39;.format(_reset, &#39;&#39;.join(F&#39;{bg}{k}&#39; for k, bg in enumerate(clrmap, 1)), header)
            header_length += 3 + len(clrmap)

        _tw = get_terminal_size()
        width = _tw - header_length - footer_length
        if width &lt; 16:
            raise RuntimeError(F&#39;computed terminal width {_tw} is too small for heatmap&#39;)

        def entropy_select(value, map):
            index = min(len(map) - 1, math.floor(value * len(map)))
            return map[index]

        view = memoryview(data)
        size = len(data)
        chunk_size = 0

        for block_size in range(1, width + 1):
            block_count = width // block_size
            chunk_size = size // block_count
            if chunk_size &gt; 1024:
                break

        q, remainder = divmod(width, block_size)
        assert q == block_count
        indices = list(range(q))
        random.seed(sum(view[:1024]))
        random.shuffle(indices)

        block_sizes = [block_size] * q
        q, r = divmod(remainder, block_count)
        for i in indices:
            block_sizes[i] += q
        for i in indices[:r]:
            block_sizes[i] += 1
        assert sum(block_sizes) == width

        q, remainder = divmod(size, block_count)
        assert q == chunk_size
        chunk_sizes = [chunk_size] * block_count
        for i in indices[:remainder]:
            chunk_sizes[i] += 1
        assert sum(chunk_sizes) == size

        stream = MemoryFile(view)
        filler = self.args.block_char if nobg else &#39; &#39;

        try:
            stderr.write(header)
            if label is not None:
                stderr.write(colorama.Fore.WHITE)
                stderr.flush()
            it = itertools.chain(itertools.repeat(filler, 3), label, itertools.cycle(filler))
            cp = None
            for chunk_size, block_size in zip(chunk_sizes, block_sizes):
                chunk = stream.read(chunk_size)
                chunk_entropy = entropy(chunk)
                pp = entropy_select(chunk_entropy, clrmap)
                string = &#39;&#39;.join(itertools.islice(it, block_size))
                if pp != cp:
                    string = F&#39;{pp}{string}&#39;
                cp = pp
                stderr.write(string)
                stderr.flush()
        except BaseException:
            eraser = &#39; &#39; * width
            stderr.write(F&#39;\r{_reset}{eraser}\r&#39;)
            raise
        else:
            stderr.write(F&#39;{_reset}] [---.--%]&#39;)
            te = meta[&#39;entropy&#39;]
            stderr.write(&#39;\b&#39; * footer_length)
            stderr.write(F&#39;] [{te!r:&gt;7}]\n&#39;)
            stderr.flush()
        if not self.isatty:
            yield data</code></pre>
</details>
</dd>
<dt id="refinery.shell.iff"><code class="flex name class">
<span>class <span class="ident">iff</span></span>
<span>(</span><span>*expression, ge=None, gt=None, le=None, lt=None, ct=None, ne=None, iN=None, eq=None, retain=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Filter incoming chunks depending on whether a given Python expression evaluates to true. If no
expression is given, the unit filters out empty chunks.</p>
<p>Note: The reverse operation of a conditional unit uses the logical negation of its condition.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/meta/iff.py#L11-L111" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class iff(ConditionalUnit, extend_docs=True):
    &#34;&#34;&#34;
    Filter incoming chunks depending on whether a given Python expression evaluates to true. If no
    expression is given, the unit filters out empty chunks.
    &#34;&#34;&#34;
    def __init__(
        self,
        *expression: Arg(metavar=&#39;token&#39;, type=str, help=(
            &#39;All &#34;token&#34; arguments to this unit are joined with spaces to produce the expression &#39;
            &#39;to be evaluated. This is done so that unnecessary shell quoting is avoided.&#39;)),
        ge: Arg(&#39;-ge&#39;, type=str, metavar=&#39;RHS&#39;, group=&#39;OP&#39;,
            help=&#39;check that the expression is greater or equal to {varname}&#39;) = None,
        gt: Arg(&#39;-gt&#39;, type=str, metavar=&#39;RHS&#39;, group=&#39;OP&#39;,
            help=&#39;check that the expression is greater than {varname}&#39;) = None,
        le: Arg(&#39;-le&#39;, type=str, metavar=&#39;RHS&#39;, group=&#39;OP&#39;,
            help=&#39;check that the expression is less or equal to {varname}&#39;) = None,
        lt: Arg(&#39;-lt&#39;, type=str, metavar=&#39;RHS&#39;, group=&#39;OP&#39;,
            help=&#39;check that the expression is less than {varname}&#39;) = None,
        ct: Arg(&#39;-ct&#39;, type=str, metavar=&#39;RHS&#39;, group=&#39;OP&#39;,
            help=&#39;check that the expression contains {varname}&#39;) = None,
        ne: Arg(&#39;-ne&#39;, type=str, metavar=&#39;RHS&#39;, group=&#39;OP&#39;,
            help=&#39;check that the expression is equal to {varname}&#39;) = None,
        iN: Arg(&#39;-in&#39;, &#39;-i&#39;, type=str, metavar=&#39;RHS&#39;, group=&#39;OP&#39;,
            help=&#39;check that the expression is contained in {varname}&#39;) = None,
        eq: Arg(&#39;-eq&#39;, &#39;-e&#39;, type=str, metavar=&#39;RHS&#39;, group=&#39;OP&#39;,
            help=&#39;check that the expression is equal to {varname}&#39;) = None,
        retain=False,
    ):
        def encodings(v: str):
            if not isinstance(v, str):
                return
            for codec in [self.codec, &#39;latin1&#39;, &#39;utf-16le&#39;]:
                yield v.encode(codec)

        def __br_contains__(container, value):
            if value in container:
                return True
            if isinstance(value, str):
                return any(b in container for b in encodings(value))
            else:
                return any(value == b for v in container for b in encodings(v))

        operators = [
            (ge, operator.__ge__),
            (gt, operator.__gt__),
            (le, operator.__le__),
            (lt, operator.__lt__),
            (eq, operator.__eq__),
            (ne, operator.__ne__),
            (ct, __br_contains__),
            (iN, lambda a, b: __br_contains__(b, a)),
        ]

        operators = [
            (rhs, cmp)
            for (rhs, cmp) in operators
            if rhs is not None
        ]

        rhs, cmp, lhs = None, None, &#39;\x20&#39;.join(expression) or None

        if len(operators) &gt; 0:
            if not lhs:
                raise ValueError(&#39;Comparison operator with empty left hand side.&#39;)
            if len(operators) &gt; 1:
                raise ValueError(&#39;Only one comparison operation can be specified.&#39;)
            rhs, cmp = operators[0]

        super().__init__(
            lhs=lhs,
            rhs=rhs,
            cmp=cmp,
            retain=retain,
        )

    def match(self, chunk):
        meta = metavars(chunk)
        lhs: Optional[str] = self.args.lhs
        rhs: Optional[Any] = self.args.rhs
        cmp: Optional[Callable[[Any, Any], bool]] = self.args.cmp

        if cmp is None and rhs is not None:
            raise ValueError(&#39;right hand side defined but no operator&#39;)

        if lhs is not None:
            if rhs is not None:
                lhs = DelayedNumSeqArgument(lhs, additional_types=(float, str))(chunk)
            else:
                lhs = PythonExpression.Evaluate(lhs, meta)

        rhs = rhs and DelayedNumSeqArgument(rhs, additional_types=(float, str))(chunk)

        self.log_info(F&#39;lhs: type={lhs.__class__.__name__}; value={lhs!r}&#39;)
        self.log_info(F&#39;rhs: type={rhs.__class__.__name__}; value={rhs!r}&#39;)

        if lhs is None:
            return bool(chunk)
        if rhs is None:
            return bool(lhs)

        return cmp(lhs, rhs)</code></pre>
</details>
</dd>
<dt id="refinery.shell.iffp"><code class="flex name class">
<span>class <span class="ident">iffp</span></span>
<span>(</span><span>*patterns, partial=False, retain=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Filter incoming chunks depending on whether it matches any of a given set of patterns. The
available patterns are the following: integer, float, number, string, multiline_string, cmdstr, ps1str, vbastr, vbaint, printable, urlquote, urlquote_coarse, urlquote_narrow, intarray, numarray, word, letters, wshenc, alphanumeric, b32, b64, b85, a85, b92, b64any, b64url, hex, uppercase_hex, spaced_hex, spaced_b64, spaced_b85, spaced_a85, utf8, hexdump, hexarray, uuencode, domain, email, guid, ipv4, ipv6, md5, sha1, sha256, hostname, socket, subdomain, url, btc, pem, xmr, path, winpath, nixpath, environment_variable.</p>
<p>Note: The reverse operation of a conditional unit uses the logical negation of its condition.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/meta/iffp.py#L12-L36" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class iffp(ConditionalUnit, extend_docs=True):
    &#34;&#34;&#34;
    Filter incoming chunks depending on whether it matches any of a given set of patterns. The
    available patterns are the following: {}.
    &#34;&#34;&#34;

    def __init__(
        self,
        *patterns: Arg.Choice(metavar=&#39;pattern&#39;, choices=_PATTERNS),
        partial: Arg.Switch(&#39;-p&#39;, help=&#39;Allow partial matches on the data.&#39;) = False,
        retain=False
    ):
        super().__init__(
            retain=retain,
            patterns=patterns,
            partial=partial
        )

    def match(self, chunk):
        for name in self.args.patterns:
            p: pattern = _PATTERNS[name]
            matcher = p.match if self.args.partial else p.fullmatch
            if matcher(chunk):
                return True
        return False</code></pre>
</details>
</dd>
<dt id="refinery.shell.iffs"><code class="flex name class">
<span>class <span class="ident">iffs</span></span>
<span>(</span><span>needle, retain=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Filter incoming chunks depending on whether they contain a given binary substring.</p>
<p>Note: The reverse operation of a conditional unit uses the logical negation of its condition.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/meta/iffs.py#L6-L21" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class iffs(ConditionalUnit, extend_docs=True):
    &#34;&#34;&#34;
    Filter incoming chunks depending on whether they contain a given binary substring.
    &#34;&#34;&#34;
    def __init__(
        self,
        needle: Arg(help=&#39;the string to search for&#39;),
        retain=False,
    ):
        super().__init__(
            needle=needle,
            retain=retain,
        )

    def match(self, chunk):
        return self.args.needle in chunk</code></pre>
</details>
</dd>
<dt id="refinery.shell.iffx"><code class="flex name class">
<span>class <span class="ident">iffx</span></span>
<span>(</span><span>regex, count=0, fullmatch=False, multiline=False, ignorecase=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Filter incoming chunks by discarding those that do not match the given
regular expression.</p>
<p>Note: The reverse operation of a conditional unit uses the logical negation of its condition.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/meta/iffx.py#L7-L13" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class iffx(SingleRegexUnit, ConditionalUnit, extend_docs=True):
    &#34;&#34;&#34;
    Filter incoming chunks by discarding those that do not match the given
    regular expression.
    &#34;&#34;&#34;
    def match(self, chunk):
        return bool(self.matcher(chunk))</code></pre>
</details>
</dd>
<dt id="refinery.shell.ifps"><code class="flex name class">
<span>class <span class="ident">ifps</span></span>
<span>(</span><span>codec='cp1252')</span>
</code></dt>
<dd>
<section class="desc"><p>Disassembles compiled Pascal script files that start with the magic sequence "IFPS". These
scripts can be found, for example, when unpacking InnoSetup installers using innounp.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/formats/ifps.py#L917-L927" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class ifps(IFPSBase):
    &#34;&#34;&#34;
    Disassembles compiled Pascal script files that start with the magic sequence &#34;IFPS&#34;. These
    scripts can be found, for example, when unpacking InnoSetup installers using innounp.
    &#34;&#34;&#34;
    def process(self, data):
        return str(IFPSFile(data, self.args.codec)).encode(self.codec)

    @classmethod
    def handles(self, data: bytearray) -&gt; bool:
        return data.startswith(IFPSFile.Magic)</code></pre>
</details>
</dd>
<dt id="refinery.shell.ifpsstr"><code class="flex name class">
<span>class <span class="ident">ifpsstr</span></span>
<span>(</span><span>codec='cp1252')</span>
</code></dt>
<dd>
<section class="desc"><p>Extracts strings from compiled Pascal script files that start with the magic sequence "IFPS".
These scripts can be found, for example, when unpacking InnoSetup installers using innounp.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/formats/ifpsstr.py#L6-L18" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class ifpsstr(IFPSBase):
    &#34;&#34;&#34;
    Extracts strings from compiled Pascal script files that start with the magic sequence &#34;IFPS&#34;.
    These scripts can be found, for example, when unpacking InnoSetup installers using innounp.
    &#34;&#34;&#34;
    def process(self, data):
        ifps = IFPSFile(data, self.args.codec)
        for string in ifps.strings:
            yield string.encode(self.codec)

    @classmethod
    def handles(self, data: bytearray) -&gt; bool:
        return data.startswith(IFPSFile.Magic)</code></pre>
</details>
</dd>
<dt id="refinery.shell.imphash"><code class="flex name class">
<span>class <span class="ident">imphash</span></span>
<span>(</span><span>text=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Implements the import hash for PE files.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/crypto/hash/imphash.py#L10-L21" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class imphash(HashUnit):
    &#34;&#34;&#34;
    Implements the import hash for PE files.
    &#34;&#34;&#34;

    def _algorithm(self, data):
        pe = PE(data=data, fast_load=True)
        pe.parse_data_directories(directories=[IMAGE_DIRECTORY_ENTRY_IMPORT])
        th = pe.get_imphash()
        if not th:
            raise ValueError(&#39;no import directory.&#39;)
        return bytes.fromhex(th)</code></pre>
</details>
</dd>
<dt id="refinery.shell.isaac"><code class="flex name class">
<span>class <span class="ident">isaac</span></span>
<span>(</span><span>key, discard=0, stateful=False)</span>
</code></dt>
<dd>
<section class="desc"><p>The ISAAC (Indirection, Shift, Accumulate, Add, Count) cipher.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/crypto/cipher/isaac.py#L10-L67" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class isaac(StreamCipherUnit):
    &#34;&#34;&#34;
    The ISAAC (Indirection, Shift, Accumulate, Add, Count) cipher.
    &#34;&#34;&#34;

    def keystream(self) -&gt; Iterable[int]:
        key = self.args.key

        A: int = 0
        B: int = 0
        C: int = 0
        S: List[int] = [0x9E3779B9] * 8
        T: List[int] = []
        K = list(chunks.unpack(key + bytearray(0x400 - len(key)), 4, bigendian=False))
        U = 0xFFFFFFFF

        def _mix_state():
            a, b, c, d, e, f, g, h = S
            a ^= (b &lt;&lt; 0x0B) &amp; U; d = d + a &amp; U; b = b + c &amp; U # noqa
            b ^= (c &gt;&gt; 0x02) &amp; U; e = e + b &amp; U; c = c + d &amp; U # noqa
            c ^= (d &lt;&lt; 0x08) &amp; U; f = f + c &amp; U; d = d + e &amp; U # noqa
            d ^= (e &gt;&gt; 0x10) &amp; U; g = g + d &amp; U; e = e + f &amp; U # noqa
            e ^= (f &lt;&lt; 0x0A) &amp; U; h = h + e &amp; U; f = f + g &amp; U # noqa
            f ^= (g &gt;&gt; 0x04) &amp; U; a = a + f &amp; U; g = g + h &amp; U # noqa
            g ^= (h &lt;&lt; 0x08) &amp; U; b = b + g &amp; U; h = h + a &amp; U # noqa
            h ^= (a &gt;&gt; 0x09) &amp; U; c = c + h &amp; U; a = a + b &amp; U # noqa
            S[:] = a, b, c, d, e, f, g, h
            return S

        def _initialize_with(R: List[int]):
            for i in range(0, 0x100, 8):
                S[:] = (x + R[j] &amp; U for j, x in enumerate(S, i))
                T[i:i + 8] = _mix_state()

        for _ in range(4):
            _mix_state()

        _initialize_with(K)
        _initialize_with(T)

        operations = [
            (__lshift__, 0x0D),
            (__rshift__, 0x06),
            (__lshift__, 0x02),
            (__rshift__, 0x10),
        ]

        while True:
            C = (C + 1) &amp; U
            B = (B + C) &amp; U
            for i in range(0x100):
                X = T[i]
                shift, k = operations[i % 4]
                A = (A ^ shift(A, k)) &amp; U
                A = (A + T[i ^ 0x80]) &amp; U
                Y = T[+i] = T[X // 4 &amp; 0xFF] + A + B &amp; U
                B = K[~i] = X + T[Y // 1024 &amp; 0xFF] &amp; U
            yield from chunks.pack(K, 4, True)</code></pre>
</details>
</dd>
<dt id="refinery.shell.jcalg"><code class="flex name class">
<span>class <span class="ident">jcalg</span></span>
<span>(</span><span>ignore_header=False)</span>
</code></dt>
<dd>
<section class="desc"><p>JCALG decompression.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/compression/jcalg.py#L11-L126" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class jcalg(Unit):
    &#34;&#34;&#34;
    JCALG decompression.
    &#34;&#34;&#34;
    def __init__(
        self,
        ignore_header: Unit.Arg(&#39;-g&#39;, help=(
            &#39;Keep decompressing even after the output has reached the final size as given by the header value.&#39;)) = False,
    ):
        super().__init__(ignore_header=ignore_header)

    def process(self, data: bytearray):
        with MemoryFile() as output, StructReader(data) as reader:
            if reader.read(2) != B&#39;JC&#39;:
                self.log_warn(&#39;data does not begin with magic sequence, assuming that header is missing&#39;)
                reader.seek(0)
                size = checksum = None
            else:
                size = reader.u32()
                checksum = reader.u32()
            if self.args.ignore_header:
                size = None
            self._decompress(output, reader, size)
            if size is not None:
                if len(output) &gt; size:
                    self.log_info(F&#39;tuncating to size {size}&#39;)
                    output.truncate(size)
                elif len(output) &lt; size:
                    self.log_warn(F&#39;header size was {size}, but only {len(data)} bytes were decompressed&#39;)
            data = output.getvalue()
            if checksum:
                c = self._checksum(data)
                if c != checksum:
                    self.log_warn(F&#39;header checksum was {checksum:08X}, computed value is {c:08X}&#39;)
            return data

    @classmethod
    def handles(cls, data: bytearray):
        if data[:2] == B&#39;JC&#39;:
            return True

    def _checksum(self, data):
        from refinery.lib import chunks
        checksum = 0
        it = chunks.unpack(data, 4)
        if len(data) % 4:
            import itertools
            it = itertools.chain(it, (int.from_bytes(data[-4:], &#39;little&#39;),))
        for chunk in it:
            checksum += chunk
            checksum ^= ((chunk &amp; 0x7FFFFFFF) &lt;&lt; 1) + (chunk &gt;&gt; 31) + 1
            checksum &amp;= 0xFFFFFFFF
        return checksum

    def _decompress(self, writer: MemoryFile, reader_: StructReader[bytearray], size: Optional[int] = None):
        index = 1
        base = 8
        literal_bits = None
        literal_offset = None
        flags = BitBufferedReader(reader_, 32)

        while True:
            if size and len(writer) &gt;= size:
                break
            if flags.next():
                b = flags.read(literal_bits) + literal_offset
                b = b &amp; 0xFF
                writer.write_byte(b)
                continue
            if flags.next():
                high = flags.variable_length_integer()
                if high == 2:
                    match_length = flags.variable_length_integer()
                else:
                    index = ((high - 3) &lt;&lt; base) + flags.read(base)
                    match_length = flags.variable_length_integer()
                    if index &gt;= 0x10000:
                        match_length += 3
                    elif index &gt;= 0x37FF:
                        match_length += 2
                    elif index &gt;= 0x27F:
                        match_length += 1
                    elif index &lt;= 127:
                        match_length += 4
                writer.replay(index, match_length)
                continue
            if not flags.next():
                new_index = flags.read(7)
                match_length = 2 + flags.read(2)
                if new_index == 0:
                    if match_length == 2:
                        break
                    base = flags.read(match_length + 1)
                else:
                    index = new_index
                    writer.replay(index, match_length)
                continue
            one_byte_phrase_value = flags.read(4) - 1
            if one_byte_phrase_value == 0:
                writer.write_byte(0)
            elif one_byte_phrase_value &gt; 0:
                b = writer.getbuffer()[-one_byte_phrase_value]
                writer.write_byte(b)
            else:
                if not flags.next():
                    literal_bits = 7 + flags.next()
                    literal_offset = 0
                    if literal_bits != 8:
                        literal_offset = flags.read(8)
                    continue
                while True:
                    for _ in range(0x100):
                        b = flags.read(8)
                        writer.write_byte(b)
                    if not flags.next():
                        break</code></pre>
</details>
</dd>
<dt id="refinery.shell.jvdasm"><code class="flex name class">
<span>class <span class="ident">jvdasm</span></span>
<span>(</span><span>*paths, gray=False, path=b'path', regex=False, exact=False, fuzzy=0, drop_path=False, join_path=False, list=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Disassembles the JVM bytecode instructions of methods of classes defined in Java class
files. The unit is implemented as a path extractor and each path name corresponds to the
name of one method defined in the class file.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/formats/java/jvdasm.py#L63-L214" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class jvdasm(PathExtractorUnit):
    &#34;&#34;&#34;
    Disassembles the JVM bytecode instructions of methods of classes defined in Java class
    files. The unit is implemented as a path extractor and each path name corresponds to the
    name of one method defined in the class file.
    &#34;&#34;&#34;
    _OPC_STRLEN = max(len(op.name) for op in opc)

    def _hex(self, bytestring, sep=&#39;&#39;):
        return sep.join(F&#39;{x:02x}&#39; for x in bytestring)

    def __init__(
        self, *paths,
        gray: PathExtractorUnit.Arg.Switch(&#39;-g&#39;, help=&#39;Disable colored output.&#39;) = False,
        **keywords
    ):
        super().__init__(*paths, gray=gray, **keywords)

    def unpack(self, data):
        def _name(method: JvClassMember):
            name = method.name
            if name == &#39;&lt;init&gt;&#39;:
                _, _, name = str(jc.this).rpartition(&#39;/&#39;)
            elif m := re.fullmatch(&#39;&lt;(.*?)&gt;&#39;, name):
                name = F&#39;.{m[0]}&#39;
            return name

        def _path(method: JvClassMember):
            return F&#39;{jc.this!s}/{_name(method)}&#39;
        try:
            if self.args.gray or not self.isatty:
                raise ImportError
            import colorama
        except ImportError:
            class _FG():
                def __getattr__(self, _):
                    return &#39;&#39;
            FG = _FG()
            RS = &#39;&#39;
        else:
            FG = colorama.Fore
            RS = colorama.Style.RESET_ALL
        finally:
            c_none = RS
            c_space = FG.LIGHTCYAN_EX
            c_types = FG.LIGHTCYAN_EX
            c_member = FG.LIGHTYELLOW_EX
            c_kwd = FG.LIGHTYELLOW_EX
            c_const = FG.LIGHTRED_EX
            c_string = FG.LIGHTRED_EX
            c_address = FG.LIGHTBLACK_EX
            c_label = RS

        def _color(arg, offset):
            if isinstance(arg, (str, JvString)):
                color = c_string
            elif isinstance(arg, (JvClassProperty, JvTypePath)):
                ns, dd, prop = str(arg).partition(&#39;::&#39;)
                if not dd:
                    return repr(arg)
                ns = ns.split(&#39;.&#39;)
                ns = &#39;.&#39;.join(F&#39;{c_space}{p}{c_none}&#39; for p in ns)
                return F&#39;{ns}{dd}{c_member}{prop}{c_none}&#39;
            elif isinstance(arg, int) and arg + offset in labels:
                return F&#39;{c_label}0x{arg + offset:08X}{c_none}&#39;
            elif isinstance(arg, (bool, int, float)):
                color = c_const
            elif isinstance(arg, JvBaseType):
                color = c_kwd
            else:
                return repr(arg)
            return F&#39;{color}{arg!r}{c_none}&#39;

        jc = JvClassFile(data)
        tab = &#39; &#39;
        namespace = &#39;.&#39;.join(str(jc.this).split(&#39;/&#39;))
        opcw = self._OPC_STRLEN
        path_counter = collections.defaultdict(int)
        path_index = collections.defaultdict(int)

        for method in jc.methods:
            path_counter[_path(method)] += 1
        for method in jc.methods:
            for attribute in method.attributes:
                if attribute.name == &#39;Code&#39;: break
            else:
                self.log_warn(F&#39;no code found for method: {method.name}&#39;)
                continue
            code: JvCode = attribute.parse(JvCode)
            with io.StringIO() as display:
                rv, args = _parse_descriptor(method.descriptor, c_none, c_space, c_types, c_kwd)
                args = &#39;, &#39;.join(args)
                print(
                    F&#39;{c_types}{rv}{c_none} {c_space}{namespace}{c_none}&#39;
                    F&#39;::{c_member}{_name(method)}{c_none}({args})&#39;, file=display)
                offset = 0
                labels = set()
                addresses = set()

                for op in code.disassembly:
                    addresses.add(offset)
                    if op.table:
                        labels.update(offset + jmp for jmp in op.table.values())
                    elif op.code in (opc.goto, opc.goto_w):
                        labels.update(offset + arg for arg in op.arguments if isinstance(arg, int))
                    offset += len(op.raw)

                offset = 0
                labels = labels &amp; addresses

                for op in code.disassembly:
                    if offset in labels:
                        label = F&#39;{c_label}{offset:08X}{c_none}:&#39;
                    else:
                        label = F&#39;{c_address}{offset:08X}{c_none}:&#39;
                    addr = offset
                    olen = len(op.raw)
                    offset += olen
                    if op.table is None:
                        args = &#39;, &#39;.join(_color(a, addr) for a in op.arguments)
                    else:
                        ow = 4 if op.code is opc.tableswitch else 8
                        olen = olen - (len(op.table) - 1) * ow
                        args = F&#39;___default =&gt; {c_label}{op.table[None] + addr:#010x}{c_none}&#39;
                        jmps = []
                        for k, (key, jmp) in enumerate(op.table.items()):
                            if key is None:
                                continue
                            raw = self._hex(op.raw[olen + k * ow: olen + k * ow + ow], &#39; &#39;)
                            jmps.append(
                                F&#39;{label}{tab}&#39;
                                F&#39;{raw!s:&lt;{opcw + 15}} &#39;
                                F&#39;{c_const}{key:#010x}{c_none} =&gt; &#39;
                                F&#39;{c_label}{jmp + addr:#010x}{c_none}&#39;)
                        args = &#39;\n&#39;.join((args, *jmps))
                    opch = self._hex(op.raw[:olen], &#39; &#39;)
                    if len(opch) &gt; 14:
                        opch += F&#39;\n{label}{tab}{tab:&lt;15}&#39;
                    print(
                        F&#39;{label}{tab}&#39;
                        F&#39;{opch:&lt;15}&#39;
                        F&#39;{c_kwd}{op.code!r:&lt;{opcw}}{c_none} {args}&#39;, file=display)
                path = _path(method)
                if path_counter[path] &gt; 1:
                    k = path_index[path]
                    path_index[path] = k + 1
                    path = F&#39;{path}[{k}]&#39;
                yield UnpackResult(path, display.getvalue().encode(self.codec))

    @classmethod
    def handles(self, data):
        return data[:4] == B&#39;\xCA\xFE\xBA\xBE&#39;</code></pre>
</details>
</dd>
<dt id="refinery.shell.jvstr"><code class="flex name class">
<span>class <span class="ident">jvstr</span></span>
</code></dt>
<dd>
<section class="desc"><p>Extract string constants from Java class files.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/formats/java/jvstr.py#L7-L18" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class jvstr(Unit):
    &#34;&#34;&#34;
    Extract string constants from Java class files.
    &#34;&#34;&#34;
    def process(self, data):
        jc = JvClassFile(data)
        for string in jc.strings:
            yield string.encode(self.codec)

    @classmethod
    def handles(self, data):
        return data[:4] == B&#39;\xCA\xFE\xBA\xBE&#39;</code></pre>
</details>
</dd>
<dt id="refinery.shell.kblob"><code class="flex name class">
<span>class <span class="ident">kblob</span></span>
</code></dt>
<dd>
<section class="desc"><p>Extracts a key from a Microsoft Crypto API BLOB structure.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/crypto/keyderive/kblob.py#L7-L21" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class kblob(Unit):
    &#34;&#34;&#34;
    Extracts a key from a Microsoft Crypto API BLOB structure.
    &#34;&#34;&#34;

    def process(self, data):
        blob = CRYPTOKEY(data)
        try:
            return self.labelled(
                bytes(blob.key),
                type=blob.header.type.name,
                algorithm=blob.header.algorithm.name
            )
        except AttributeError as A:
            raise ValueError(F&#39;unable to derive key from {blob.header.type!s}&#39;) from A</code></pre>
</details>
</dd>
<dt id="refinery.shell.keccak256"><code class="flex name class">
<span>class <span class="ident">keccak256</span></span>
<span>(</span><span>text=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Returns the KECCAK256 hash of the input data.</p></section>
</dd>
<dt id="refinery.shell.kramer"><code class="flex name class">
<span>class <span class="ident">kramer</span></span>
</code></dt>
<dd>
<section class="desc"><p>Deobfuscate Python samples obfuscated with Kramer.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/malware/kramer.py#L17-L111" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class kramer(Unit):
    &#34;&#34;&#34;
    Deobfuscate Python samples obfuscated with Kramer.
    &#34;&#34;&#34;

    _LINEBREAK_MAGIC = 950

    def process(self, data):
        kramer = str()
        secret = set()
        _pyver = None

        def crawl(code: CodeType, depth=1):
            nonlocal kramer
            nonlocal secret
            for instruction in disassemble_code(code, _pyver):
                arg = instruction.argval
                if arg is None:
                    continue
                if isinstance(arg, tuple):
                    continue
                if isinstance(arg, str):
                    if len(arg) &gt; len(kramer):
                        kramer = arg
                    continue
                if isinstance(arg, int):
                    secret.add(arg)
                    continue
                try:
                    crawl(arg, depth + 1)
                except Exception as E:
                    self.log_info(F&#39;error crawling arg of type {type(arg).__name__} at depth {depth}: {E}&#39;)

        for code in extract_code_from_buffer(bytes(data)):
            _pyver = code.version
            crawl(code.container)

        if not kramer:
            raise ValueError(&#39;could not find the encoded string&#39;)

        separator = re.search(&#39;[^a-fA-F0-9]+&#39;, kramer)

        if not separator:
            raise ValueError(&#39;no separator detected; encoding method may have changed&#39;)

        def rotchar(c: int):
            if c in range(0x61, 0x7a) or c in range(0x30, 0x39):
                return c + 1
            if c == 0x7a:
                return 0x30
            if c == 0x39:
                return 0x61
            return c

        def decrypt(c: int, k: int):
            if c &gt;= k:
                out = rotchar(c - k)
                if out not in range(0x100):
                    raise _WrongKey
                return out
            if c == self._LINEBREAK_MAGIC:
                return 0x0A
            raise _WrongKey

        def decrypt_with_key(key: int):
            decrypted = bytearray(decrypt(c, key) for c in encrypted)
            if not re.fullmatch(B&#39;[\\s!-~]+&#39;, decrypted):
                raise _WrongKey
            return decrypted

        separator = separator.group(0)
        encrypted = [ord(bytes.fromhex(e).decode()) for e in kramer.split(separator)]

        ubound = min(x for x in encrypted if x != self._LINEBREAK_MAGIC)
        lbound = ubound - 0xFF

        secret = {k for k in secret if k &gt; lbound and k &lt; ubound}
        self.log_debug(&#39;potential secrets from code:&#39;, secret)

        for key in sorted(secret, reverse=True):
            try:
                return decrypt_with_key(key)
            except _WrongKey:
                pass

        self.log_info(F&#39;all candidates failed, searching [{lbound}, {ubound}]&#39;)

        for key in range(ubound, lbound - 1, -1):
            try:
                self.log_debug(&#39;attempting key:&#39;, key)
                return decrypt_with_key(key)
            except _WrongKey:
                pass

        raise RuntimeError(&#39;could not find decryption key&#39;)</code></pre>
</details>
</dd>
<dt id="refinery.shell.lnk"><code class="flex name class">
<span>class <span class="ident">lnk</span></span>
<span>(</span><span>tabular=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Parse Windows Shortcuts (LNK files) and returns the parsed information in JSON format. This
unit is a thin wrapper around the LnkParse3 library.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/formats/lnk.py#L10-L30" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class lnk(Unit):
    &#34;&#34;&#34;
    Parse Windows Shortcuts (LNK files) and returns the parsed information in JSON format. This
    unit is a thin wrapper around the LnkParse3 library.
    &#34;&#34;&#34;

    @Unit.Requires(&#39;LnkParse3&gt;=1.4.0&#39;, &#39;formats&#39;, &#39;default&#39;, &#39;extended&#39;)
    def _LnkParse3():
        import LnkParse3
        return LnkParse3

    def __init__(self, tabular: Unit.Arg(&#39;-t&#39;, help=&#39;Print information in a table rather than as JSON&#39;) = False):
        super().__init__(tabular=tabular)

    def process(self, data):
        with NoLogging():
            parsed = self._LnkParse3.lnk_file(MemoryFile(data)).get_json()
        with JSONEncoderEx as encoder:
            pp = ppjson(tabular=self.args.tabular)
            yield from pp._pretty_output(
                parsed, indent=4, cls=encoder, ensure_ascii=False)</code></pre>
</details>
</dd>
<dt id="refinery.shell.loop"><code class="flex name class">
<span>class <span class="ident">loop</span></span>
<span>(</span><span>count, suffix, do_while, do_until, fullmatch=False, multiline=False, ignorecase=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Applies a given multibin suffix to the input chunk repeatedly. For example, the following
command would carve the largest base64-encoded buffer from the input, decode it, and then
decompress the result 20 times:</p>
<pre><code>emit data | loop 20 csd[b64]:zl
</code></pre>
<p>Notably, the argument after the count is a suffix, which means that handlers are applied
from left to right (not from right to left). The loop is aborted and the previous result
returned if the newly computed result is empty. If the an error occurs while computing
the suffix and the unit is lenient (i.e. the <code>-L</code> switch is set), the last known result
is returned.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/meta/loop.py#L9-L78" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class loop(RegexUnit):
    &#34;&#34;&#34;
    Applies a given multibin suffix to the input chunk repeatedly. For example, the following
    command would carve the largest base64-encoded buffer from the input, decode it, and then
    decompress the result 20 times:

        emit data | loop 20 csd[b64]:zl

    Notably, the argument after the count is a suffix, which means that handlers are applied
    from left to right (not from right to left). The loop is aborted and the previous result
    returned if the newly computed result is empty. If the an error occurs while computing
    the suffix and the unit is lenient (i.e. the `-L` switch is set), the last known result
    is returned.
    &#34;&#34;&#34;

    def __init__(
        self,
        count: Arg.Number(metavar=&#39;count&#39;, help=&#39;The number of repeated applications of the suffix.&#39;),
        suffix: Arg(type=str, help=&#39;A multibin expression suffix.&#39;),
        do_while: Arg(&#39;-w&#39;, &#39;--while&#39;, type=regexp, metavar=&#39;RE&#39;,
            help=&#39;Halt when the given regular expression does not match the data.&#39;),
        do_until: Arg(&#39;-u&#39;, &#39;--until&#39;, type=regexp, metavar=&#39;RE&#39;,
            help=&#39;Halt when the given regular expression matches the data.&#39;),
        fullmatch=False, multiline=False, ignorecase=False,
    ):
        super().__init__(
            count=count,
            suffix=suffix,
            do_while=do_while,
            do_until=do_until,
            fullmatch=fullmatch,
            multiline=multiline,
            ignorecase=ignorecase,
        )

    def process(self, data):
        _count = self.args.count
        _width = len(str(_count))
        _while = self._while
        _until = self._until

        for k in range(_count):
            if _while and not _while(data):
                self.log_info(F&#39;step {k:0{_width}}: stopping, while-condition violated&#39;)
                break
            if _until and _until(data):
                self.log_info(F&#39;step {k:0{_width}}: stopping, until-condition satisfied&#39;)
                break
            try:
                out = DelayedBinaryArgument(
                    self.args.suffix, reverse=True, seed=data)(data)
            except Exception as error:
                self.log_info(F&#39;step {k:0{_width}}: error;&#39;, exception_to_string(error))
                msg = F&#39;Stopped after {k} steps, increase verbosity for additional details.&#39;
                raise RefineryPartialResult(msg, data) from error
            if not out:
                self.log_info(F&#39;step {k:0{_width}}: stopping after empty result&#39;)
                break
            data[:] = out
            self.log_debug(F&#39;step {k:0{_width}}: data =&#39;, data, clip=True)

        return data

    @property
    def _while(self):
        return self._make_matcher(self.args.do_while)

    @property
    def _until(self):
        return self._make_matcher(self.args.do_until)</code></pre>
</details>
</dd>
<dt id="refinery.shell.lz4"><code class="flex name class">
<span>class <span class="ident">lz4</span></span>
</code></dt>
<dd>
<section class="desc"><p>LZ4 block decompression. See also:
<a href="https://github.com/lz4/lz4/blob/master/doc/lz4_Block_format.md#compressed-block-format">https://github.com/lz4/lz4/blob/master/doc/lz4_Block_format.md#compressed-block-format</a></p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/compression/lz4.py#L21-L149" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class lz4(Unit):
    &#34;&#34;&#34;
    LZ4 block decompression. See also:
    https://github.com/lz4/lz4/blob/master/doc/lz4_Block_format.md#compressed-block-format
    &#34;&#34;&#34;
    def _read_block(self, reader: StructReader, output: io.BytesIO, ubound=None):
        entry = reader.tell()
        lastend = 0

        def ubound_check():
            if ubound is None:
                return False
            consumed = reader.tell() - entry
            if consumed &gt; ubound:
                raise ValueError(F&#39;upper bound {ubound} exceeded by {consumed - ubound} in LZ4 block&#39;)
            return consumed == ubound

        while not reader.eof:
            reflen = reader.read_nibble()
            litlen = reader.read_nibble()
            litlen = reader.read_size(litlen)
            literal = reader.read(litlen)
            output.write(literal)
            if ubound_check():
                break
            try:
                refpos = reader.u16()
            except EOFError:
                break
            if refpos - 1 not in range(output.tell()):
                with StreamDetour(output, lastend):
                    if output.read(len(literal)) == literal:
                        # This literal could have been encoded in the last match, but it wasn&#39;t.
                        # Therefore, it is very likely that we have reached the end of the stream.
                        break
                position = reader.tell()
                remaining = len(literal) - position
                raise RefineryPartialResult(
                    F&#39;encountered invalid match offset value {refpos} at position {position} with {remaining} bytes remaining&#39;,
                    partial=output.getvalue())
            reflen = reader.read_size(reflen)
            if ubound_check():
                raise ValueError(&#39;last sequence in block contained a match&#39;)
            reflen += 4
            available_bytes = min(refpos, reflen)
            q, r = divmod(reflen, available_bytes)
            with StreamDetour(output, -refpos, io.SEEK_CUR):
                match = output.read(available_bytes)
                match = q * match + match[:r]
                assert len(match) == reflen
                lastend = output.tell() - available_bytes + r
            output.write(match)

    def process(self, data):
        output = io.BytesIO()
        reader = LZ4Reader(memoryview(data))
        try:
            magic = reader.u32() == 0x184D2204
        except EOFError:
            magic = False
        if not magic:
            reader.seek(0)
            self._read_block(reader, output)
            return output.getbuffer()

        (dict_id, rsrv1, content_checksummed, content_size,
            blocks_checksummed, blocks_independent, v2, v1) = reader.read_bits(8)
        rsrv2 = reader.read_nibble()
        try:
            block_maximum = {
                7: 0x400000,
                6: 0x100000,
                5: 0x040000,
                4: 0x010000,
            }[reader.read_integer(3)]
        except KeyError:
            raise ValueError(&#39;unknown maximum block size value in LZ4 frame header&#39;)
        rsrv3 = reader.read_bit()
        if any((rsrv1, rsrv2, rsrv3)):
            self.log_warn(&#39;nonzero reserved value in LZ4 frame header&#39;)
        if (v1, v2) != (0, 1):
            self.log_warn(F&#39;invalid version ({v1},{v2}) in LZ4 frame header&#39;)
        content_size = content_size and reader.u64() or None
        dict_id = dict_id and reader.u32() or None
        # Header Checksum
        xxh = xxhash(data[4:reader.tell()]).intdigest() &gt;&gt; 8 &amp; 0xFF
        chk = reader.read_byte()
        if chk != xxh:
            self.log_warn(F&#39;header checksum {chk:02X} does not match computed value {xxh:02X}&#39;)

        self.log_debug(lambda: F&#39;dictionary id: {dict_id}&#39;)
        self.log_debug(lambda: F&#39;block max: 0x{block_maximum:X}&#39;)
        if content_size is not None:
            self.log_debug(lambda: F&#39;chunk max: 0x{content_size:X}&#39;)
        self.log_debug(lambda: F&#39;blocks independent: {bool(blocks_independent)}&#39;)
        self.log_debug(lambda: F&#39;blocks checksummed: {bool(blocks_checksummed)}&#39;)

        blockindex = 0

        while True:
            blockindex += 1
            size = reader.read_integer(31)
            uncompressed = reader.read_bit()
            if not size:
                assert not uncompressed
                break
            self.log_info(F&#39;reading block of size 0x{size:06X}&#39;)
            assert reader.byte_aligned
            assert size &lt;= block_maximum, &#39;block size exceeds maximum size&#39;
            if uncompressed:
                output.write(reader.read_exactly(size))
            else:
                self._read_block(reader, output, size)
            if blocks_checksummed:
                with StreamDetour(reader, -size, io.SEEK_CUR):
                    xxh = xxhash(reader.read_exactly(size)).intdigest()
                chk = reader.u32()
                if chk != xxh:
                    self.log_warn(F&#39;block {blockindex} had checksum {chk:08X} which did not match computed value {xxh:08X}&#39;)
        if content_checksummed:
            self.log_info(&#39;computing checksum&#39;)
            xxh = xxhash(output.getbuffer()).intdigest()
            chk = reader.u32()
            if chk != xxh:
                self.log_warn(F&#39;the given checksum {chk:08X} did not match the computed checksum {xxh:08X}&#39;)
        if not reader.eof:
            pos = reader.tell()
            self.log_warn(F&#39;found {len(data) - pos} additional bytes starting at position 0x{pos:X} after compressed data&#39;)
        return output.getbuffer()</code></pre>
</details>
</dd>
<dt id="refinery.shell.lzf"><code class="flex name class">
<span>class <span class="ident">lzf</span></span>
<span>(</span><span>fast=False)</span>
</code></dt>
<dd>
<section class="desc"><p>This unit implements LZF compression and decompression.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/compression/lzf.py#L36-L202" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class lzf(Unit):
    &#34;&#34;&#34;
    This unit implements LZF compression and decompression.
    &#34;&#34;&#34;

    def __init__(self, fast: Arg.Switch(&#39;-x&#39;, help=&#39;Enable fast compression mode.&#39;) = False):
        super().__init__(fast=fast)

    def reverse(self, data):
        def FRST(p: memoryview) -&gt; int:
            return ((p[0]) &lt;&lt; 8) | p[1]

        def NEXT(v: int, p: memoryview) -&gt; int:
            return ((v &lt;&lt; 8) | p[2]) &amp; 0xFFFFFFFF

        def DELTA(p: memoryview):
            return view.nbytes - p.nbytes

        if self.args.fast:
            def HIDX(h: int) -&gt; int:
                return (((h &gt;&gt; (3 * 8 - _HSLOG)) - h * 5) &amp; (_HSIZE - 1))
        else:
            def HIDX(h: int) -&gt; int:
                q = (h ^ (h &lt;&lt; 5))
                return (((q &gt;&gt; (3 * 8 - _HSLOG)) - h * 5) &amp; (_HSIZE - 1))

        if not data:
            return data

        ip = view = memoryview(data)
        op = bytearray()

        if len(data) == 1:
            op.append(0)
            op.extend(data)
            return op

        hval = FRST(ip)
        htab = [0] * _HSIZE
        fast = 1 if self.args.fast else 0

        lit = 0

        def begin_literal():
            nonlocal lit
            op.append(0)
            lit = 0

        def advance_literal():
            nonlocal lit, ip
            lit += 1
            op.append(ip[0])
            ip = ip[1:]
            if lit == _MAX_LIT:
                op[-lit - 1] = lit - 1
                begin_literal()

        def commit_literal():
            if lit &gt; 0:
                op[-lit - 1] = lit - 1
            else:
                op.pop()

        begin_literal()

        while ip.nbytes &gt; 2:
            hval = NEXT(hval, ip)
            hpos = HIDX(hval)
            ipos = DELTA(ip)
            length = 2
            r, htab[hpos] = htab[hpos], ipos
            off = ipos - r - 1
            ref = view[r:]

            if off &gt;= _MAX_OFF or r &lt;= 0 or ref[:3] != ip[:3]:
                advance_literal()
                continue
            else:
                commit_literal()

            maxlen = min(_MAX_REF, ip.nbytes - length)

            while True:
                length += 1
                if length &gt;= maxlen or ref[length] != ip[length]:
                    length -= 2
                    break

            if length &lt; 7:
                op.append((off &gt;&gt; 8) + (length &lt;&lt; 5))
            else:
                op.append((off &gt;&gt; 8) + (7 &lt;&lt; 5))
                op.append(length - 7)

            op.append(off &amp; 0xFF)
            begin_literal()

            if ip.nbytes &lt;= length + 3:
                ip = ip[length + 2:]
                break
            if fast:
                ip = ip[length:]
                hval = FRST(ip)
                for _ in range(2):
                    hval = NEXT(hval, ip)
                    htab[HIDX(hval)] = DELTA(ip)
                    ip = ip[1:]
            else:
                ip = ip[1:]
                for _ in range(length + 1):
                    hval = NEXT(hval, ip)
                    htab[HIDX(hval)] = DELTA(ip)
                    ip = ip[1:]
        while ip.nbytes:
            advance_literal()
        commit_literal()
        return op

    def _decompress_chunk(self, data: memoryview, out: MemoryFile):
        ip = StructReader(data)
        while not ip.eof:
            ctrl = ip.u8()
            if ctrl &lt; 0B100000:
                ctrl += 1
                out.write(ip.read_exactly(ctrl))
            else:
                length = ctrl &gt;&gt; 5
                offset = 1 + ((ctrl &amp; 0B11111) &lt;&lt; 8)
                if length == 7:
                    length += ip.u8()
                offset += ip.u8()
                length += 2
                out.replay(offset, length)

    def process(self, data):
        mem = memoryview(data)
        out = MemoryFile()

        try:
            reader = StructReader(mem)
            header = LZFHeader(reader)
        except Exception:
            self.log_info(&#39;no header detected, decompressing as raw stream&#39;)
            self._decompress_chunk(mem, out)
            return out.getvalue()

        for k in itertools.count(1):
            self.log_info(F&#39;chunk: e=0x{header.encoded_size:04X} d=0x{header.decoded_size:04X}&#39;)
            chunk = reader.read(header.encoded_size)
            if header.compressed:
                self._decompress_chunk(chunk, out)
            else:
                out.write(chunk)
            if reader.eof:
                break
            try:
                header = LZFHeader(reader)
            except Exception as E:
                msg = F&#39;failed parsing next header after {k} chunks: {E!s}&#39;
                raise RefineryPartialResult(msg, out.getvalue())

        return out.getvalue()

    @classmethod
    def handles(self, data: bytearray):
        if data[:2] == LZFHeader.MAGIC:
            return True</code></pre>
</details>
</dd>
<dt id="refinery.shell.lzg"><code class="flex name class">
<span>class <span class="ident">lzg</span></span>
</code></dt>
<dd>
<section class="desc"><p>LZG decompression.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/compression/lzg.py#L181-L196" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class lzg(Unit):
    &#34;&#34;&#34;
    LZG decompression.
    &#34;&#34;&#34;
    def process(self, data: bytearray):
        stream = LZGStream(data)
        out = stream.decompress()
        if len(out) != stream.decoded_size:
            msg = F&#39;LZG header announced {stream.decoded_size} bytes, but decompressed buffer had size {len(out)}.&#39;
            raise RefineryPartialResult(msg, out)
        return out

    @classmethod
    def handles(cls, data: bytearray):
        if data[:3] == B&#39;LZG&#39;:
            return True</code></pre>
</details>
</dd>
<dt id="refinery.shell.lzip"><code class="flex name class">
<span>class <span class="ident">lzip</span></span>
</code></dt>
<dd>
<section class="desc"><p>LZIP decompression</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/compression/lzip.py#L329-L376" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class lzip(Unit):
    &#34;&#34;&#34;
    LZIP decompression
    &#34;&#34;&#34;
    def process(self, data: bytearray):
        view = memoryview(data)
        with MemoryFile() as output, StructReader(view) as reader:
            for k in count(1):
                if reader.eof:
                    break
                trailing_size = len(data) - reader.tell()
                try:
                    ID, VN, DS = reader.read_struct(&#39;4sBB&#39;)
                    if ID != B&#39;LZIP&#39;:
                        if k &gt; 1:
                            raise EOF
                        else:
                            self.log_warn(F&#39;ignoring invalid LZIP signature: {ID.hex()}&#39;)
                    if VN != 1:
                        self.log_warn(F&#39;ignoring invalid LZIP version: {VN}&#39;)
                    dict_size = 1 &lt;&lt; (DS &amp; 0x1F)
                    dict_size -= (dict_size // 16) * ((DS &gt;&gt; 5) &amp; 7)
                    if dict_size not in range(_MIN_DICT_SIZE, _MAX_DICT_SIZE + 1):
                        raise ValueError(
                            F&#39;The dictionary size {dict_size} is out of the valid range &#39;
                            F&#39;[{_MIN_DICT_SIZE}, {_MAX_DICT_SIZE}]; unable to proceed.&#39;
                        )
                    decoder = MemberDecoder(dict_size, reader, output)
                    if not decoder():
                        raise ValueError(F&#39;Data error in stream {k}.&#39;)
                    crc32, data_size, member_size = reader.read_struct(&#39;&lt;LQQ&#39;)
                    if crc32 != decoder.crc32:
                        self.log_warn(F&#39;checksum in stream {k} was {decoder.crc:08X}, should have been {crc32:08X}.&#39;)
                    if member_size - 20 != decoder.member_position:
                        self.log_warn(F&#39;member size in stream {k} was {decoder.member_position}, should have been {member_size}.&#39;)
                    if data_size != decoder.data_position:
                        self.log_warn(F&#39;data size in stream {k} was {decoder.data_position}, should have been {data_size}.&#39;)
                except EOFError:
                    if k &lt;= 1:
                        raise
                    self.log_info(F&#39;silently ignoring {trailing_size} bytes of trailing data&#39;)
                    break

            return output.getvalue()

    @classmethod
    def handles(self, data: bytearray):
        return data[:4] == B&#39;LZIP&#39;</code></pre>
</details>
</dd>
<dt id="refinery.shell.lzjb"><code class="flex name class">
<span>class <span class="ident">lzjb</span></span>
</code></dt>
<dd>
<section class="desc"><p>LZJB compression and decompression. This LZ-type compression is used in the ZFS file system.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/compression/lzjb.py#L14-L79" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class lzjb(Unit):
    &#34;&#34;&#34;
    LZJB compression and decompression. This LZ-type compression is used in the ZFS file system.
    &#34;&#34;&#34;
    def reverse(self, src):
        # https://web.archive.org/web/20100807223517/ ..
        # .. http://cvs.opensolaris.org/source/xref/onnv/onnv-gate/usr/src/uts/common/fs/zfs/lzjb.c
        output = bytearray()
        lempel = [0] * _LEMPEL_SIZE
        copymask = 0x80
        position = 0
        while position &lt; len(src):
            copymask &lt;&lt;= 1
            if copymask &gt;= 0x100:
                copymask = 1
                copymap = len(output)
                output.append(0)
            if position &gt; len(src) - _MATCH_MAX:
                output.append(src[position])
                position += 1
                continue
            hsh = (src[position] &lt;&lt; 16) + (src[position + 1] &lt;&lt; 8) + src[position + 2]
            hsh += hsh &gt;&gt; 9
            hsh += hsh &gt;&gt; 5
            hsh %= len(lempel)
            offset = (position - lempel[hsh]) &amp; _OFFSET_MASK
            lempel[hsh] = position
            cpy = position - offset
            if cpy &gt;= 0 and cpy != position and src[position:position + 3] == src[cpy:cpy + 3]:
                output[copymap] |= copymask
                for mlen in range(_MATCH_MIN, min(len(src) - position, _MATCH_MAX)):
                    if src[position + mlen] != src[cpy + mlen]:
                        break
                output.append(((mlen - _MATCH_MIN) &lt;&lt; (8 - _MATCH_LEN)) | (offset &gt;&gt; 8))
                output.append(offset &amp; 255)
                position += mlen
            else:
                output.append(src[position])
                position += 1
        return output

    def process(self, data):
        dst = bytearray()
        src = StructReader(data)
        while not src.eof:
            copy = src.read_byte()
            for mask in (0x01, 0x02, 0x04, 0x08, 0x10, 0x20, 0x40, 0x80):
                if src.eof:
                    break
                if not copy &amp; mask:
                    dst.append(src.read_byte())
                    continue
                elif not dst:
                    raise ValueError(&#39;copy requested against empty buffer&#39;)
                with src.be:
                    match_len = src.read_integer(6) + _MATCH_MIN
                    match_pos = src.read_integer(10)
                if not match_pos or match_pos &gt; len(dst):
                    raise RuntimeError(F&#39;invalid match offset at position {src.tell()}&#39;)
                match_pos = len(dst) - match_pos
                while match_len &gt; 0:
                    match = dst[match_pos:match_pos + match_len]
                    dst.extend(match)
                    match_pos += len(match)
                    match_len -= len(match)
        return dst</code></pre>
</details>
</dd>
<dt id="refinery.shell.lzma"><code class="flex name class">
<span>class <span class="ident">lzma</span></span>
<span>(</span><span>raw=False, alone=False, xz=False, level=9, delta=None)</span>
</code></dt>
<dd>
<section class="desc"><p>LZMA compression and decompression.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/compression/lz.py#L32-L166" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class lzma(Unit):
    &#34;&#34;&#34;
    LZMA compression and decompression.
    &#34;&#34;&#34;

    _SEARCH_MIN_DICT = 0x1_0000
    _SEARCH_MAX_DICT = 0x1000_0000
    _SEARCH_MAX_BLOW = 1.2
    _SEARCH_SKIP1 = 0x08
    _SEARCH_SKIP2 = 0x10
    _ATTEMPT_PARTIAL = True

    def __init__(
        self,
        raw   : Arg.Switch(&#39;-r&#39;, group=&#39;MODE&#39;, help=&#39;Use raw (no container) format.&#39;) = False,
        alone : Arg.Switch(&#39;-a&#39;, group=&#39;MODE&#39;, help=&#39;Use the lzma container format.&#39;) = False,
        xz    : Arg.Switch(&#39;-x&#39;, group=&#39;MODE&#39;, help=&#39;Use the default xz format.&#39;) = False,
        level : Arg.Number(&#39;-l&#39;, bound=(0, 9), help=&#39;The compression level preset; between 0 and 9.&#39;) = 9,
        delta : Arg.Number(&#39;-d&#39;, help=&#39;Add a delta filter when compressing.&#39;) = None,
    ):
        if (raw, alone, xz).count(True) &gt; 1:
            raise ValueError(&#39;Only one container format can be enabled.&#39;)
        if level not in range(10):
            raise ValueError(&#39;Compression level must be a number between 0 and 9.&#39;)
        super().__init__(filter=filter, raw=raw, alone=alone, xz=xz, delta=delta,
            level=level | PRESET_EXTREME)

    def reverse(self, data):
        filters = []
        if self.args.delta is not None:
            self.log_debug(&#39;adding delta filter&#39;)
            filters.append({&#39;id&#39;: FILTER_DELTA, &#39;dist&#39;: self.args.delta})
        if self.args.alone:
            self.log_debug(&#39;setting alone format&#39;)
            mode = FORMAT_ALONE
            filters.append({&#39;id&#39;: FILTER_LZMA1, &#39;preset&#39;: self.args.level})
        elif self.args.raw:
            self.log_debug(&#39;setting raw format&#39;)
            mode = FORMAT_RAW
            filters.append({&#39;id&#39;: FILTER_LZMA2, &#39;preset&#39;: self.args.level})
        else:
            if not self.args.xz:
                self.log_info(&#39;choosing default .xz container format for compression&#39;)
            mode = FORMAT_XZ
            filters.append({&#39;id&#39;: FILTER_LZMA2, &#39;preset&#39;: self.args.level})
        lz = LZMACompressor(mode, filters=filters)
        output = lz.compress(data)
        output += lz.flush()
        return output

    def _decompress(self, data: bytearray, lz: LZMADecompressor, partial: bool = False):
        temp = bytearray()
        sizes = repeat(1) if partial else [len(data)]
        with MemoryFile(temp) as output:
            with MemoryFile(data) as stream:
                for size in sizes:
                    if stream.eof or stream.closed:
                        break
                    try:
                        offset = stream.tell()
                        output.write(lz.decompress(stream.read(size)))
                    except (EOFError, LZMAError):
                        raise RefineryPartialResult(
                            F&#39;compression failed at offset {offset}&#39;, temp)
        if n := len(lz.unused_data):
            raise RefineryPartialResult(F&#39;Data stream is truncated, {n} bytes unused.&#39;, temp)
        return temp

    def _process(self, data: bytearray, partial=False):
        try:
            dc = LZMADecompressor()
            return self._decompress(data, dc, partial)
        except RefineryPartialResult as pe:
            best = pe
        except Exception:
            best = None
            self.log_info(&#39;default LZMA decompressor failed, brute-forcing custom header&#39;)
        view = memoryview(data)
        min_original_size = {
            # https://sourceforge.net/p/sevenzip/discussion/45797/thread/b6bd62f8/
            1: int((len(data) - 64_000) / 1.100), # noqa
            2: int((len(data) -  1_000) / 1.001), # noqa
        }
        for (version, p), offset_prop, to_data in product(
            ((1, 5),
             (2, 1)),
            range(self._SEARCH_SKIP1 + 1),
            range(self._SEARCH_SKIP2 + 1),
        ):
            if offset_prop + to_data &gt; p + 20:
                # expect no more than a 20 byte header on top of the properties
                # that would be enough for, e.g. compressed &amp; uncompressed size
                # each filling a full 64bit integer and 4 additional bytes.
                continue
            try:
                filter = parse_lzma_properties(
                    view[offset_prop:offset_prop + p],
                    version,
                    min_dict=self._SEARCH_MIN_DICT,
                    max_dict=self._SEARCH_MAX_DICT,
                )
                self.log_debug(F&#39;attempt LZMA{version} at {offset_prop:02d}, skipping {to_data:02d}, filter: {filter!r}&#39;)
                engine = LZMADecompressor(FORMAT_RAW, filters=[filter])
                result = self._decompress(view[offset_prop + p + to_data:], engine, partial)
            except RefineryPartialResult as pe:
                if best is None:
                    best = pe
                elif len(best.partial) &lt; len(pe.partial):
                    best = pe
                continue
            except Exception:
                continue
            if len(result) &lt; min_original_size[version]:
                continue
            if len(result) * self._SEARCH_MAX_BLOW &lt; len(data):
                continue
            self.log_info(
                F&#39;success with LZMA{version} properties at {offset_prop} and raw stream starting at {to_data + offset_prop + p}&#39;)
            return result
        if partial or not self._ATTEMPT_PARTIAL:
            if best and len(best.partial) &gt; 0:
                raise best
            raise ValueError(&#39;unable to find an LZMA stream&#39;)

    def process(self, data: bytearray):
        if out := self._process(data):
            return out
        return self._process(data, partial=True)

    @classmethod
    def handles(self, data: bytearray):
        if data[:4] == B&#39;\x5D\0\0\0&#39;:
            return True
        if data[:5] == B&#39;\xFD7zXZ&#39;:
            return True</code></pre>
</details>
</dd>
<dt id="refinery.shell.lznt1"><code class="flex name class">
<span>class <span class="ident">lznt1</span></span>
<span>(</span><span>chunk_size=4096)</span>
</code></dt>
<dd>
<section class="desc"><p>LZNT1 compression and decompression. This compression algorithm is expected
by the Win32 API routine <code>RtlDecompressBuffer</code>, for example.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/compression/lznt1.py#L10-L148" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class lznt1(Unit):
    &#34;&#34;&#34;
    LZNT1 compression and decompression. This compression algorithm is expected
    by the Win32 API routine `RtlDecompressBuffer`, for example.
    &#34;&#34;&#34;

    def _decompress_chunk(self, chunk):
        out = B&#39;&#39;
        while chunk:
            flags = chunk[0]
            chunk = chunk[1:]
            for i in range(8):
                if not (flags &gt;&gt; i &amp; 1):
                    out += chunk[:1]
                    chunk = chunk[1:]
                else:
                    flag = struct.unpack(&#39;&lt;H&#39;, chunk[:2])[0]
                    pos = len(out) - 1
                    l_mask = 0xFFF
                    o_shift = 12
                    while pos &gt;= 0x10:
                        l_mask &gt;&gt;= 1
                        o_shift -= 1
                        pos &gt;&gt;= 1
                    length = (flag &amp; l_mask) + 3
                    offset = (flag &gt;&gt; o_shift) + 1
                    if length &gt;= offset:
                        tmp = out[-offset:] * (0xFFF // len(out[-offset:]) + 1)
                        out += tmp[:length]
                    else:
                        out += out[-offset:length - offset]
                    chunk = chunk[2:]
                if len(chunk) == 0:
                    break
        return out

    def _find(self, src, target, max_len):
        result_offset = 0
        result_length = 0
        for i in range(1, max_len):
            offset = src.rfind(target[:i])
            if offset == -1:
                break
            tmp_offset = len(src) - offset
            tmp_length = i
            if tmp_offset == tmp_length:
                tmp = src[offset:] * (0xFFF // len(src[offset:]) + 1)
                for j in range(i, max_len + 1):
                    offset = tmp.rfind(target[:j])
                    if offset == -1:
                        break
                    tmp_length = j
            if tmp_length &gt; result_length:
                result_offset = tmp_offset
                result_length = tmp_length
        if result_length &lt; 3:
            return 0, 0
        return result_offset, result_length

    def _compress_chunk(self, chunk):
        blob = copy.copy(chunk)
        out = B&#39;&#39;
        pow2 = 0x10
        l_mask3 = 0x1002
        o_shift = 12
        while len(blob) &gt; 0:
            bits = 0
            tmp = B&#39;&#39;
            for i in range(8):
                bits &gt;&gt;= 1
                while pow2 &lt; (len(chunk) - len(blob)):
                    pow2 &lt;&lt;= 1
                    l_mask3 = (l_mask3 &gt;&gt; 1) + 1
                    o_shift -= 1
                if len(blob) &lt; l_mask3:
                    max_len = len(blob)
                else:
                    max_len = l_mask3
                offset1, length1 = self._find(
                    chunk[:len(chunk) - len(blob)], blob, max_len)
                # try to find more compressed pattern
                offset2, length2 = self._find(
                    chunk[:len(chunk) - len(blob) + 1], blob[1:], max_len)
                if length1 &lt; length2:
                    length1 = 0
                if length1 &gt; 0:
                    symbol = ((offset1 - 1) &lt;&lt; o_shift) | (length1 - 3)
                    tmp += struct.pack(&#39;&lt;H&#39;, symbol)
                    bits |= 0x80  # set the highest bit
                    blob = blob[length1:]
                else:
                    tmp += blob[:1]
                    blob = blob[1:]
                if len(blob) == 0:
                    break
            out += struct.pack(&#39;B&#39;, bits &gt;&gt; (7 - i))
            out += tmp
        return out

    def reverse(self, buf):
        out = B&#39;&#39;
        while buf:
            chunk = buf[:self.args.chunk_size]
            compressed = self._compress_chunk(chunk)
            if len(compressed) &lt; len(chunk):  # chunk is compressed
                flags = 0xB000
                header = struct.pack(&#39;&lt;H&#39;, flags | (len(compressed) - 1))
                out += header + compressed
            else:
                flags = 0x3000
                header = struct.pack(&#39;&lt;H&#39;, flags | (len(chunk) - 1))
                out += header + chunk
            buf = buf[self.args.chunk_size:]
        return out

    def process(self, data):
        out = io.BytesIO()
        offset = 0
        while offset &lt; len(data):
            try:
                header, = struct.unpack(&#39;&lt;H&#39;, data[offset:offset + 2])
            except struct.error as err:
                raise RefineryPartialResult(str(err), partial=out.getvalue())
            offset += 2
            size = (header &amp; 0xFFF) + 1
            if size + 1 &gt;= len(data):
                raise RefineryPartialResult(
                    F&#39;chunk header indicates size {size}, but only {len(data)} bytes remain.&#39;,
                    partial=out.getvalue()
                )
            chunk = data[offset:offset + size]
            offset += size
            if header &amp; 0x8000:
                chunk = self._decompress_chunk(chunk)
            out.write(chunk)
        return out.getvalue()

    def __init__(self, chunk_size: Arg.Number(&#39;-c&#39;, help=&#39;Optionally specify the chunk size for compression, default is 0x1000.&#39;) = 0x1000):
        super().__init__(chunk_size=chunk_size)</code></pre>
</details>
</dd>
<dt id="refinery.shell.lzo"><code class="flex name class">
<span>class <span class="ident">lzo</span></span>
</code></dt>
<dd>
<section class="desc"><p>LZO decompression. The code works against simple test cases, but it is known to fail for certain outputs produced by the lzop
command-line tool when high compression ratio is favoured (i.e. when the -9 switch is used).</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/compression/lzo.py#L148-L282" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class lzo(Unit):
    &#34;&#34;&#34;
    LZO decompression. The code works against simple test cases, but it is known to fail for certain outputs produced by the lzop
    command-line tool when high compression ratio is favoured (i.e. when the -9 switch is used).
    &#34;&#34;&#34;
    def decompress_stream(self, data: ByteString, LZOv1: bool = False) -&gt; bytearray:
        &#34;&#34;&#34;
        An implementation of LZO decompression. We use the article
        &#34;[LZO stream format as understood by Linux&#39;s LZO decompressor](https://www.kernel.org/doc/html/latest/staging/lzo.html)&#34;
        as a reference since no proper specification is available.
        &#34;&#34;&#34;
        def integer() -&gt; int:
            length = 0
            while True:
                byte = src.read_byte()
                if byte:
                    return length + byte
                length += 0xFF
                if length &gt; 0x100000:
                    raise LZOError(&#39;Too many zeros in integer encoding.&#39;)

        def literal(count):
            dst.write(src.read_bytes(count))

        def copy(distance: int, length: int):
            if distance &gt; len(dst):
                raise LZOError(F&#39;Distance {distance} &gt; bufsize {len(dst)}&#39;)
            buffer = dst.getbuffer()
            if distance &gt; length:
                start = len(buffer) - distance
                end = start + length
                dst.write(buffer[start:end])
            else:
                block = buffer[-distance:]
                while len(block) &lt; length:
                    block += block[:length - len(block)]
                if len(block) &gt; length:
                    block[length:] = ()
                dst.write(block)

        src = StructReader(memoryview(data))
        dst = MemoryFile()

        state = 0
        first = src.read_byte()

        if first == 0x10:
            raise LZOError(&#39;Invalid first stream byte 0x10.&#39;)
        elif first &lt;= 0x12:
            src.seekrel(-1)
        elif first &lt;= 0x15:
            state = first - 0x11
            literal(state)
        else:
            state = 4
            literal(first - 0x11)

        while True:
            instruction = src.read_byte()
            if instruction &lt; 0x10:
                if state == 0:
                    length = instruction or integer() + 15
                    state = length + 3
                    if state &lt; 4:
                        raise LZOError(&#39;Literal encoding is too short.&#39;)
                else:
                    state = instruction &amp; 0b0011
                    D = (instruction &amp; 0b1100) &gt;&gt; 2
                    H = src.read_byte()
                    distance = (H &lt;&lt; 2) + D + 1
                    if state &gt;= 4:
                        distance += 0x800
                        length = 3
                    else:
                        length = 2
                    copy(distance, length)
            elif instruction &lt; 0x20:
                L = instruction &amp; 0b0111
                H = instruction &amp; 0b1000
                length = L or integer() + 7
                argument = src.u16()
                state = argument &amp; 3
                distance = (H &lt;&lt; 11) + (argument &gt;&gt; 2)
                if not distance:
                    return dst.getbuffer()
                if LZOv1 and distance &amp; 0x803F == 0x803F and length in range(261, 265):
                    raise LZOError(&#39;Compressed data contains sequence that is banned in LZOv1.&#39;)
                if LZOv1 and distance == 0xBFFF:
                    X = src.read_byte()
                    count = ((X &lt;&lt; 3) | L) + 4
                    self.log_debug(F&#39;Writing run of {X} zero bytes according to LZOv1.&#39;)
                    dst.write(B&#39;\0&#39; * count)
                else:
                    copy(distance + 0x4000, length + 2)
            elif instruction &lt; 0x40:
                L = instruction &amp; 0b11111
                length = L or integer() + 31
                argument = src.u16()
                state = argument &amp; 3
                distance = (argument &gt;&gt; 2) + 1
                copy(distance, length + 2)
            else:
                if instruction &lt; 0x80:
                    length = 3 + ((instruction &gt;&gt; 5) &amp; 1)
                else:
                    length = 5 + ((instruction &gt;&gt; 5) &amp; 3)
                H = src.read_byte()
                D = (instruction &amp; 0b11100) &gt;&gt; 2
                state = instruction &amp; 3
                distance = (H &lt;&lt; 3) + D + 1
                copy(distance, length)
            if state:
                literal(state)

    def process(self, data):
        try:
            lzo = LZO(data)
        except LZOError:
            self.log_info(&#39;Not an LZO archive, processing raw stream.&#39;)
            return self.decompress_stream(data)
        with MemoryFile() as output:
            for k, chunk in enumerate(lzo, 1):
                self.log_debug(F&#39;decompressing chunk {k}&#39;)
                output.write(self.decompress_stream(chunk.data))
            return self.labelled(
                output.getbuffer(),
                path=lzo.name,
                date=date_from_timestamp(lzo.mtime)
            )

    @classmethod
    def handles(self, data: bytearray) -&gt; Optional[bool]:
        sig = LZO.SIGNATURE
        if data[:len(sig)] == sig:
            return True</code></pre>
</details>
</dd>
<dt id="refinery.shell.lzw"><code class="flex name class">
<span>class <span class="ident">lzw</span></span>
</code></dt>
<dd>
<section class="desc"><p>LZW decompression based on ancient Unix sources.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/compression/lzw.py#L22-L142" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class lzw(Unit):
    &#39;&#39;&#39;
    LZW decompression based on ancient Unix sources.
    &#39;&#39;&#39;

    _MAGIC = B&#39;\x1F\x9D&#39;

    def process(self, data: bytearray):
        out = MemoryFile()
        inf = StructReader(memoryview(data))

        if inf.peek(2) != self._MAGIC:
            self.log_info(&#39;No LZW signature found, assuming raw stream.&#39;)
            maxbits = LZW.BITS
            block_mode = True
        else:
            inf.seekrel(2)
            maxbits = inf.read_integer(5)
            if inf.read_integer(2) != 0:
                self.log_info(&#39;reserved bits were set in LZW header&#39;)
            block_mode = bool(inf.read_bit())

        if maxbits &gt; LZW.BITS:
            raise ValueError(F&#39;Compressed with {maxbits} bits; cannot handle file.&#39;)

        maxmaxcode = 1 &lt;&lt; maxbits

        ibuf = inf.read()

        tab_suffix = bytearray(LZW.WSIZE * 2)
        tab_prefix = array(&#39;H&#39;, itertools.repeat(0, 1 &lt;&lt; LZW.BITS))

        n_bits = LZW.INIT_BITS
        maxcode = (1 &lt;&lt; n_bits) - 1
        bitmask = (1 &lt;&lt; n_bits) - 1
        oldcode = ~0
        finchar = +0
        posbits = +0

        free_entry = LZW.FIRST if block_mode else 0x100
        tab_suffix[:0x100] = range(0x100)
        resetbuf = True

        while resetbuf:
            resetbuf = False

            ibuf = ibuf[posbits &gt;&gt; 3:]
            insize = len(ibuf)
            posbits = 0
            inbits = (insize &lt;&lt; 3) - (n_bits - 1)

            while inbits &gt; posbits:
                if free_entry &gt; maxcode:
                    n = n_bits &lt;&lt; 3
                    p = posbits - 1
                    posbits = p + (n - (p + n) % n)
                    n_bits += 1
                    if (n_bits == maxbits):
                        maxcode = maxmaxcode
                    else:
                        maxcode = (1 &lt;&lt; n_bits) - 1
                    bitmask = (1 &lt;&lt; n_bits) - 1
                    resetbuf = True
                    break

                p = ibuf[posbits &gt;&gt; 3:]
                code = int.from_bytes(p[:3], &#39;little&#39;) &gt;&gt; (posbits &amp; 7) &amp; bitmask
                posbits += n_bits

                if oldcode == -1:
                    if code &gt;= 256:
                        raise ValueError(&#39;corrupt input.&#39;)
                    oldcode = code
                    finchar = oldcode
                    out.write_byte(finchar)
                    continue

                if code == LZW.CLEAR and block_mode:
                    tab_prefix[:0x100] = array(&#39;H&#39;, itertools.repeat(0, 0x100))
                    free_entry = LZW.FIRST - 1
                    n = n_bits &lt;&lt; 3
                    p = posbits - 1
                    posbits = p + (n - (p + n) % n)
                    n_bits = LZW.INIT_BITS
                    maxcode = (1 &lt;&lt; n_bits) - 1
                    bitmask = (1 &lt;&lt; n_bits) - 1
                    resetbuf = True
                    break

                incode = code
                stack = bytearray()

                if code &gt;= free_entry:
                    if code &gt; free_entry:
                        raise RefineryPartialResult(&#39;corrupt input.&#39;, out.getbuffer())
                    stack.append(finchar)
                    code = oldcode
                while code &gt;= 256:
                    stack.append(tab_suffix[code])
                    code = tab_prefix[code]

                finchar = tab_suffix[code]
                stack.append(finchar)
                stack.reverse()
                out.write(stack)
                code = free_entry

                if code &lt; maxmaxcode:
                    tab_prefix[code] = oldcode &amp; 0xFFFF
                    tab_suffix[code] = finchar &amp; 0x00FF
                    free_entry = code + 1

                oldcode = incode

        return out.getvalue()

    @classmethod
    def handles(self, data: bytearray) -&gt; Optional[bool]:
        sig = self._MAGIC
        if data[:len(sig)] == sig:
            return True</code></pre>
</details>
</dd>
<dt id="refinery.shell.machometa"><code class="flex name class">
<span>class <span class="ident">machometa</span></span>
<span>(</span><span>all=True, header=False, linked_images=False, signatures=False, version=False, load_commands=False, exports=False, imports=False, tabular=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Extract metadata from Mach-O files.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/formats/macho/machometa.py#L25-L279" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class machometa(Unit):
    &#34;&#34;&#34;
    Extract metadata from Mach-O files.
    &#34;&#34;&#34;
    def __init__(
        self, all: Arg(&#39;-c&#39;, &#39;--custom&#39;,
            help=&#39;Unless enabled, all default categories will be extracted.&#39;) = True,
        header: Arg(&#39;-H&#39;, help=&#39;Parse basic data from the Mach-O header.&#39;) = False,
        linked_images: Arg(&#39;-K&#39;, help=&#39;Parse all library images linked by the Mach-O.&#39;) = False,
        signatures: Arg(&#39;-S&#39;, help=&#39;Parse signature and entitlement information.&#39;) = False,
        version: Arg(&#39;-V&#39;, help=&#39;Parse version information from the Mach-O load commands.&#39;) = False,
        load_commands: Arg(&#39;-D&#39;, help=&#39;Parse load commands from the Mach-O header.&#39;) = False,
        exports: Arg(&#39;-E&#39;, help=&#39;List all exported functions.&#39;) = False,
        imports: Arg(&#39;-I&#39;, help=&#39;List all imported functions.&#39;) = False,
        tabular: Arg(&#39;-t&#39;, help=&#39;Print information in a table rather than as JSON&#39;) = False,
    ):
        super().__init__(
            header=all or header,
            linked_images=all or linked_images,
            version=all or version,
            signatures=all or signatures,
            load_commands=load_commands,
            imports=imports,
            exports=exports,
            tabular=tabular,
        )

    @Unit.Requires(&#39;k2l&gt;=2.0&#39;, &#39;all&#39;)
    def _ktool():
        import ktool
        import ktool.macho
        import ktool.codesign
        return ktool

    def compute_symhash(self, macho_image: Image) -&gt; Dict:
        def _symbols(symbols: Iterable[Symbol]):
            for sym in symbols:
                if sym.types:
                    continue
                yield sym.fullname
        symbols = sorted(set(_symbols(macho_image.symbol_table.ext)))
        symbols: str = &#39;,&#39;.join(symbols)
        return md5(symbols.encode(&#39;utf8&#39;)).hexdigest()

    def parse_macho_header(self, macho_image: Image, data=None) -&gt; Dict:
        info = {}
        macho_header = macho_image.macho_header
        dyld_header = macho_image.macho_header.dyld_header
        if dyld_header is not None:
            info[&#39;Type&#39;] = dyld_header.type_name
            info[&#39;Magic&#39;] = dyld_header.magic
            info[&#39;CPUType&#39;] = macho_image.slice.type.name
            info[&#39;CPUSubType&#39;] = macho_image.slice.subtype.name
            info[&#39;FileType&#39;] = macho_image.macho_header.filetype.name
            info[&#39;LoadCount&#39;] = dyld_header.loadcnt
            info[&#39;LoadSize&#39;] = dyld_header.loadsize
            info[&#39;Flags&#39;] = [flag.name for flag in macho_header.flags]
            info[&#39;Reserved&#39;] = dyld_header.reserved
        return info

    def parse_linked_images(self, macho_image: Image, data=None) -&gt; Dict:
        load_command_images = {}
        linked_images = macho_image.linked_images
        LOAD_COMMAND = self._ktool.macho.LOAD_COMMAND
        for linked_image in linked_images:
            load_command_name = LOAD_COMMAND(linked_image.cmd.cmd).name
            load_command_images.setdefault(load_command_name, []).append(linked_image.install_name)
        return load_command_images

    def parse_signature(self, macho_image: Image, data=None) -&gt; Dict:

        _km = self._ktool.macho
        _kc = self._ktool.codesign

        class CodeDirectoryBlob(_km.Struct):
            FIELDS = {
                &#39;magic&#39;: _km.uint32_t,
                &#39;length&#39;: _km.uint32_t,
                &#39;version&#39;: _km.uint32_t,
                &#39;flags&#39;: _km.uint32_t,
                &#39;hashOffset&#39;: _km.uint32_t,
                &#39;identOffset&#39;: _km.uint32_t,
                &#39;nSpecialSlots&#39;: _km.uint32_t,
                &#39;nCodeSlots&#39;: _km.uint32_t,
                &#39;codeLimit&#39;: _km.uint32_t,
                &#39;hashSize&#39;: _km.uint8_t,
                &#39;hashType&#39;: _km.uint8_t,
                &#39;platform&#39;: _km.uint8_t,
                &#39;pageSize&#39;: _km.uint8_t,
                &#39;spare2&#39;: _km.uint32_t
            }

            def __init__(self, byte_order=&#39;little&#39;):
                super().__init__(byte_order=byte_order)
                self.magic = 0
                self.length = 0
                self.version = 0
                self.flags = 0
                self.hashOffset = 0
                self.identOffset = 0
                self.nSpecialSlots = 0
                self.nCodeSlots = 0
                self.codeLimit = 0
                self.hashSize = 0
                self.hashType = 0
                self.platform = 0
                self.pageSize = 0
                self.spare2 = 0

        info = {}
        if macho_image.codesign_info is not None:
            superblob: SuperBlob = macho_image.codesign_info.superblob

            for blob in macho_image.codesign_info.slots:
                blob: BlobIndex
                # ktool does not include code for extracting Blobs of types
                # CSSLOT_CODEDIRECTORY, CSSLOT_CMS_SIGNATURE
                # so we must do it ourselves here.
                if blob.type == _kc.CSSLOT_CODEDIRECTORY:
                    start = superblob.off + blob.offset
                    codedirectory_blob = macho_image.read_struct(start, CodeDirectoryBlob)

                    # Ad-hoc signing
                    flags = _kc.swap_32(codedirectory_blob.flags)
                    if flags &amp; CS_ADHOC != 0:
                        info[&#39;AdHocSigned&#39;] = True
                    else:
                        info[&#39;AdHocSigned&#39;] = False

                    # Signature identifier
                    identifier_offset = _kc.swap_32(codedirectory_blob.identOffset)
                    identifier_data = macho_image.read_cstr(start + identifier_offset)
                    info[&#39;SignatureIdentifier&#39;] = identifier_data

                if blob.type == 0x10000:  # CSSLOT_CMS_SIGNATURE
                    start = superblob.off + blob.offset
                    blob_data = macho_image.read_struct(start, _kc.Blob)
                    blob_data.magic = _kc.swap_32(blob_data.magic)
                    blob_data.length = _kc.swap_32(blob_data.length)
                    cms_signature = macho_image.read_bytearray(start + _kc.Blob.SIZE, blob_data.length - _kc.Blob.SIZE)

                    if len(cms_signature) != 0:
                        try:
                            parsed_cms_signature = pemeta.parse_signature(bytearray(cms_signature))
                            info[&#39;Signature&#39;] = parsed_cms_signature
                        except ValueError as pkcs7_parse_error:
                            self.log_warn(F&#39;Could not parse the data in CSSLOT_CMS_SIGNATURE as valid PKCS7 data: {pkcs7_parse_error!s}&#39;)

            if macho_image.codesign_info.req_dat is not None:
                # TODO: Parse the requirements blob,
                # which is encoded according to the code signing requirements language:
                # https://developer.apple.com/library/archive/documentation/Security/Conceptual/CodeSigningGuide/RequirementLang/RequirementLang.html
                info[&#39;Requirements&#39;] = macho_image.codesign_info.req_dat.hex()
            if macho_image.codesign_info.entitlements is not None:
                entitlements: str = macho_image.codesign_info.entitlements
                if entitlements:
                    try:
                        entitlements = plistlib.loads(entitlements.encode(&#39;utf8&#39;))
                    except Exception as error:
                        self.log_warn(F&#39;failed to parse entitlements: {error!s}&#39;)
                    else:
                        info[&#39;Entitlements&#39;] = entitlements

        return info

    def parse_version(self, macho_image: Image, data=None) -&gt; Dict:
        info = {}
        load_commands = macho_image.macho_header.load_commands

        SVC = self._ktool.macho.source_version_command
        BVC = self._ktool.macho.build_version_command

        for load_command in load_commands:
            if isinstance(load_command, SVC):
                if &#39;SourceVersion&#39; not in info:
                    info[&#39;SourceVersion&#39;] = load_command.version
                else:
                    self.log_warn(&#39;More than one load command of type source_version_command found; the MachO file is possibly malformed&#39;)
            elif isinstance(load_command, BVC):
                if &#39;BuildVersion&#39; not in info:
                    info[&#39;BuildVersion&#39;] = {}
                    info[&#39;BuildVersion&#39;][&#39;Platform&#39;] = macho_image.platform.name
                    info[&#39;BuildVersion&#39;][&#39;MinOS&#39;] = F&#39;{macho_image.minos.x}.{macho_image.minos.y}.{macho_image.minos.z}&#39;
                    info[&#39;BuildVersion&#39;][&#39;SDK&#39;] = F&#39;{macho_image.sdk_version.x}.{macho_image.sdk_version.y}.{macho_image.sdk_version.z}&#39;
                    info[&#39;BuildVersion&#39;][&#39;Ntools&#39;] = load_command.ntools
                else:
                    self.log_warn(&#39;More than one load command of type build_version_command found; the MachO file is possibly malformed&#39;)
        return info

    def parse_load_commands(self, macho_image: Image, data=None) -&gt; List:
        info = []
        load_commands = macho_image.macho_header.load_commands
        for load_command in load_commands:
            info.append(load_command.serialize())
        return info

    def parse_imports(self, macho_image: Image, data=None) -&gt; List:
        info = []
        for imp in macho_image.imports:
            info.append(imp.name)
        return info

    def parse_exports(self, macho_image: Image, data=None) -&gt; List:
        info = []
        for exp in macho_image.exports:
            info.append(exp.name)
        return info

    def process(self, data: bytearray):
        result = {}
        ktool = self._ktool
        with NoLogging(NoLogging.Mode.ALL):
            macho = ktool.load_macho_file(fp=MemoryFile(memoryview(data)), use_mmaped_io=False)
        if macho.type is ktool.MachOFileType.FAT:
            result[&#39;FileType&#39;] = &#39;FAT&#39;
        elif macho.type is ktool.MachOFileType.THIN:
            result[&#39;FileType&#39;] = &#39;THIN&#39;

        slices = []

        for macho_slice in macho.slices:
            slice_result = {}
            macho_image = ktool.load_image(fp=macho_slice)

            for switch, resolver, name in [
                (self.args.header, self.parse_macho_header, &#39;Header&#39;),
                (self.args.linked_images, self.parse_linked_images, &#39;LinkedImages&#39;),
                (self.args.signatures, self.parse_signature, &#39;Signatures&#39;),
                (self.args.version, self.parse_version, &#39;Version&#39;),
                (self.args.load_commands, self.parse_load_commands, &#39;LoadCommands&#39;),
                (self.args.imports, self.parse_imports, &#39;Imports&#39;),
                (self.args.exports, self.parse_exports, &#39;Exports&#39;),
            ]:
                if not switch:
                    continue
                self.log_debug(F&#39;parsing: {name}&#39;)
                try:
                    info = resolver(macho_image, data)
                except Exception as E:
                    self.log_info(F&#39;failed to obtain {name}: {E!s}&#39;)
                    continue
                if info:
                    slice_result[name] = info

            if macho_image.uuid is not None:
                uuid: bytes = macho_image.uuid
                slice_result[&#39;UUID&#39;] = uuid.hex()
            slice_result[&#39;SymHash&#39;] = self.compute_symhash(macho_image)
            slice_result[&#39;BaseName&#39;] = macho_image.base_name
            slice_result[&#39;InstallName&#39;] = macho_image.install_name
            slices.append(slice_result)

        if slices:
            result[&#39;Slices&#39;] = slices
            yield from ppjson(tabular=self.args.tabular)._pretty_output(result, indent=4, ensure_ascii=False)</code></pre>
</details>
</dd>
<dt id="refinery.shell.map"><code class="flex name class">
<span>class <span class="ident">map</span></span>
<span>(</span><span>index, image, blocksize=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Each block of the input data which occurs as a block of the index argument is replaced by the
corresponding block of the image argument. If a block size is specified, and if the index or
image argument are byte sequences, they are unpacked into chunks of that size, and excess bytes
that are not an integer multiple of the block size are discarded. To prevent any automatic
chunking, the <code><a title="refinery.lib.argformats.DelayedArgument.btoi" href="lib/argformats.html#refinery.lib.argformats.DelayedArgument.btoi">DelayedArgument.btoi()</a></code> handler can be used.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/blockwise/map.py#L9-L62" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class map(BlockTransformation):
    &#34;&#34;&#34;
    Each block of the input data which occurs as a block of the index argument is replaced by the
    corresponding block of the image argument. If a block size is specified, and if the index or
    image argument are byte sequences, they are unpacked into chunks of that size, and excess bytes
    that are not an integer multiple of the block size are discarded. To prevent any automatic
    chunking, the `refinery.lib.argformats.DelayedArgument.btoi` handler can be used.
    &#34;&#34;&#34;
    _map: Optional[Dict[int, int]]

    def __init__(
        self,
        index: Arg.NumSeq(help=&#39;index characters&#39;),
        image: Arg.NumSeq(help=&#39;image characters&#39;),
        blocksize=None
    ):
        super().__init__(blocksize=blocksize, index=index, image=image, _truncate=2)
        self._map = None

    def reverse(self, data):
        return self._process(data, self.args.image, self.args.index)

    def process(self, data):
        return self._process(data, self.args.index, self.args.image)

    def _process(self, data: bytearray, index: Sequence[int], image: Sequence[int]):
        if not self.bytestream:
            if isbuffer(index):
                self.log_info(F&#39;chunking index sequence into blocks of size {self.blocksize}&#39;)
                index = list(self.chunk(index))
                self.log_debug(F&#39;index sequence: {index}&#39;)
            if isbuffer(image):
                self.log_info(F&#39;chunking image sequence into blocks of size {self.blocksize}&#39;)
                image = list(self.chunk(image))
                self.log_debug(F&#39;image sequence: {image}&#39;)
        if len(set(index)) != len(index):
            raise ValueError(&#39;The index sequence contains duplicates.&#39;)
        if len(index) &gt; len(image):
            raise ValueError(&#39;The index sequence is longer than the image sequence.&#39;)
        if self.bytestream:
            mapping = dict(zip(index, image))
            mapping = bytes(mapping.get(c, c) for c in range(0x100))
            if not isinstance(data, bytearray):
                data = bytearray(data)
            data[:] = (mapping[b] for b in data)
            return data
        try:
            self._map = dict(zip(index, image))
            return super().process(data)
        finally:
            self._map = None

    def process_block(self, token):
        return self._map.get(token, token)</code></pre>
</details>
</dd>
<dt id="refinery.shell.maru"><code class="flex name class">
<span>class <span class="ident">maru</span></span>
<span>(</span><span>seed=0, text=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Returns the 64bit maru hash of the input data.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/crypto/hash/maru.py#L7-L15" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class maru(HashUnit):
    &#34;&#34;&#34;
    Returns the 64bit maru hash of the input data.
    &#34;&#34;&#34;
    def __init__(self, seed: Arg.Number(help=&#39;optional seed value&#39;) = 0, text=False):
        super().__init__(seed=seed, text=text)

    def _algorithm(self, data: bytes) -&gt; bytes:
        return maru32digest(data, self.args.seed)</code></pre>
</details>
</dd>
<dt id="refinery.shell.max_"><code class="flex name class">
<span>class <span class="ident">max_</span></span>
<span>(</span><span>key=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Picks the maximum of all elements in the current <code><a title="refinery.lib.frame" href="lib/frame.html">refinery.lib.frame</a></code>.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/meta/max.py#L8-L61" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class max_(Unit):
    &#34;&#34;&#34;
    Picks the maximum of all elements in the current `refinery.lib.frame`.
    &#34;&#34;&#34;

    def __init__(
        self,
        key: Arg(&#39;key&#39;, type=str, help=&#39;A meta variable expression to sort by instead of sorting the content.&#39;) = None,
    ):
        super().__init__(key=key)

    def filter(self, chunks: Iterable[Chunk]):
        def get_value(chunk: Chunk):
            if key is None:
                return chunk
            return metavars(chunk).get(key)

        key = self.args.key
        it = iter(chunks)

        for max_chunk in it:
            if not max_chunk.visible:
                yield max_chunk
            else:
                max_index = 0
                max_value = get_value(max_chunk)
                break
        else:
            return

        for index, chunk in enumerate(chunks, 1):
            if not chunk.visible:
                yield chunk
                continue
            value = get_value(chunk)
            try:
                is_max = value &gt; max_value
            except TypeError:
                if max_value is None:
                    self.log_info(
                        F&#39;Discarding chunk {max_index} in favor of {index} because {key} was not &#39;
                        F&#39;set on the former; new maximum is {value!r}.&#39;)
                    is_max = True
                else:
                    self.log_info(
                        F&#39;Discarding chunk {index} because {key} had value {value!r}; it could not &#39;
                        F&#39;be compared to the current maximum {max_value!r} on chunk {max_index}.&#39;)
                    is_max = False
            if is_max:
                max_value = value
                max_chunk = chunk
                max_index = index

        yield max_chunk</code></pre>
</details>
</dd>
<dt id="refinery.shell.md2"><code class="flex name class">
<span>class <span class="ident">md2</span></span>
<span>(</span><span>text=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Returns the MD2 hash of the input data.</p></section>
</dd>
<dt id="refinery.shell.md4"><code class="flex name class">
<span>class <span class="ident">md4</span></span>
<span>(</span><span>text=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Returns the MD4 hash of the input data.</p></section>
</dd>
<dt id="refinery.shell.md5"><code class="flex name class">
<span>class <span class="ident">md5</span></span>
<span>(</span><span>text=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Returns the MD5 hash of the input data.</p></section>
</dd>
<dt id="refinery.shell.mimewords"><code class="flex name class">
<span>class <span class="ident">mimewords</span></span>
</code></dt>
<dd>
<section class="desc"><p>Implements the decoding of MIME encoded-word syntax from RFC-2047.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/pattern/mimewords.py#L12-L24" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class mimewords(Unit):
    &#34;&#34;&#34;
    Implements the decoding of MIME encoded-word syntax from RFC-2047.
    &#34;&#34;&#34;

    @unicoded
    def process(self, data: str) -&gt; str:
        def replacer(match):
            self.log_info(&#39;encoded mime word:&#39;, match[0])
            decoded, = decode_header(match[0])
            raw, codec = decoded
            return codecs.decode(raw, codec, errors=&#39;surrogateescape&#39;)
        return re.sub(R&#34;=(?:\?[^\?]*){3}\?=&#34;, replacer, data)</code></pre>
</details>
</dd>
<dt id="refinery.shell.min_"><code class="flex name class">
<span>class <span class="ident">min_</span></span>
<span>(</span><span>key=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Picks the minimum of all elements in the current <code><a title="refinery.lib.frame" href="lib/frame.html">refinery.lib.frame</a></code>.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/meta/min.py#L8-L61" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class min_(Unit):
    &#34;&#34;&#34;
    Picks the minimum of all elements in the current `refinery.lib.frame`.
    &#34;&#34;&#34;

    def __init__(
        self,
        key: Arg(&#39;key&#39;, type=str, help=&#39;A meta variable expression to sort by instead of sorting the content.&#39;) = None,
    ):
        super().__init__(key=key)

    def filter(self, chunks: Iterable[Chunk]):
        def get_value(chunk: Chunk):
            if key is None:
                return chunk
            return metavars(chunk).get(key)

        key = self.args.key
        it = iter(chunks)

        for min_chunk in it:
            if not min_chunk.visible:
                yield min_chunk
            else:
                min_index = 0
                min_value = get_value(min_chunk)
                break
        else:
            return

        for index, chunk in enumerate(chunks, 1):
            if not chunk.visible:
                yield chunk
                continue
            value = get_value(chunk)
            try:
                is_min = value &lt; min_value
            except TypeError:
                if min_value is None:
                    self.log_info(
                        F&#39;Discarding chunk {min_index} in favor of {index} because {key} was not &#39;
                        F&#39;set on the former; new minimum is {value!r}.&#39;)
                    is_min = True
                else:
                    self.log_info(
                        F&#39;Discarding chunk {index} because {key} had value {value!r}; it could not &#39;
                        F&#39;be compared to the current minimum {min_value!r} on chunk {min_index}.&#39;)
                    is_min = False
            if is_min:
                min_value = value
                min_chunk = chunk
                min_index = index

        yield min_chunk</code></pre>
</details>
</dd>
<dt id="refinery.shell.mmh128x32"><code class="flex name class">
<span>class <span class="ident">mmh128x32</span></span>
<span>(</span><span>seed=0, text=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Returns the 128bit Murmur Hash of the input data, 64bit variant.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/crypto/hash/murmur.py#L28-L33" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class mmh128x32(MurMurHash):
    &#34;&#34;&#34;
    Returns the 128bit Murmur Hash of the input data, 64bit variant.
    &#34;&#34;&#34;
    def _algorithm(self, data: bytes) -&gt; bytes:
        return mmh128digest32(data, self.args.seed)</code></pre>
</details>
</dd>
<dt id="refinery.shell.mmh128x64"><code class="flex name class">
<span>class <span class="ident">mmh128x64</span></span>
<span>(</span><span>seed=0, text=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Returns the 128bit Murmur Hash of the input data, 64bit variant.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/crypto/hash/murmur.py#L20-L25" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class mmh128x64(MurMurHash):
    &#34;&#34;&#34;
    Returns the 128bit Murmur Hash of the input data, 64bit variant.
    &#34;&#34;&#34;
    def _algorithm(self, data: bytes) -&gt; bytes:
        return mmh128digest64(data, self.args.seed)</code></pre>
</details>
</dd>
<dt id="refinery.shell.mmh32"><code class="flex name class">
<span>class <span class="ident">mmh32</span></span>
<span>(</span><span>seed=0, text=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Returns the 32bit Murmur Hash of the input data.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/crypto/hash/murmur.py#L12-L17" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class mmh32(MurMurHash):
    &#34;&#34;&#34;
    Returns the 32bit Murmur Hash of the input data.
    &#34;&#34;&#34;
    def _algorithm(self, data: bytes) -&gt; bytes:
        return mmh32digest(data, self.args.seed)</code></pre>
</details>
</dd>
<dt id="refinery.shell.morse"><code class="flex name class">
<span>class <span class="ident">morse</span></span>
<span>(</span><span>language=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Morse encoding and decoding. All tokens in the input data which consist of dashes and dots are
replaced by their Morse decoding.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/encoding/morse.py#L35-L333" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class morse(Unit):
    &#34;&#34;&#34;
    Morse encoding and decoding. All tokens in the input data which consist of dashes and dots are
    replaced by their Morse decoding.
    &#34;&#34;&#34;
    def __init__(
        self,
        language: Arg.Option(choices=MorseLanguage, help=(
            &#39;Optionally choose a language. If none is specified, the unit will attempt to detect &#39;
            &#39;the language automatically. Options are: {choices}&#39;)) = None,
    ):
        super().__init__(language=Arg.AsOption(language, MorseLanguage))

    @classmethod
    def handles(self, data: bytearray):
        if re.fullmatch(BR&#39;[-.\s]+&#39;, data, re.DOTALL):
            return True

    @unicoded
    def process(self, data: str):
        language: MorseLanguage = self.args.language
        parsed = re.split(&#39;(\\s+)&#39;, data)
        tokens = {t for t in parsed[::2] if t}
        tables = [
            self._DECODE_SYMBOL,
            self._DECODE_DIGITS,
        ]

        if language is not None:
            tables.append(self._DECODE[language])
        else:
            special = set(self._DECODE_SYMBOL) | set(self._DECODE_DIGITS)
            best_ratio = 1 # number of unused codes
            best_table = None
            for language in MorseLanguage:
                table = self._DECODE[language]
                codes = set(table)
                if not tokens &lt;= codes | special:
                    continue
                if language == MorseLanguage.EN:
                    best_table = table
                    break
                ratio = len(codes - tokens) / len(codes)
                if ratio &lt; best_ratio:
                    best_ratio = ratio
                    best_table = table
            if best_table is None:
                raise LookupError(&#39;Unable to determine language, please specify it manually.&#39;)
            tables.append(best_table)

        with io.StringIO() as out:
            for k, string in enumerate(parsed):
                if k % 2 == 1:
                    string = string[1:]
                    if len(string) &gt; 1:
                        string = string[:-1]
                    out.write(string)
                    continue
                if not string:
                    continue
                for table in tables:
                    try:
                        out.write(table[string])
                        break
                    except KeyError:
                        continue
                else:
                    raise ValueError(F&#39;invalid token: {string}&#39;)
            return out.getvalue()

    @unicoded
    def reverse(self, data: str):
        language: MorseLanguage = self.args.language
        tables = [
            self._ENCODE_SYMBOL,
            self._ENCODE_DIGITS,
        ]
        if language is not None:
            tables.append(self._ENCODE[language])
        else:
            tables.extend(self._ENCODE.values())

        def _encode(letter):
            for table in tables:
                try:
                    return table[letter]
                except KeyError:
                    continue
            else:
                raise ValueError(F&#39;cannot encode letter &#34;{letter}&#34;&#39;)

        with io.StringIO() as out:
            for k, word in enumerate(re.split(&#39;(\\s+)&#39;, data)):
                if k % 2 == 1:
                    out.write(F&#39; {word} &#39;)
                    continue
                out.write(&#39; &#39;.join(_encode(letter) for letter in word.lower()))
            return out.getvalue()

    _ENCODE = {
        MorseLanguage.EN: {
            &#39;a&#39;: &#39;.-&#39;,
            &#39;b&#39;: &#39;-...&#39;,
            &#39;c&#39;: &#39;-.-.&#39;,
            &#39;d&#39;: &#39;-..&#39;,
            &#39;e&#39;: &#39;.&#39;,
            &#39;f&#39;: &#39;..-.&#39;,
            &#39;g&#39;: &#39;--.&#39;,
            &#39;h&#39;: &#39;....&#39;,
            &#39;i&#39;: &#39;..&#39;,
            &#39;j&#39;: &#39;.---&#39;,
            &#39;k&#39;: &#39;-.-&#39;,
            &#39;l&#39;: &#39;.-..&#39;,
            &#39;m&#39;: &#39;--&#39;,
            &#39;n&#39;: &#39;-.&#39;,
            &#39;o&#39;: &#39;---&#39;,
            &#39;p&#39;: &#39;.--.&#39;,
            &#39;q&#39;: &#39;--.-&#39;,
            &#39;r&#39;: &#39;.-.&#39;,
            &#39;s&#39;: &#39;...&#39;,
            &#39;t&#39;: &#39;-&#39;,
            &#39;u&#39;: &#39;..-&#39;,
            &#39;v&#39;: &#39;...-&#39;,
            &#39;w&#39;: &#39;.--&#39;,
            &#39;x&#39;: &#39;-..-&#39;,
            &#39;y&#39;: &#39;-.--&#39;,
            &#39;z&#39;: &#39;--..&#39;,
        }
    }
    _ENCODE[MorseLanguage.ES] = _extend_dictionary(_ENCODE[MorseLanguage.EN], {
        &#39;&#39;: &#39;.--.-&#39;,
        &#39;&#39;: &#39;..-..&#39;,
        &#39;&#39;: &#39;..&#39;,
        &#39;&#39;: &#39;--.--&#39;,
        &#39;&#39;: &#39;---.&#39;,
        &#39;&#39;: &#39;..-&#39;,
        &#39;&#39;: &#39;..--&#39;,
        &#39;&#39;: &#39;..-.-&#39;,
        &#39;&#39;: &#39;--...-&#39;,
    })
    _ENCODE[MorseLanguage.DE] = _extend_dictionary(_ENCODE[MorseLanguage.EN], {
        &#39;&#39;: &#39;.-.-&#39;,
        &#39;&#39;: &#39;---.&#39;,
        &#39;&#39;: &#39;..--&#39;,
        &#39;&#39;: &#39;...--..&#39;,
    })
    _ENCODE[MorseLanguage.FR] = _extend_dictionary(_ENCODE[MorseLanguage.EN], {
        &#39;&#39;: &#39;.--.-&#39;,
        &#39;&#39;: &#39;.--.-&#39;,
        &#39;&#39;: &#39;-.-..&#39;,
        &#39;&#39;: &#39;.-..-&#39;,
        &#39;&#39;: &#39;..-..&#39;,
        &#39;&#39;: &#39;-..-.&#39;,
        &#39;&#39;: &#39;..-..&#39;,
        &#39;&#39;: &#39;..&#39;,
        &#39;&#39;: &#39;-..--&#39;,
        &#39;&#39;: &#39;---&#39;,
        &#39;&#39;: &#39;..-&#39;,
        &#39;&#39;: &#39;..--&#39;,
    })
    _ENCODE[MorseLanguage.RU] = {
        &#39;&#39;: &#39;.-&#39;,
        &#39;&#39;: &#39;-...&#39;,
        &#39;&#39;: &#39;.--&#39;,
        &#39;&#39;: &#39;--.&#39;,
        &#39;&#39;: &#39;-..&#39;,
        &#39;&#39;: &#39;.&#39;,
        &#39;&#39;: &#39;.&#39;,
        &#39;&#39;: &#39;...-&#39;,
        &#39;&#39;: &#39;--..&#39;,
        &#39;&#39;: &#39;..&#39;,
        &#39;&#39;: &#39;.---&#39;,
        &#39;&#39;: &#39;-.-&#39;,
        &#39;&#39;: &#39;.-..&#39;,
        &#39;&#39;: &#39;--&#39;,
        &#39;&#39;: &#39;-.&#39;,
        &#39;&#39;: &#39;---&#39;,
        &#39;&#39;: &#39;.--.&#39;,
        &#39;&#39;: &#39;.-.&#39;,
        &#39;&#39;: &#39;...&#39;,
        &#39;&#39;: &#39;-&#39;,
        &#39;&#39;: &#39;..-&#39;,
        &#39;&#39;: &#39;..-.&#39;,
        &#39;&#39;: &#39;....&#39;,
        &#39;&#39;: &#39;-.-.&#39;,
        &#39;&#39;: &#39;---.&#39;,
        &#39;&#39;: &#39;----&#39;,
        &#39;&#39;: &#39;--.-&#39;,
        &#39;&#39;: &#39;--.--&#39;,
        &#39;&#39;: &#39;-.--&#39;,
        &#39;&#39;: &#39;-..-&#39;,
        &#39;&#39;: &#39;..-..&#39;,
        &#39;&#39;: &#39;..--&#39;,
        &#39;&#39;: &#39;.-.-&#39;,
    }
    _ENCODE[MorseLanguage.UA] = _extend_dictionary(_ENCODE[MorseLanguage.RU], {
        &#39;&#39;: &#39;--.&#39;,
        &#39;&#39;: &#39;-.--&#39;,
        &#39;&#39;: &#39;.---.&#39;,
    })
    _ENCODE[MorseLanguage.UA][&#39;&#39;] = _ENCODE[MorseLanguage.UA].pop(&#39;&#39;)
    _ENCODE[MorseLanguage.UA][&#39;&#39;] = _ENCODE[MorseLanguage.UA].pop(&#39;&#39;)

    _ENCODE[MorseLanguage.HE] = {
        &#39;&#39;: &#39;.-&#39;,
        &#39;&#39;: &#39;-...&#39;,
        &#39;&#39;: &#39;--.&#39;,
        &#39;&#39;: &#39;-..&#39;,
        &#39;&#39;: &#39;---&#39;,
        &#39;&#39;: &#39;.&#39;,
        &#39;&#39;: &#39;--..&#39;,
        &#39;&#39;: &#39;....&#39;,
        &#39;&#39;: &#39;..--&#39;,
        &#39;&#39;: &#39;..&#39;,
        &#39;&#39;: &#39;-.&#39;,
        &#39;&#39;: &#39;.-..&#39;,
        &#39;&#39;: &#39;--&#39;,
        &#39;&#39;: &#39;--.&#39;,
        &#39;&#39;: &#39;-.-.&#39;,
        &#39;&#39;: &#39;.---&#39;,
        &#39;&#39;: &#39;.--.&#39;,
        &#39;&#39;: &#39;.--&#39;,
        &#39;&#39;: &#39;--.-&#39;,
        &#39;&#39;: &#39;.-.&#39;,
        &#39;&#39;: &#39;...&#39;,
        &#39;&#39;: &#39;-&#39;,
    }

    _ENCODE[MorseLanguage.AR] = {
        &#39;&#39;: &#39;.-&#39;,
        &#39;&#39;: &#39;-...&#39;,
        &#39;&#39;: &#39;-&#39;,
        &#39;&#39;: &#39;-.-.&#39;,
        &#39;&#39;: &#39;.---&#39;,
        &#39;&#39;: &#39;....&#39;,
        &#39;&#39;: &#39;---&#39;,
        &#39;&#39;: &#39;-..&#39;,
        &#39;&#39;: &#39;--..&#39;,
        &#39;&#39;: &#39;.-.&#39;,
        &#39;&#39;: &#39;---.&#39;,
        &#39;&#39;: &#39;...&#39;,
        &#39;&#39;: &#39;----&#39;,
        &#39;&#39;: &#39;-..-&#39;,
        &#39;&#39;: &#39;...-&#39;,
        &#39;&#39;: &#39;..-&#39;,
        &#39;&#39;: &#39;-.--&#39;,
        &#39;&#39;: &#39;.-.-&#39;,
        &#39;&#39;: &#39;--.&#39;,
        &#39;&#39;: &#39;..-.&#39;,
        &#39;&#39;: &#39;--.-&#39;,
        &#39;&#39;: &#39;-.-&#39;,
        &#39;&#39;: &#39;.-..&#39;,
        &#39;&#39;: &#39;--&#39;,
        &#39;&#39;: &#39;-.&#39;,
        &#39;&#39;: &#39;..-..&#39;,
        &#39;&#39;: &#39;.--&#39;,
        &#39;&#39;: &#39;..&#39;,
        &#39;&#39;: &#39;.&#39;,
    }

    _ENCODE_DIGITS = {
        &#39;0&#39;: &#39;-----&#39;,
        &#39;1&#39;: &#39;.----&#39;,
        &#39;2&#39;: &#39;..---&#39;,
        &#39;3&#39;: &#39;...--&#39;,
        &#39;4&#39;: &#39;....-&#39;,
        &#39;5&#39;: &#39;.....&#39;,
        &#39;6&#39;: &#39;-....&#39;,
        &#39;7&#39;: &#39;--...&#39;,
        &#39;8&#39;: &#39;---..&#39;,
        &#39;9&#39;: &#39;----.&#39;
    }

    _ENCODE_SYMBOL = {
        &#39;_&#39;: &#39;..--.-&#39;,
        &#39;-&#39;: &#39;-....-&#39;,
        &#39;,&#39;: &#39;--..--&#39;,
        &#39;;&#39;: &#39;-.-.-.&#39;,
        &#39;:&#39;: &#39;---...&#39;,
        &#39;!&#39;: &#39;-.-.--&#39;,
        &#39;?&#39;: &#39;..--..&#39;,
        &#39;.&#39;: &#39;.-.-.-&#39;,
        &#39;&#34;&#39;: &#39;.-..-.&#39;,
        &#39;(&#39;: &#39;-.--.&#39;,
        &#39;)&#39;: &#39;-.--.-&#39;,
        &#39;@&#39;: &#39;.--.-.&#39;,
        &#39;/&#39;: &#39;-..-.&#39;,
        &#39;\\&#39;: &#39;-..-.&#39;,
        &#39;&amp;&#39;: &#39;.-...&#39;,
        &#39;+&#39;: &#39;.-.-.&#39;,
        &#39;=&#39;: &#39;-...-&#39;,
        &#39;$&#39;: &#39;...-..-&#39;,
        &#34;&#39;&#34;: &#39;.----.&#39;,
    }

    _DECODE = {
        lng: _reverse_dictionary(tbl) for lng, tbl in _ENCODE.items()}
    _DECODE_SYMBOL = _reverse_dictionary(_ENCODE_SYMBOL)
    _DECODE_DIGITS = _reverse_dictionary(_ENCODE_DIGITS)</code></pre>
</details>
</dd>
<dt id="refinery.shell.mscdk"><code class="flex name class">
<span>class <span class="ident">mscdk</span></span>
<span>(</span><span>size, hash='MD5')</span>
</code></dt>
<dd>
<section class="desc"><p>An implementation of the CryptDeriveKey routine available from the Win32 API.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/crypto/keyderive/mscdk.py#L11-L38" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class mscdk(KeyDerivation):
    &#34;&#34;&#34;
    An implementation of the CryptDeriveKey routine available from the Win32 API.
    &#34;&#34;&#34;

    def __init__(self, size, hash=&#39;MD5&#39;):
        super().__init__(size=size, salt=None, hash=hash)

    def process(self, data):
        def digest(x):
            return self.hash.new(x).digest()
        size = self.args.size
        if self.args.hash in (HASH.SHA224, HASH.SHA256, HASH.SHA384, HASH.SHA512):
            buffer = digest(data)
            max_size = len(buffer)
        else:
            max_size = 2 * self.hash.digest_size
            value = digest(data)
            del data
            buffer1 = bytearray([0x36] * 64)
            buffer2 = bytearray([0x5C] * 64)
            for k, b in enumerate(value):
                buffer1[k] ^= b
                buffer2[k] ^= b
            buffer = digest(buffer1) + digest(buffer2)
        if size &gt; max_size:
            raise RefineryPartialResult(F&#39;too many bytes requested, can only provide {max_size}&#39;, partial=buffer)
        return buffer[:size]</code></pre>
</details>
</dd>
<dt id="refinery.shell.mscf"><code class="flex name class">
<span>class <span class="ident">mscf</span></span>
<span>(</span><span>mode=None)</span>
</code></dt>
<dd>
<section class="desc"><p>The Microsoft Compression Format unit implements the format and algorithms used by the Microsoft
Compression API. The implementation for LZMS is currently missing, but MSZIP and XPRESS (both
with and without Huffman table) are supported. This pure Python implementation is very slow when
compared to native code, so decompressing very large inputs can take several minutes.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/compression/mscf.py#L32-L251" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class mscf(Unit):
    &#34;&#34;&#34;
    The Microsoft Compression Format unit implements the format and algorithms used by the Microsoft
    Compression API. The implementation for LZMS is currently missing, but MSZIP and XPRESS (both
    with and without Huffman table) are supported. This pure Python implementation is very slow when
    compared to native code, so decompressing very large inputs can take several minutes.
    &#34;&#34;&#34;

    _SIGNATURE = B&#39;\x0A\x51\xE5\xC0&#39;

    def __init__(
        self,
        mode: Unit.Arg.Option(choices=MODE, help=(
            &#39;Manually select decompression mode ({choices}); by default the unit attempts to derive the &#39;
            &#39;mode from the header, but this will fail for raw streams. However, even if a header is &#39;
            &#39;found, a manually specified mode will take precedence.&#39;)) = None,
    ):
        mode = Unit.Arg.AsOption(mode, MODE)
        super().__init__(mode=mode)

    def process(self, data):
        mode: MODE = self.args.mode
        with StructReader(memoryview(data)) as reader, MemoryFile() as writer:
            reader: StructReader[memoryview]
            check = zlib.crc32(reader.peek(6))
            magic = reader.read(4)
            if magic != self._SIGNATURE:
                if mode is None:
                    self.log_warn(
                        F&#39;data starts with {magic.hex().upper()} rather than the expected sequence &#39;
                        F&#39;{self._SIGNATURE.hex().upper()}; this could be a raw stream.&#39;)
                else:
                    reader.seek(0)
                    handler = self._get_handler(mode)
                    handler(reader, writer, None)
                    return writer.getbuffer()

            header_size = reader.u16()
            if header_size != 24:
                self.log_warn(F&#39;the header size {header_size} was not equal to 24&#39;)

            crc32byte = reader.u8()
            check = zlib.crc32(reader.peek(0x11), check) &amp; 0xFF
            if check != crc32byte:
                self.log_warn(F&#39;the CRC32 check byte was {crc32byte}, computed value was {check}&#39;)

            _mode_code = reader.u8()

            try:
                _mode = MODE(_mode_code)
            except ValueError:
                msg = F&#39;header contains unknown compression type code {_mode_code}&#39;
                if mode is None:
                    raise ValueError(msg)
                else:
                    self.log_warn(msg)
            else:
                if mode is not None and mode != _mode:
                    logger = self.log_warn
                else:
                    logger = self.log_info
                    mode = _mode
                logger(F&#39;header specifies algorithm {_mode.name}&#39;)

            self.log_info(F&#39;using algorithm {mode.name}&#39;)
            decompress = self._get_handler(mode)

            final_size = reader.u32()
            _unknown_1 = reader.u32()
            chunk_size = reader.u32()
            _unknown_2 = reader.u32()

            if _unknown_1 != 0:
                self.log_warn(F&#39;unknown value 1 was unexpectedly nonzero: 0x{_unknown_1:08X}&#39;)
            if _unknown_2 != 0:
                self.log_warn(F&#39;unknown value 2 was unexpectedly nonzero: 0x{_unknown_2:08X}&#39;)

            self.log_debug(F&#39;final size: 0x{final_size:08X}&#39;)
            self.log_debug(F&#39;chunk size: 0x{chunk_size:08X}&#39;)

            if chunk_size &gt; COMPRESS_MAX_CHUNK:
                raise ValueError(&#39;the header chunk size is greater than the maximum value&#39;)

            while len(writer) &lt; final_size:
                src_size = reader.u32()
                src_data = reader.read(src_size)
                if len(src_data) != src_size:
                    raise IndexError(F&#39;Attempted to read {src_size} bytes, but got only {len(src_data)}.&#39;)
                if src_size + len(writer) == final_size:
                    self.log_debug(F&#39;final chunk is uncompressed, appending {src_size} raw bytes to output&#39;)
                    writer.write(src_data)
                    break
                self.log_debug(F&#39;reading chunk of size {src_size}&#39;)
                start = writer.tell()
                chunk = StructReader(src_data)
                target = min(chunk_size, final_size - len(writer))
                decompress(chunk, writer, target)
                writer.flush()
                written = writer.tell() - start
                if written != target:
                    raise RuntimeError(F&#39;decompressed output had unexpected size {written} instead of {chunk_size}&#39;)

            if not reader.eof:
                self.log_info(F&#39;compression complete with {reader.remaining_bytes} bytes remaining in input&#39;)
            return writer.getbuffer()

    def _get_handler(self, mode: MODE) -&gt; Callable[[StructReader, MemoryFile, Optional[int]], None]:
        decompress = {
            mode.MSZIP       : self._decompress_mszip,
            mode.XPRESS_HUFF : self._decompress_xpress_huffman,
            mode.XPRESS      : self._decompress_xpress,
        }.get(mode, None)
        if decompress is None:
            raise NotImplementedError(F&#39;algorithm {mode.name} is not yet implemented&#39;)
        return decompress

    def _decompress_mszip(self, reader: StructReader, writer: MemoryFile, target: Optional[int] = None):
        header = bytes(reader.read(2))
        if header != B&#39;CK&#39;:
            raise ValueError(F&#39;chunk did not begin with CK header, got {header!r} instead&#39;)
        decompress = zlib.decompressobj(-zlib.MAX_WBITS, zdict=writer.getbuffer())
        writer.write(decompress.decompress(reader.read()))
        writer.write(decompress.flush())

    def _decompress_xpress_huffman(
        self,
        reader: StructReader,
        writer: MemoryFile,
        target: Optional[int] = None,
        max_chunk_size: int = 0x10000
    ) -&gt; None:
        limit = writer.tell()
        if target is not None:
            target += limit

        while not reader.eof:

            if reader.remaining_bytes &lt; XPRESS_NUM_SYMBOLS // 2:
                raise IndexError(
                    F&#39;There are only {reader.remaining_bytes} bytes reamining in the input buffer,&#39;
                    F&#39; but at least {XPRESS_NUM_SYMBOLS // 2} are required to read a Huffman table.&#39;)

            table = bytearray(reader.read_integer(4) for _ in range(XPRESS_NUM_SYMBOLS))
            table = make_huffman_decode_table(table, XPRESS_TABLEBITS, XPRESS_MAX_CODEWORD_LEN)
            limit = limit + max_chunk_size
            flags = BitBufferedReader(reader, 16)

            while True:
                position = writer.tell()
                if position == target:
                    if reader.remaining_bytes:
                        self.log_info(F&#39;chunk decompressed with {reader.remaining_bytes} bytes remaining in input buffer&#39;)
                    return
                if position &gt;= limit:
                    if position &gt; limit:
                        limit = position
                        self.log_info(F&#39;decompression of one chunk generated more than the limit of {max_chunk_size} bytes&#39;)
                    flags.collect()
                    break
                try:
                    sym = read_huffman_symbol(flags, table, XPRESS_TABLEBITS, XPRESS_MAX_CODEWORD_LEN)
                except EOFError:
                    self.log_debug(&#39;end of file while reading huffman symbol&#39;)
                    break
                if sym &lt; XPRESS_NUM_CHARS:
                    writer.write_byte(sym)
                    continue
                length = sym &amp; 0xF
                offsetlog = (sym &gt;&gt; 4) &amp; 0xF
                flags.collect()
                if reader.eof:
                    break
                offset = (1 &lt;&lt; offsetlog) | flags.read(offsetlog)
                if length == 0xF:
                    nudge = reader.read_byte()
                    if nudge &lt; 0xFF:
                        length += nudge
                    else:
                        length = reader.u16() or reader.u32()
                length += XPRESS_MIN_MATCH_LEN
                writer.replay(offset, length)

    def _decompress_xpress(self, reader: StructReader, writer: MemoryFile, target: Optional[int] = None) -&gt; bytearray:
        if target is not None:
            target += writer.tell()
        flags = BitBufferedReader(reader)
        nibble_cache = None
        while not reader.eof:
            if target is not None and writer.tell() &gt;= target:
                return
            if not flags.next():
                writer.write(reader.read(1))
                continue
            offset, length = divmod(reader.u16(), 8)
            offset += 1
            if length == 7:
                length = nibble_cache
                if length is None:
                    length_pair = reader.u8()
                    nibble_cache = length_pair &gt;&gt; 4
                    length = length_pair &amp; 0xF
                else:
                    nibble_cache = None
                if length == 15:
                    length = reader.u8()
                    if length == 0xFF:
                        length = reader.u16() or reader.u32()
                        length -= 22
                        if length &lt; 0:
                            raise RuntimeError(F&#39;Invalid match length of {length} for long delta sequence&#39;)
                    length += 15
                length += 7
            length += 3
            writer.replay(offset, length)

    @classmethod
    def handles(cls, data: bytearray) -&gt; Optional[bool]:
        sig = cls._SIGNATURE
        if data[:len(sig)] == sig:
            return True</code></pre>
</details>
</dd>
<dt id="refinery.shell.msgpack"><code class="flex name class">
<span>class <span class="ident">msgpack</span></span>
</code></dt>
<dd>
<section class="desc"><p>Converts a message-pack (msgpack) buffer to JSON and vice-versa.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/formats/msgpack.py#L11-L29" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class msgpack(Unit):
    &#34;&#34;&#34;
    Converts a message-pack (msgpack) buffer to JSON and vice-versa.
    &#34;&#34;&#34;
    def reverse(self, data):
        return mp.dumps(json.loads(data))

    def process(self, data):
        unpacker: mp.fallback.Unpacker = mp.Unpacker(MemoryFile(data, read_as_bytes=True))
        for k in itertools.count():
            try:
                last = unpacker.tell()
                item = unpacker.unpack()
            except Exception as E:
                if isinstance(E, mp.OutOfData) and k == 1:
                    break
                raise RefineryPartialResult(str(E), memoryview(data)[last:]) from E
            else:
                yield json.dumps(item).encode(self.codec)</code></pre>
</details>
</dd>
<dt id="refinery.shell.mspdb"><code class="flex name class">
<span>class <span class="ident">mspdb</span></span>
<span>(</span><span>size, salt, iter=100, hash='SHA1')</span>
</code></dt>
<dd>
<section class="desc"><p>An implementation of the PasswordDeriveBytes routine available from the .NET
standard library. According to documentation, it is an extension of PBKDF1.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/crypto/keyderive/mspdb.py#L6-L25" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class mspdb(KeyDerivation):
    &#34;&#34;&#34;
    An implementation of the PasswordDeriveBytes routine available from the .NET
    standard library. According to documentation, it is an extension of PBKDF1.
    &#34;&#34;&#34;
    def __init__(self, size, salt, iter=100, hash=&#39;SHA1&#39;):
        self.superinit(super(), **vars())

    def process(self, data):
        if self.codec != &#39;UTF8&#39;:
            data = data.decode(self.codec).encode(&#39;UTF8&#39;)
        data += self.args.salt
        for _ in range(self.args.iter - 1):
            data = self.hash.new(data).digest()
        counter, seedhash = 1, data
        data = self.hash.new(data).digest()
        while len(data) &lt; self.args.size:
            data += self.hash.new(B&#39;%d%s&#39; % (counter, seedhash)).digest()
            counter += 1
        return data[:self.args.size]</code></pre>
</details>
</dd>
<dt id="refinery.shell.mvg"><code class="flex name class">
<span>class <span class="ident">mvg</span></span>
<span>(</span><span>*names, top=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Short for "Make Variable Global": This unit can move meta variables into the scope of the
parent frame. If used at the end of a frame, the variables will be moved the scope of the
frame that the pipeline will return to. Otherwise and if the &ndash;top switch is being used,
variables will be moved to scope 0, i.e. to the topmost frame in the current tree.</p>
<p>Note that it is not possible to promote a variable to a parent frame if that variable does not
have the same value on all chunks in the current frame - such variables will always be removed
when the frame closes.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/meta/mvg.py#L7-L42" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class mvg(Unit):
    &#34;&#34;&#34;
    Short for &#34;Make Variable Global&#34;: This unit can move meta variables into the scope of the
    parent frame. If used at the end of a frame, the variables will be moved the scope of the
    frame that the pipeline will return to. Otherwise and if the --top switch is being used,
    variables will be moved to scope 0, i.e. to the topmost frame in the current tree.

    Note that it is not possible to promote a variable to a parent frame if that variable does not
    have the same value on all chunks in the current frame - such variables will always be removed
    when the frame closes.
    &#34;&#34;&#34;
    def __init__(
        self,
        *names: Arg(type=str, metavar=&#39;name&#39;, help=(
            &#39;Name of a variable to be removed. If no variables are explicitly specified, all &#39;
            &#39;variables in the current chunk will be rescoped.&#39;
        )),
        top: Arg.Switch(&#39;-t&#39;, help=&#39;Move the variable(s) to the topmost frame layer.&#39;) = False
    ):
        super().__init__(names=names, top=top)

    def process(self, data):
        meta = metavars(data)
        nest = self.args.nesting
        if nest &lt; 0 and not self.args.top:
            spot = meta.scope + nest
        else:
            spot = 1
        for name in self.args.names or meta.variable_names():
            try:
                if meta.get_scope(name) &lt;= spot:
                    continue
                meta.set_scope(name, spot)
            except KeyError:
                self.log_info(F&#39;variable not defined: {name}&#39;)
        return data</code></pre>
</details>
</dd>
<dt id="refinery.shell.n40"><code class="flex name class">
<span>class <span class="ident">n40</span></span>
<span>(</span><span>key)</span>
</code></dt>
<dd>
<section class="desc"><p>Decrypts hex-encoded strings in various latin-american banker families, including N40.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/malware/n40.py#L10-L23" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class n40(Unit):
    &#34;&#34;&#34;
    Decrypts hex-encoded strings in various latin-american banker families, including N40.
    &#34;&#34;&#34;
    def __init__(self, key: Arg(help=&#39;Decryption key.&#39;)):
        ...

    def process(self, data):
        try:
            data = b16decode(data, casefold=True)
        except Error:
            self.log_info(&#39;Input was not hex-encoded; ignoring this step.&#39;)
        mask = data[1:] | xor(self.args.key) | bytearray
        return bytearray(0xFF + b - a if b &lt;= a else b - a for a, b in zip(data, mask))</code></pre>
</details>
</dd>
<dt id="refinery.shell.neg"><code class="flex name class">
<span>class <span class="ident">neg</span></span>
<span>(</span><span>bigendian=False, blocksize=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Each block of the input data is negated bitwise. This is sometimes
also called the bitwise complement or inverse.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/blockwise/neg.py#L6-L12" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class neg(UnaryOperation):
    &#34;&#34;&#34;
    Each block of the input data is negated bitwise. This is sometimes
    also called the bitwise complement or inverse.
    &#34;&#34;&#34;
    def operate(self, a): return ~a
    def inplace(self, a): a ^= self.fmask</code></pre>
</details>
</dd>
<dt id="refinery.shell.netbios"><code class="flex name class">
<span>class <span class="ident">netbios</span></span>
<span>(</span><span>key=b'A')</span>
</code></dt>
<dd>
<section class="desc"><p>Encodes and decodes strings using the same algorithm that is used for NetBIOS
labels. Each byte 0xUL is encoded as two bytes, which are the sum of 0xU and
0xL with an offset character, respectively. The default offset is the capital
letter A.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/encoding/netbios.py#L6-L38" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class netbios(Unit):
    &#34;&#34;&#34;
    Encodes and decodes strings using the same algorithm that is used for NetBIOS
    labels. Each byte 0xUL is encoded as two bytes, which are the sum of 0xU and
    0xL with an offset character, respectively. The default offset is the capital
    letter A.
    &#34;&#34;&#34;

    def __init__(self, key: Arg(help=&#34;Provide a single letter to use as the offset.&#34;) = B&#39;A&#39;):
        if len(key) != 1:
            raise ValueError(&#34;The key must be a binary string of length exactly 1&#34;)
        super().__init__(key=key[0])

    def reverse(self, data):
        result = bytearray(2 * len(data))
        for k, byte in enumerate(data):
            hi, lo = byte &gt;&gt; 4, byte &amp; 15
            result[2 * k + 0] = hi + self.args.key
            result[2 * k + 1] = lo + self.args.key
        return result

    def process(self, data):
        def merge(it):
            while True:
                try:
                    hi = next(it) - self.args.key
                    lo = next(it) - self.args.key
                    if hi not in range(16) or lo not in range(16):
                        raise ValueError(F&#39;Invalid character encoding detected: hi={hi:X}, lo={lo:X}.&#39;)
                    yield (hi &lt;&lt; 4) | lo
                except StopIteration:
                    break
        return bytearray(merge(iter(data)))</code></pre>
</details>
</dd>
<dt id="refinery.shell.ngrams"><code class="flex name class">
<span>class <span class="ident">ngrams</span></span>
<span>(</span><span>size=slice(2, None, None))</span>
</code></dt>
<dd>
<section class="desc"><p>Extract all n-grams from the input. The algorithm is naive, i.e. it simply iterates all n-grams
and deduplicates using a set data structure. The number n is taken from an arbitrary range given
as a Python slice expression.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/strings/ngrams.py#L7-L31" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class ngrams(Unit):
    &#34;&#34;&#34;
    Extract all n-grams from the input. The algorithm is naive, i.e. it simply iterates all n-grams
    and deduplicates using a set data structure. The number n is taken from an arbitrary range given
    as a Python slice expression.
    &#34;&#34;&#34;
    def __init__(
        self, size: Arg.Bounds(
            help=&#39;Specifies the sizes of each n-gram, i.e. the number n. Defaults to {default}.&#39;) = slice(2, None),
    ):
        super().__init__(size=size)

    def process(self, data: bytearray):
        for n in integers_of_slice(self.args.size):
            self.log_info(F&#39;emitting {n}-grams&#39;)
            if n &gt; len(data):
                break
            deduplicator = set()
            view = memoryview(data)
            for index in range(len(data) - n + 1):
                block = bytes(view[index:index + n])
                if block in deduplicator:
                    continue
                deduplicator.add(block)
                yield self.labelled(block, offset=index)</code></pre>
</details>
</dd>
<dt id="refinery.shell.nop"><code class="flex name class">
<span>class <span class="ident">nop</span></span>
</code></dt>
<dd>
<section class="desc"><p>The unit generates the exact output that was received as input. All unknown arguments passed
to nop are completely ignored, which is different from the behavior of other units. As such,
nop can be used to comment out other units in longer refinery pipelines by simply prefixing a
command with nop.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/misc/nop.py#L14-L26" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class nop(Unit):
    &#34;&#34;&#34;
    The unit generates the exact output that was received as input. All unknown arguments passed
    to nop are completely ignored, which is different from the behavior of other units. As such,
    nop can be used to comment out other units in longer refinery pipelines by simply prefixing a
    command with nop.
    &#34;&#34;&#34;
    @classmethod
    def argparser(cls, **keywords):
        argp = NopArgParser(
            keywords, prog=cls.name, description=documentation(cls), add_help=False)
        argp.set_defaults(nesting=0)
        return cls._interface(argp)</code></pre>
</details>
</dd>
<dt id="refinery.shell.nrv2b"><code class="flex name class">
<span>class <span class="ident">nrv2b</span></span>
<span>(</span><span>bits=32)</span>
</code></dt>
<dd>
<section class="desc"><p>Decompress data using the NRV2B algorithm.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/compression/nrv.py#L35-L63" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class nrv2b(NRVUnit):
    &#34;&#34;&#34;
    Decompress data using the NRV2B algorithm.
    &#34;&#34;&#34;
    def _decompress(self, src: StructReader, dst: MemoryFile, bb: BitBufferedReader):
        last_offset = 1
        while not src.eof:
            while next(bb):
                dst.write_byte(src.read_byte())
            offset = 2 + next(bb)
            while not next(bb):
                offset = 2 * offset + next(bb)
            if offset == 2:
                offset = last_offset
            else:
                offset = (offset - 3) * 0x100 + src.read_byte()
                if offset &amp; 0xFFFFFFFF == 0xFFFFFFFF:
                    break
                offset += 1
                last_offset = offset
            length = next(bb)
            length = 2 * length + next(bb)
            if length == 0:
                length = 2 + next(bb)
                while not next(bb):
                    length = 2 * length + next(bb)
                length += 2
            length += int(bool(offset &gt; 0xD00))
            dst.replay(offset, length + 1)</code></pre>
</details>
</dd>
<dt id="refinery.shell.nrv2d"><code class="flex name class">
<span>class <span class="ident">nrv2d</span></span>
<span>(</span><span>bits=32)</span>
</code></dt>
<dd>
<section class="desc"><p>Decompress data using the NRV2D algorithm.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/compression/nrv.py#L66-L96" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class nrv2d(NRVUnit):
    &#34;&#34;&#34;
    Decompress data using the NRV2D algorithm.
    &#34;&#34;&#34;
    def _decompress(self, src: StructReader, dst: MemoryFile, bb: BitBufferedReader):
        last_offset = 1
        while not src.eof:
            while next(bb):
                dst.write_byte(src.read_byte())
            offset = 2 + next(bb)
            while not next(bb):
                offset = 2 * (offset - 1) + next(bb) # noqa
                offset = 2 *  offset      + next(bb) # noqa
            if offset == 2:
                offset = last_offset
                length = next(bb)
            else:
                offset = (offset - 3) * 0x100 + src.read_byte()
                if offset &amp; 0xFFFFFFFF == 0xFFFFFFFF:
                    break
                length = (offset  ^ 1) &amp; 1 # noqa
                offset = (offset &gt;&gt; 1) + 1
                last_offset = offset
            length = 2 * length + next(bb)
            if length == 0:
                length = 2 + next(bb)
                while not next(bb):
                    length = 2 * length + next(bb)
                length += 2
            length += int(bool(offset &gt; 0x500))
            dst.replay(offset, length + 1)</code></pre>
</details>
</dd>
<dt id="refinery.shell.nrv2e"><code class="flex name class">
<span>class <span class="ident">nrv2e</span></span>
<span>(</span><span>bits=32)</span>
</code></dt>
<dd>
<section class="desc"><p>Decompress data using the NRV2E algorithm.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/compression/nrv.py#L99-L132" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class nrv2e(NRVUnit):
    &#34;&#34;&#34;
    Decompress data using the NRV2E algorithm.
    &#34;&#34;&#34;
    def _decompress(self, src: StructReader, dst: MemoryFile, bb: BitBufferedReader):
        last_offset = 1
        while not src.eof:
            while next(bb):
                dst.write_byte(src.read_byte())
            offset = 2 + next(bb)
            while not next(bb):
                offset = 2 * (offset - 1) + next(bb) # noqa
                offset = 2 *  offset      + next(bb) # noqa
            if offset == 2:
                offset = last_offset
                length = next(bb)
            else:
                offset = (offset - 3) * 0x100 + src.read_byte()
                if offset &amp; 0xFFFFFFFF == 0xFFFFFFFF:
                    break
                length = (offset ^  1) &amp; 1 # noqa
                offset = (offset &gt;&gt; 1) + 1
                last_offset = offset
            if length:
                length = 1 + next(bb)
            elif next(bb):
                length = 3 + next(bb)
            else:
                length = 2 + next(bb)
                while not next(bb):
                    length = 2 * length + next(bb)
                length += 3
            length += int(bool(offset &gt; 0x500))
            dst.replay(offset, length + 1)</code></pre>
</details>
</dd>
<dt id="refinery.shell.ntlm"><code class="flex name class">
<span>class <span class="ident">ntlm</span></span>
<span>(</span><span>text=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Returns the Windows NTLM hash of the input.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/crypto/hash/password_hashes.py#L9-L15" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class ntlm(HashUnit):
    &#34;&#34;&#34;
    Returns the Windows NTLM hash of the input.
    &#34;&#34;&#34;
    def _algorithm(self, data: bytes) -&gt; bytes:
        from Cryptodome.Hash import MD4
        return MD4.new(data.decode(self.codec).encode(&#39;utf-16le&#39;))</code></pre>
</details>
</dd>
<dt id="refinery.shell.officecrypt"><code class="flex name class">
<span>class <span class="ident">officecrypt</span></span>
<span>(</span><span>password=b'VelvetSweatshop')</span>
</code></dt>
<dd>
<section class="desc"><p>A simple proxy for the <code>msoffcrypto</code> package to decrypt office documents.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/formats/office/officecrypt.py#L7-L33" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class officecrypt(Unit):
    &#34;&#34;&#34;
    A simple proxy for the `msoffcrypto` package to decrypt office documents.
    &#34;&#34;&#34;

    def __init__(self, password: Arg.Binary(help=(
        &#39;The document password. By default, the Excel default password &#34;{default}&#34; is used.&#39;
    )) = b&#39;VelvetSweatshop&#39;):
        super().__init__(password=password)

    @Unit.Requires(&#39;msoffcrypto-tool&#39;, &#39;formats&#39;, &#39;office&#39;)
    def _msoffcrypto():
        import msoffcrypto
        return msoffcrypto

    def process(self, data):
        password: bytes = self.args.password
        with MemoryFile(data) as stream:
            doc = self._msoffcrypto.OfficeFile(stream)
            if not doc.is_encrypted():
                self.log_warn(&#39;the document is not encrypted; returning input&#39;)
                return data
            if password:
                doc.load_key(password=password.decode(self.codec))
            with MemoryFile(bytearray()) as output:
                doc.decrypt(output)
                return output.getvalue()</code></pre>
</details>
</dd>
<dt id="refinery.shell.opc"><code class="flex name class">
<span>class <span class="ident">opc</span></span>
<span>(</span><span>mode='x32', *, count=None, until=None, nvar='name', avar='addr', ovar='arg')</span>
</code></dt>
<dd>
<section class="desc"><p>Disassembles the input data using capstone and generates opcodes with metadata as output. This
is useful for programmatic disassembly, while the <code><a title="refinery.asm" href="index.html#refinery.asm">asm</a></code> unit outputs a human-readable
representation. Internally, <code><a title="refinery.asm" href="index.html#refinery.asm">asm</a></code> uses this unit and pretty-prints the output.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/formats/exe/opc.py#L14-L97" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class opc(Unit):
    &#34;&#34;&#34;
    Disassembles the input data using capstone and generates opcodes with metadata as output. This
    is useful for programmatic disassembly, while the `refinery.asm` unit outputs a human-readable
    representation. Internally, `refinery.asm` uses this unit and pretty-prints the output.
    &#34;&#34;&#34;
    def __init__(
        self,
        mode: Arg.Choice(
            help=&#39;Machine code architecture, default is {default}. Select from the following list: {choices}.&#39;,
            choices=_ARCHES, metavar=&#39;[x32|x64|..]&#39;) = &#39;x32&#39;, *,
        count: Arg.Number(&#39;-c&#39;, help=&#39;Maximum number of bytes to disassemble, infinite by default.&#39;) = None,
        until: Arg.String(&#39;-u&#39;, help=&#39;Disassemble until the given string appears among the disassembly.&#39;) = None,
        nvar: Arg.String(&#39;-n&#39;, help=(
            &#39;Variable to receive the disassembled mnemonic. Default is &#34;{default}&#34;.&#39;)) = &#39;name&#39;,
        avar: Arg.String(&#39;-a&#39;, help=(
            &#39;Variable to receive the address of the instruction. Default is &#34;{default}&#34;.&#39;)) = &#39;addr&#39;,
        ovar: Arg.String(&#39;-o&#39;, help=(
            &#39;Variable prefix for instruction operands. Default is &#34;{default}&#34;. The complete operand &#39;
            &#39;string will be in {default}s, the first argument in {default}1, the second in {default}2, &#39;
            &#39;and so on.&#39;)) = &#39;arg&#39;,
        **more
    ):
        super().__init__(
            mode=mode,
            count=count,
            until=until,
            nvar=nvar,
            avar=avar,
            ovar=ovar,
            **more)

    @Unit.Requires(&#39;capstone&#39;)
    def _capstone():
        import capstone
        return capstone

    @property
    def _capstone_engine(self) -&gt; Cs:
        cs = self._capstone
        return cs.Cs(*{
            &#39;arm&#39;    : (cs.CS_ARCH_ARM, cs.CS_MODE_ARM),
            &#39;mips32&#39; : (cs.CS_ARCH_MIPS, cs.CS_MODE_MIPS32),
            &#39;mips64&#39; : (cs.CS_ARCH_MIPS, cs.CS_MODE_MIPS64),
            &#39;ppc32&#39;  : (cs.CS_ARCH_PPC, cs.CS_MODE_32),
            &#39;ppc64&#39;  : (cs.CS_ARCH_PPC, cs.CS_MODE_64),
            &#39;x16&#39;    : (cs.CS_ARCH_X86, cs.CS_MODE_16),
            &#39;x32&#39;    : (cs.CS_ARCH_X86, cs.CS_MODE_32),
            &#39;x64&#39;    : (cs.CS_ARCH_X86, cs.CS_MODE_64),
        }.get(self.args.mode.lower()))

    def process(self, data):
        count = self.args.count or 0
        until = self.args.until
        nvar = self.args.nvar
        avar = self.args.avar
        ovar = self.args.ovar
        if isinstance(until, str):
            until = until.lower()
        for insn in self._capstone_engine.disasm(data, 0, count):
            kwargs = {
                avar: insn.address,
                nvar: insn.mnemonic,
            }
            try:
                ops = insn.op_str
                operands = [op.strip() for op in ops.split(&#39;,&#39;)]
            except Exception:
                operands = []
            else:
                kwargs[F&#39;{ovar}s&#39;] = ops
            for k, op in enumerate(operands, 1):
                if not op:
                    break
                try:
                    op = int(op, 0)
                except Exception:
                    pass
                kwargs[F&#39;{ovar}{k}&#39;] = op
            yield self.labelled(insn.bytes, **kwargs)
            if until is None:
                continue
            if until in ops.lower() or until in insn.mnemonic.lower():
                break</code></pre>
</details>
</dd>
<dt id="refinery.shell.p1"><code class="flex name class">
<span>class <span class="ident">p1</span></span>
</code></dt>
<dd>
<section class="desc"><p>A shortcut for <code><a title="refinery.pick" href="index.html#refinery.pick">pick</a></code> with the argument <code>0:1</code>.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/meta/pick.py#L118-L122" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class p1(pick):
    &#34;&#34;&#34;
    A shortcut for `refinery.pick` with the argument `0:1`.
    &#34;&#34;&#34;
    def __init__(self): super().__init__(slice(0, 1))</code></pre>
</details>
</dd>
<dt id="refinery.shell.p2"><code class="flex name class">
<span>class <span class="ident">p2</span></span>
</code></dt>
<dd>
<section class="desc"><p>A shortcut for <code><a title="refinery.pick" href="index.html#refinery.pick">pick</a></code> with the argument <code>0:2</code>.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/meta/pick.py#L125-L129" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class p2(pick):
    &#34;&#34;&#34;
    A shortcut for `refinery.pick` with the argument `0:2`.
    &#34;&#34;&#34;
    def __init__(self): super().__init__(slice(0, 2))</code></pre>
</details>
</dd>
<dt id="refinery.shell.p3"><code class="flex name class">
<span>class <span class="ident">p3</span></span>
</code></dt>
<dd>
<section class="desc"><p>A shortcut for <code><a title="refinery.pick" href="index.html#refinery.pick">pick</a></code> with the argument <code>0:3</code>.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/meta/pick.py#L132-L136" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class p3(pick):
    &#34;&#34;&#34;
    A shortcut for `refinery.pick` with the argument `0:3`.
    &#34;&#34;&#34;
    def __init__(self): super().__init__(slice(0, 3))</code></pre>
</details>
</dd>
<dt id="refinery.shell.pack"><code class="flex name class">
<span>class <span class="ident">pack</span></span>
<span>(</span><span>base=0, prefix=False, strict=False, width=0, single_floats=False, double_floats=False, bigendian=False, blocksize=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Scans the input data for numeric constants and packs them into a binary format. This is useful to convert the textual representation of
an array of numbers into its binary form. For example, <code>123,34,256,12,1,234</code> would be transformed into the byte sequence <code>7B22000C01EA</code>,
where <code>256</code> was wrapped and packed as a null byte because the default block size is one byte. If the above sequence would be packed
with options -EB2, the result would be equal to <code>007B00220100000C000100EA</code> in hexadecimal.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/blockwise/pack.py#L21-L147" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class pack(BlockTransformationBase):
    &#34;&#34;&#34;
    Scans the input data for numeric constants and packs them into a binary format. This is useful to convert the textual representation of
    an array of numbers into its binary form. For example, `123,34,256,12,1,234` would be transformed into the byte sequence `7B22000C01EA`,
    where `256` was wrapped and packed as a null byte because the default block size is one byte. If the above sequence would be packed
    with options -EB2, the result would be equal to `007B00220100000C000100EA` in hexadecimal.
    &#34;&#34;&#34;

    def __init__(self,
        base: Arg(type=number[2:36], help=(
            &#39;Find only numbers in given base. Default of 0 means that &#39;
            &#39;common expressions for hexadecimal, octal and binary are &#39;
            &#39;accepted.&#39;)) = 0,
        prefix : Arg.Switch(&#39;-r&#39;, group=&#39;FLT&#39;, help=&#39;Add numeric prefixes like 0x, 0b, and 0o in reverse mode.&#39;) = False,
        strict : Arg.Switch(&#39;-s&#39;, help=&#39;Only parse integers that fit in one block of the given block size.&#39;) = False,
        width  : Arg.Number(&#39;-w&#39;, help=&#39;Pad numbers with the specified amount of leading zeros.&#39;) = 0,
        single_floats: Arg.Switch(&#39;-f&#39;, group=&#39;FLT&#39;, help=&#39;Pack single-precision floating-point numbers. Implies -B4.&#39;) = False,
        double_floats: Arg.Switch(&#39;-d&#39;, group=&#39;FLT&#39;, help=&#39;Pack double-precision floating-point numbers. Implies -B8.&#39;) = False,
        bigendian=False, blocksize=None
    ):
        if single_floats and double_floats:
            raise ValueError(&#39;The floats and doubles option are mutually exclusive.&#39;)
        elif single_floats:
            fmode = FMode.SINGLE
            blocksize = 4
        elif double_floats:
            fmode = FMode.DOUBLE
            blocksize = 8
        else:
            fmode = FMode.TO_INT
        super().__init__(
            base=base,
            prefix=prefix,
            strict=strict,
            width=width,
            bigendian=bigendian,
            blocksize=blocksize,
            fmode=fmode,
            _truncate=2,
        )

    @property
    def bytestream(self):
        # never alow bytes to be left unchunked
        return False

    def reverse(self, data):
        base = self.args.base or 10
        width = self.args.width
        mode: FMode = self.args.fmode
        prefix = B&#39;&#39;

        self.log_debug(F&#39;using base {base:d}&#39;)

        if self.args.prefix:
            prefix = {
                0x02: b&#39;0b&#39;,
                0x08: b&#39;0o&#39;,
                0x10: b&#39;0x&#39;
            }.get(base, B&#39;&#39;)

        if mode is FMode.TO_INT:
            converter = BaseUnit(
                base,
                little_endian=not self.args.bigendian,
                strip_padding=True,
            )
            for n in self.chunk_into_bytes(data):
                converted = converter.reverse(n)
                if width:
                    converted = converted.rjust(width, B&#39;0&#39;)
                if prefix:
                    converted = prefix + converted
                yield converted
            return

        elif mode is FMode.SINGLE:
            float_format = &#39;f&#39;
            float_size = 4

        elif mode is FMode.DOUBLE:
            float_format = &#39;d&#39;
            float_size = 8

        count, rest = divmod(len(data), float_size)
        if rest:
            self.log_warn(F&#39;data contained {rest} trailing bytes that were ignored&#39;)
            data = memoryview(data)[:-rest]
        float_format *= count
        if self.args.bigendian:
            float_format = F&#39;&gt;{float_format}&#39;
        else:
            float_format = F&#39;&lt;{float_format}&#39;
        for n in struct.unpack(float_format, data):
            yield str(n).encode(self.codec)

    def process(self, data):
        base: int = self.args.base
        strict: bool = self.args.strict
        mode: FMode = self.args.fmode
        ep = &#39;&gt;&#39; if self.args.bigendian else &#39;&lt;&#39;

        def evaluate_literals(literals: Iterable[bytes]):
            for literal in literals:
                if mode is FMode.TO_INT:
                    if base == 0 and literal[0] == 0x30 and literal[1:].isdigit():
                        literal = B&#39;0o%s&#39; % literal
                    N = int(literal, base)
                elif mode is FMode.SINGLE:
                    N, = struct.unpack(F&#39;{ep}I&#39;, struct.pack(F&#39;{ep}f&#39;, float(literal)))
                elif mode is FMode.DOUBLE:
                    N, = struct.unpack(F&#39;{ep}Q&#39;, struct.pack(F&#39;{ep}d&#39;, float(literal)))
                else:
                    raise TypeError(&#39;unexpected floating point mode&#39;)
                M = N &amp; self.fmask
                if strict and M != N:
                    continue
                yield M

        if base == 0:
            pattern = formats.number
        elif base &lt;= 10:
            pattern = re.compile(B&#39;[-+]?[0-%d]{1,64}&#39; % (base - 1))
        else:
            pattern = re.compile(B&#39;[-+]?[0-9a-%c]{1,20}&#39; % (0x57 + base), re.IGNORECASE)

        return self.unchunk(evaluate_literals(m[0] for m in pattern.finditer(data)))</code></pre>
</details>
</dd>
<dt id="refinery.shell.pad"><code class="flex name class">
<span>class <span class="ident">pad</span></span>
<span>(</span><span>width, padding=b'\x00', left=False, absolute=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Allows padding of the input data.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/meta/pad.py#L6-L39" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class pad(Unit):
    &#34;&#34;&#34;
    Allows padding of the input data.
    &#34;&#34;&#34;

    def __init__(
        self,
        width: Arg.Number(help=&#39;Input is padded to the nearest multiple of this size.&#39;),
        padding: Arg(help=(
            &#39;This custom binary sequence is used (repeatedly, if necessary) to pad the &#39;
            &#39;input. The default is a zero byte.&#39;)) = B&#39;\0&#39;,
        left: Arg.Switch(&#39;-l&#39;, help=&#39;Pad on the left instead of the right.&#39;) = False,
        absolute: Arg.Switch(&#39;-a&#39;, help=(
            &#39;The width argument specifies an absolute size, not a block size.&#39;)) = False
    ):
        super().__init__(width=width, padding=padding, left=left, absolute=absolute)

    def process(self, data):
        width = self.args.width
        if self.args.absolute and len(data) &gt;= width:
            return data
        q, r = divmod(len(data), width)
        size = (q + bool(r)) * width
        missing = (size - len(data))
        if missing &lt;= 0:
            return data
        pad = self.args.padding
        if missing &gt; len(pad):
            pad *= missing // len(pad)
        if self.args.left:
            return pad[:missing] + data
        else:
            data += pad[:missing]
            return data</code></pre>
</details>
</dd>
<dt id="refinery.shell.pbkdf1"><code class="flex name class">
<span>class <span class="ident">pbkdf1</span></span>
<span>(</span><span>size, salt=b'\x00\x00\x00\x00\x00\x00\x00\x00', iter=1000, hash='SHA1')</span>
</code></dt>
<dd>
<section class="desc"><p>PBKDF1 Key derivation</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/crypto/keyderive/pbkdf1.py#L6-L17" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class pbkdf1(KeyDerivation):
    &#34;&#34;&#34;PBKDF1 Key derivation&#34;&#34;&#34;

    @Arg(&#39;salt&#39;, help=&#39;Salt for the derivation; default are 8 null bytes.&#39;)
    def __init__(self, size, salt=bytes(8), iter=1000, hash=&#39;SHA1&#39;):
        self.superinit(super(), **vars())

    def process(self, data):
        from Cryptodome.Protocol.KDF import PBKDF1
        return multidecode(data, lambda pwd: (
            PBKDF1(pwd, self.args.salt, dkLen=self.args.size, count=self.args.iter, hashAlgo=self.hash)
        ))</code></pre>
</details>
</dd>
<dt id="refinery.shell.pbkdf2"><code class="flex name class">
<span>class <span class="ident">pbkdf2</span></span>
<span>(</span><span>size, salt, iter=1000, hash='SHA1')</span>
</code></dt>
<dd>
<section class="desc"><p>PBKDF2 Key derivation. This is implemented as Rfc2898DeriveBytes in .NET
binaries.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/crypto/keyderive/pbkdf2.py#L8-L25" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class pbkdf2(KeyDerivation):
    &#34;&#34;&#34;
    PBKDF2 Key derivation. This is implemented as Rfc2898DeriveBytes in .NET
    binaries.
    &#34;&#34;&#34;

    def __init__(self, size, salt, iter=1000, hash=&#39;SHA1&#39;):
        self.superinit(super(), **vars())

    def process(self, data: ByteStr):
        from Cryptodome.Protocol.KDF import PBKDF2
        return multidecode(data, partial(
            PBKDF2,
            salt=self.args.salt,
            dkLen=self.args.size,
            hmac_hash_module=self.hash,
            count=self.args.iter
        ))</code></pre>
</details>
</dd>
<dt id="refinery.shell.pcap"><code class="flex name class">
<span>class <span class="ident">pcap</span></span>
<span>(</span><span>merge=False, client=False, server=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Performs TCP stream reassembly from packet capture (PCAP) files. By default, the unit emits the parts of
each TCP conversation, attaching several pieces of metadata to each such output: Included are the source
and destination socket address as well as the variable <code>stream</code> which identifies the conversation which
it was part of. The chunks are returned in the order that the bytes were exchanged between source and
destination. When the <code>--merge</code> parameter is specified, the unit instead collects all bytes going forward
and backwards, respectively, and emitting these as two chunks, for each TCP conversation that took place.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/formats/pcap.py#L65-L188" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class pcap(Unit):
    &#34;&#34;&#34;
    Performs TCP stream reassembly from packet capture (PCAP) files. By default, the unit emits the parts of
    each TCP conversation, attaching several pieces of metadata to each such output: Included are the source
    and destination socket address as well as the variable `stream` which identifies the conversation which
    it was part of. The chunks are returned in the order that the bytes were exchanged between source and
    destination. When the `--merge` parameter is specified, the unit instead collects all bytes going forward
    and backwards, respectively, and emitting these as two chunks, for each TCP conversation that took place.
    &#34;&#34;&#34;

    def __init__(
        self,
        merge: Arg.Switch(&#39;-m&#39;, help=&#39;Merge both parts of each TCP conversation into one chunk.&#39;) = False,
        client: Arg.Switch(&#39;-c&#39;, group=&#39;D&#39;, help=&#39;Show only the client part of each conversation.&#39;) = False,
        server: Arg.Switch(&#39;-s&#39;, group=&#39;D&#39;, help=&#39;Show only the server part of each conversation.&#39;) = False,
    ):
        super().__init__(merge=merge, client=client, server=server)

    @Unit.Requires(&#39;pypcapkit[scapy]&gt;=1.3&#39;, &#39;all&#39;)
    def _pcapkit():
        with NoLogging():
            import scapy.layers.tls.session # noqa
            import pcapkit
            return pcapkit

    @Unit.Requires(&#39;scapy&#39;, &#39;all&#39;)
    def _scapy():
        import scapy
        import scapy.packet
        return scapy

    def process(self, data):
        pcapkit = self._pcapkit
        merge = self.args.merge

        with NoLogging(), VirtualFileSystem() as fs:
            vf = VirtualFile(fs, data, &#39;pcap&#39;)
            pcap = pcapkit.extract(
                fin=vf.path,
                engine=pcapkit.Scapy,
                store=True,
                nofile=True,
                extension=False,
                ip=True,
                tcp=True,
                reassembly=True,
                reasm_strict=True,
            )
            tcp: List[Datagram] = list(pcap.reassembly.tcp)
            tcp.sort(key=lambda p: min(p.index, default=0))

        count, convo = 0, None
        src_buffer = MemoryFile()
        dst_buffer = MemoryFile()

        self.log_debug(F&#39;extracted {len(pcap.frame)} packets, assembled {len(tcp)} datagrams&#39;)
        PT = self._scapy.packet

        def payload(packet: Packet):
            ok = (bytes, bytearray, PT.Raw)
            no = (PT.NoPayload, PT.Padding)
            circle = set()
            while True:
                try:
                    inner = packet.payload
                except AttributeError:
                    break
                if isinstance(packet, ok) and not isinstance(packet, no):
                    return packet.original
                if id(inner) in circle:
                    break
                packet = inner
                circle.add(id(inner))
            return B&#39;&#39;

        def sequence(i: int):
            packet = pcap.frame[i - 1]
            while len(packet):
                try:
                    return packet.seq
                except AttributeError:
                    pass
                try:
                    packet = packet.payload
                except AttributeError:
                    break
            return 0

        client = self.args.client
        server = self.args.server

        def commit():
            if src_buffer.tell():
                if not server:
                    yield self.labelled(src_buffer.getvalue(), **convo.src_to_dst())
                src_buffer.truncate(0)
            if dst_buffer.tell():
                if not client:
                    yield self.labelled(dst_buffer.getvalue(), **convo.dst_to_src())
                dst_buffer.truncate(0)

        for datagram in tcp:
            this_convo = Conversation.FromID(datagram.id)
            if this_convo != convo:
                if count and merge:
                    yield from commit()
                count = count + 1
                convo = this_convo

            data = bytearray()
            for index in sorted(datagram.index, key=sequence):
                data.extend(payload(pcap.frame[index - 1]))
            if not data:
                continue
            if not merge:
                yield self.labelled(data, **this_convo.src_to_dst(), stream=count)
            elif this_convo.src == convo.src:
                src_buffer.write(data)
            elif this_convo.dst == convo.src:
                dst_buffer.write(data)
            else:
                raise RuntimeError(F&#39;direction of packet {convo!s} in conversation {count} is unknown&#39;)

        yield from commit()</code></pre>
</details>
</dd>
<dt id="refinery.shell.pcap_http"><code class="flex name class">
<span>class <span class="ident">pcap_http</span></span>
</code></dt>
<dd>
<section class="desc"><p>Extracts HTTP payloads from packet capture (PCAP) files.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/formats/pcap_http.py#L46-L86" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class pcap_http(Unit):
    &#34;&#34;&#34;
    Extracts HTTP payloads from packet capture (PCAP) files.
    &#34;&#34;&#34;
    def process(self, data):
        http_parser = httpresponse()
        requests: List[_HTTP_Request] = []
        responses: List[bytearray] = []

        def lookup(src, dst):
            for k, request in enumerate(requests):
                if request.src == dst and request.dst == src:
                    requests.pop(k)
                    return self.labelled(data, url=request.url)
            return None

        for stream in data | pcap():
            try:
                data = http_parser.process(stream)
            except Exception:
                try:
                    rq = _parse_http_request(stream)
                    requests.append(rq)
                except _HTTPParseError as E:
                    self.log_info(F&#39;error parsing http request: {E!s}&#39;)
                except Exception:
                    pass
                continue
            if not data:
                continue
            src, dst = stream[&#39;src&#39;], stream[&#39;dst&#39;]
            item = lookup(src, dst)
            if item is None:
                responses.append((src, dst, data))
                continue
            yield item

        while responses:
            src, dst, data = responses.pop()
            item = lookup(src, dst)
            yield data if item is None else item</code></pre>
</details>
</dd>
<dt id="refinery.shell.pedebloat"><code class="flex name class">
<span>class <span class="ident">pedebloat</span></span>
<span>(</span><span>*names, certificate=False, directories=False, memdump=False, resources=False, sections=False, trim_code=False, trim_rsrc=False, threshold=0.05, size_limit=10.0 MB, keep_limit=False, aggressive=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Removes junk or excess data from PE files and returns the stripped executable. By default, only
the PE overlay is considered; use the flags <code>-r</code> and <code>-s</code> to also consider resources and entire
sections. Any buffer is only considered for removal if it exceeds a certain size. If this
condition is met, a binary search is performed to determine the offset inside the buffer up to
which the compression ratio is above a certain threshold; everything beyond that point is then
removed. By setting the threshold compression ratio to 1, each large buffer is removed entirely.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/formats/pe/pedebloat.py#L28-L352" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class pedebloat(OverlayUnit):
    &#34;&#34;&#34;
    Removes junk or excess data from PE files and returns the stripped executable. By default, only
    the PE overlay is considered; use the flags `-r` and `-s` to also consider resources and entire
    sections. Any buffer is only considered for removal if it exceeds a certain size. If this
    condition is met, a binary search is performed to determine the offset inside the buffer up to
    which the compression ratio is above a certain threshold; everything beyond that point is then
    removed. By setting the threshold compression ratio to 1, each large buffer is removed entirely.
    &#34;&#34;&#34;
    def __init__(
        self,
        *names: Arg(type=str),
        certificate=False,
        directories=False,
        memdump=False,
        resources: Arg.Switch(&#39;-r&#39;, help=&#39;Strip large resources.&#39;) = False,
        sections : Arg.Switch(&#39;-s&#39;, help=&#39;Strip large sections.&#39;) = False,
        trim_code: Arg.Switch(&#39;-X&#39;, help=&#39;Lift the exception on code sections for stripping.&#39;) = False,
        trim_rsrc: Arg.Switch(&#39;-Y&#39;, help=&#39;Lift the exception on rsrc sections for stripping.&#39;) = False,
        threshold: Arg(&#39;-t&#39;, metavar=&#39;T&#39;, type=percent, help=(
            &#39;Trailing data from resources and sections is stripped until the compression ratio &#39;
            &#39;of the remaining data rises above this threshold. The default value is {default}. &#39;
            &#39;Set this to 1 to ignore the limit entirely and trim every structure as much as &#39;
            &#39;possible without violating alignment. Setting this value to 0 will only strip repeated &#39;
            &#39;occurrences of the last byte.&#39;)) = 0.05,
        size_limit: Arg.Number(&#39;-l&#39;, help=(
            &#39;Structures below this size are not stripped. Default is {default!r}.&#39;)) = _STRIP,
        keep_limit: Arg.Switch(&#39;-k&#39;, help=(
            &#39;Do not strip structures to below the above size limit.&#39;)) = False,
        aggressive: Arg.Switch(&#39;-a&#39;, help=(
            &#39;Equivalent to -srt1: Strip large sections and resources aggressively.&#39;)) = False,
    ):
        if aggressive:
            sections = True
            resources = True
            threshold = 1

        super().__init__(
            certificate,
            directories,
            memdump,
            sections=sections,
            resources=resources,
            size_limit=size_limit,
            keep_limit=keep_limit,
            threshold=threshold,
            trim_rsrc=trim_rsrc,
            trim_code=trim_code,
            names=names,
        )

    def _right_strip_data(self, data: memoryview, alignment=1, block_size=_MB) -&gt; int:
        if not data:
            return 0
        threshold = self.args.threshold
        data_overhang = len(data) % alignment
        result = data_overhang

        if 0 &lt; threshold &lt; 1:
            def compression_ratio(offset: int):
                ratio = len(zlib.compress(data[:offset], level=1)) / offset
                self.log_debug(F&#39;compressing {SizeInt(offset)!r} ratio={ratio:6.4f}&#39;)
                return ratio
            upper = len(data)
            lower = result
            if compression_ratio(upper) &lt;= threshold:
                while block_size &lt; upper - lower:
                    pivot = (lower + upper) // 2
                    ratio = compression_ratio(pivot)
                    if ratio &gt; threshold:
                        lower = pivot + 1
                        continue
                    upper = pivot
                    if abs(ratio - threshold) &lt; 1e-10:
                        break
            result = upper
        elif threshold == 0:
            result = len(data)
        elif threshold == 1:
            result = 0

        while result &gt; 1 and data[result - 2] == data[result - 1]:
            result -= 1

        result = max(result, data_overhang)

        if self.args.keep_limit:
            result = max(result, self.args.size_limit)

        result = result + (data_overhang - result) % alignment

        if result &gt; len(data):
            excess = result - len(data)
            excess = excess + (-excess % alignment)
            result = result - excess

        return result

    def _adjust_offsets(self, pe: PE, gap_offset: int, gap_size: int):
        base = pe.OPTIONAL_HEADER.ImageBase
        alignment = pe.OPTIONAL_HEADER.FileAlignment
        rva_offset = pe.get_rva_from_offset(gap_offset)
        tva_offset = rva_offset + base

        section = pe.get_section_by_offset(gap_offset)
        new_section_size = section.SizeOfRawData - gap_size
        if new_section_size % alignment != 0:
            raise RuntimeError(
                F&#39;trimming 0x{gap_size:X} bytes from section {_ASCII(section.Name)} of size 0x{section.SizeOfRawData:X} &#39;
                F&#39;violates required section alignment of 0x{alignment:X} bytes&#39;)
        inside_section_offset = gap_offset - section.PointerToRawData
        if inside_section_offset &gt; new_section_size:
            overlap = inside_section_offset - new_section_size
            raise RuntimeError(F&#39;trimming from section {_ASCII(section.Name)}; data extends {overlap} beyond section&#39;)

        rva_lbound = section.VirtualAddress
        rva_ubound = section.VirtualAddress + section.Misc_VirtualSize - 1
        tva_lbound = rva_lbound + base
        tva_ubound = rva_ubound + base

        def adjust_attributes_of_structure(
            structure: Structure,
            gap_offset: int,
            valid_values_lower_bound: Optional[int],
            valid_values_upper_bound: Optional[int],
            attributes: Iterable[str]
        ):
            for attribute in attributes:
                old_value = getattr(structure, attribute, 0)
                if old_value &lt;= gap_offset:
                    continue
                if valid_values_lower_bound is not None and old_value &lt; valid_values_lower_bound:
                    continue
                if valid_values_upper_bound is not None and old_value &gt; valid_values_upper_bound:
                    continue
                new_value = old_value - gap_size
                if new_value &lt; gap_offset:
                    raise BrokenLink(F&#39;attribute {attribute} points into removed region&#39;)
                self.log_debug(F&#39;adjusting field in {structure.name}: {attribute}&#39;)
                setattr(structure, attribute, new_value)

        it: Iterable[Structure] = iter(pe.__structures__)
        remove = []

        for index, structure in enumerate(it):
            old_offset = structure.get_file_offset()
            new_offset = old_offset - gap_offset

            if old_offset &gt; gap_offset:
                if old_offset &lt; gap_offset + gap_size:
                    self.log_debug(F&#39;removing structure {structure.name}; starts inside removed region&#39;)
                    remove.append(index)
                    continue
                if isinstance(structure, SectionStructure) and new_offset % alignment != 0:
                    raise RuntimeError(
                        F&#39;structure {structure.name} would be moved to offset 0x{new_offset:X}, &#39;
                        F&#39;violating section alignment value 0x{alignment:X}.&#39;)
                structure.set_file_offset(new_offset)

            try:
                adjust_attributes_of_structure(structure, rva_offset, rva_lbound, rva_ubound, (
                    &#39;OffsetToData&#39;,
                    &#39;AddressOfData&#39;,
                    &#39;VirtualAddress&#39;,
                    &#39;AddressOfNames&#39;,
                    &#39;AddressOfNameOrdinals&#39;,
                    &#39;AddressOfFunctions&#39;,
                    &#39;AddressOfEntryPoint&#39;,
                    &#39;AddressOfRawData&#39;,
                    &#39;BaseOfCode&#39;,
                    &#39;BaseOfData&#39;,
                ))
                adjust_attributes_of_structure(structure, tva_offset, tva_lbound, tva_ubound, (
                    &#39;StartAddressOfRawData&#39;,
                    &#39;EndAddressOfRawData&#39;,
                    &#39;AddressOfIndex&#39;,
                    &#39;AddressOfCallBacks&#39;,
                ))
                adjust_attributes_of_structure(structure, gap_offset, None, None, (
                    &#39;OffsetModuleName&#39;,
                    &#39;PointerToRawData&#39;,
                ))
            except BrokenLink as error:
                self.log_debug(F&#39;removing structure {structure.name}; {error!s}&#39;)
                remove.append(index)
                continue

            for attribute in (
                &#39;CvHeaderOffset&#39;,
                &#39;OffsetIn2Qwords&#39;,
                &#39;OffsetInQwords&#39;,
                &#39;Offset&#39;,
                &#39;OffsetLow&#39;,
                &#39;OffsetHigh&#39;
            ):
                if not hasattr(structure, attribute):
                    continue
                self.log_warn(F&#39;potential offset in structure {structure.name} ignored: {attribute}&#39;)

        while remove:
            index = remove.pop()
            pe.__structures__[index:index + 1] = []

        section.SizeOfRawData = new_section_size

    def _trim_sections(self, pe: PE, data: bytearray) -&gt; int:
        S = self.args.size_limit
        P = self.args.names
        trimmed = 0
        for section in pe.sections:
            section: SectionStructure
            offset = section.PointerToRawData
            name = _ASCII(section.Name)
            if not self.args.trim_code and name.lower() in (&#39;.text&#39;, &#39;.code&#39;):
                self.log_debug(F&#39;skipping code section {name}; specify --trim-code to override.&#39;)
                continue
            if not self.args.trim_rsrc and name.lower() == &#39;.rsrc&#39;:
                self.log_debug(F&#39;skipping rsrc section {name}; specify --trim-rsrc to override.&#39;)
                continue
            old_size = section.SizeOfRawData
            if old_size &lt;= S and not any(fnmatch(name, p) for p in P):
                self.log_debug(F&#39;criteria not satisfied for section: {SizeInt(old_size)!r} {name}&#39;)
                continue
            new_size = self._right_strip_data(
                memoryview(data)[offset:offset + old_size],
                pe.OPTIONAL_HEADER.FileAlignment)
            if new_size == old_size:
                continue
            self.log_info(F&#39;stripping section {name} from {TI(old_size)!r} to {TI(new_size)!r}&#39;)
            gap_size = old_size - new_size
            gap_offset = offset + new_size
            if gap_size &lt;= 0:
                continue
            self._adjust_offsets(pe, gap_offset, gap_size)
            trimmed += gap_size
            data[gap_offset:gap_offset + gap_size] = []
        return trimmed

    def _trim_pe_resources(self, pe: PE, data: bytearray) -&gt; int:
        S = self.args.size_limit
        P = self.args.names
        trimmed = 0

        def find_bloated_resources(pe: PE, directory, level: int = 0, *path) -&gt; Generator[Structure, None, None]:
            for entry in directory.entries:
                name = getattr(entry, &#39;name&#39;)
                numeric = getattr(entry, &#39;id&#39;)
                if not name:
                    if level == 0 and numeric in iter(RSRC):
                        name = RSRC(entry.id)
                    elif numeric is not None:
                        name = str(numeric)
                name = name and str(name) or &#39;?&#39;
                if entry.struct.DataIsDirectory:
                    yield from find_bloated_resources(pe, entry.directory, level + 1, *path, name)
                    continue
                struct: Structure = entry.data.struct
                name = &#39;/&#39;.join((*path, name))
                if struct.Size &lt;= S and not any(fnmatch(name, p) for p in P):
                    self.log_debug(F&#39;criteria not satisfied for resource: {SizeInt(struct.Size)!r} {name}&#39;)
                    continue
                yield name, struct

        RSRC_INDEX = DIRECTORY_ENTRY[&#39;IMAGE_DIRECTORY_ENTRY_RESOURCE&#39;]
        pe.parse_data_directories(directories=[RSRC_INDEX])

        try:
            resources = pe.DIRECTORY_ENTRY_RESOURCE
        except AttributeError:
            return 0
        for name, resource in find_bloated_resources(pe, resources):
            offset = pe.get_offset_from_rva(resource.OffsetToData)
            old_size = resource.Size
            new_size = self._right_strip_data(
                memoryview(data)[offset:offset + old_size],
                pe.OPTIONAL_HEADER.FileAlignment)
            self.log_info(F&#39;stripping resource {name} from {old_size} to {new_size}&#39;)
            gap_size = old_size - new_size
            gap_offset = offset + new_size
            if gap_size &lt;= 0:
                continue
            resource.Size = new_size
            self._adjust_offsets(pe, gap_offset, gap_size)
            trimmed += gap_size
            data[gap_offset:gap_offset + gap_size] = []

        pe.OPTIONAL_HEADER.DATA_DIRECTORY[RSRC_INDEX].Size -= trimmed
        self.log_info(F&#39;trimming size of resource data directory by {TI(trimmed)!r}&#39;)
        return trimmed

    def process(self, data: bytearray) -&gt; bytearray:
        overlay_offset = self._get_size(data)
        if len(data) - overlay_offset &gt;= self.args.size_limit:
            view = memoryview(data)
            overlay_length = self._right_strip_data(view[overlay_offset:])
            body_size = overlay_offset + overlay_length
            try:
                data[body_size:] = []
            except Exception:
                data = data[:body_size]
        if not self.args.resources and not self.args.sections:
            return data
        pe = PE(data=data, fast_load=True)
        total = len(data)
        trimmed = 0
        view = pe.__data__
        copy = False
        if not isinstance(view, bytearray):
            view = memoryview(view)
            try:
                view[0] = 0x4D
            except Exception:
                copy = True
                view = bytearray(pe.__data__)
        if self.args.resources:
            trimmed += self._trim_pe_resources(pe, view)
        if self.args.sections:
            trimmed += self._trim_sections(pe, view)
        if copy:
            pe.__data__ = view
        data = pe.write()
        end = total - trimmed
        if end &lt; len(data):
            self.log_warn(F&#39;output contains {len(data) - end} trailing bytes&#39;)
        return data</code></pre>
</details>
</dd>
<dt id="refinery.shell.peek"><code class="flex name class">
<span>class <span class="ident">peek</span></span>
<span>(</span><span>lines=10, all=False, brief=False, decode=0, escape=False, bare=False, meta=0, gray=False, index=False, stdout=False, narrow=False, blocks=1, dense=False, expand=False, width=0)</span>
</code></dt>
<dd>
<section class="desc"><p>The unit extracts preview information of the input data and displays it on the standard error stream. If the standard
output of this unit is connected by a pipe, the incoming data is forwarded. However, if the unit outputs to a terminal,
the data is discarded instead.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/sinks/peek.py#L19-L322" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class peek(HexViewer):
    &#34;&#34;&#34;
    The unit extracts preview information of the input data and displays it on the standard error stream. If the standard
    output of this unit is connected by a pipe, the incoming data is forwarded. However, if the unit outputs to a terminal,
    the data is discarded instead.
    &#34;&#34;&#34;

    def __init__(
        self,
        lines  : Arg.Number(&#39;-l&#39;, group=&#39;SIZE&#39;, help=&#39;Specify number N of lines in the preview, default is 10.&#39;) = 10,
        all    : Arg.Switch(&#39;-a&#39;, group=&#39;SIZE&#39;, help=&#39;Output all possible preview lines without restriction&#39;) = False,
        brief  : Arg.Switch(&#39;-b&#39;, group=&#39;SIZE&#39;, help=&#39;One line peek, implies --lines=1.&#39;) = False,
        decode : Arg.Counts(&#39;-d&#39;, group=&#39;MODE&#39;, help=(
            &#39;Attempt to decode and display printable data. Specify twice to enable line wrapping.&#39;)) = 0,
        escape : Arg.Switch(&#39;-e&#39;, group=&#39;MODE&#39;, help=&#39;Always peek data as string, escape characters if necessary.&#39;) = False,
        bare   : Arg.Switch(&#39;-r&#39;, group=&#39;META&#39;, help=&#39;Only peek the data itself, do not show a metadata preview.&#39;) = False,
        meta   : Arg.Counts(&#39;-m&#39;, group=&#39;META&#39;, help=(
            &#39;Show more auto-derivable metadata. Specify multiple times to populate more variables.&#39;)) = 0,
        gray   : Arg.Switch(&#39;-g&#39;, help=&#39;Do not colorize the output.&#39;) = False,
        index  : Arg.Switch(&#39;-i&#39;, help=&#39;Display the index of each chunk within the current frame.&#39;) = False,
        stdout : Arg.Switch(&#39;-2&#39;, help=&#39;Print the peek to STDOUT rather than STDERR; the input data is lost.&#39;) = False,
        narrow=False, blocks=1, dense=False, expand=False, width=0
    ):
        if decode and escape:
            raise ValueError(&#39;The decode and esc options are exclusive.&#39;)
        if brief:
            narrow = True
        if environment.colorless.value:
            gray = True
        lines = 1 if brief else INF if all else lines
        super(peek, self).__init__(
            brief=brief,
            gray=gray,
            blocks=blocks,
            decode=decode,
            dense=dense,
            index=index,
            escape=escape,
            expand=expand,
            narrow=narrow,
            lines=lines,
            meta=meta,
            bare=bare,
            width=width,
            stdout=stdout,
        )

    @HexViewer.Requires(&#39;colorama&#39;, &#39;display&#39;, &#39;default&#39;, &#39;extended&#39;)
    def _colorama():
        import colorama
        return colorama

    def process(self, data):
        colorize = not self.args.gray and not self.args.stdout
        lines = self._peeklines(data, colorize)

        if self.args.stdout:
            for line in lines:
                yield line.encode(self.codec)
            return

        stderr = sys.stderr

        if colorize:
            colorama = self._colorama
            if os.name == &#39;nt&#39;:
                stderr = colorama.AnsiToWin32(stderr).stream
            _erase = &#39; &#39; * get_terminal_size()
            _reset = F&#39;\r{colorama.Style.RESET_ALL}{_erase}\r&#39;
        else:
            _reset = &#39;&#39;

        try:
            for line in lines:
                print(line, file=stderr)
        except BaseException:
            stderr.write(_reset)
            raise
        if not self.isatty:
            self.log_info(&#39;forwarding input to next unit&#39;)
            yield data

    def _peekmeta(self, linewidth, sep, meta: dict, peek=None) -&gt; Generator[str, None, None]:
        if not meta and not peek:
            return
        width = max((len(name) for name in meta), default=0)
        separators = iter([sep])
        if peek is not None:
            if len(peek) &gt; linewidth:
                peek = peek[:linewidth - 3] + &#39;...&#39;
            yield from separators
            yield peek
        for name in sorted(meta, key=lambda s: (len(s) &lt;= 3, s)):
            value = meta[name]
            if value is None:
                continue
            if isinstance(value, CustomStringRepresentation):
                value = repr(value).strip()
            elif isbuffer(value):
                value = repr(ByteStringWrapper(value))
            elif isinstance(value, int):
                if value in range(-999, 1000):
                    value = str(value)
                elif value &gt; 0:
                    value = F&#39;0x{value:X}&#39;
                else:
                    value = F&#39;-0x{-value:X}&#39;
            elif isinstance(value, float):
                value = F&#39;{value:.4f}&#39;
            metavar = F&#39;{name:&gt;{width + 2}} = {value!s}&#39;
            if len(metavar) &gt; linewidth:
                metavar = metavar[:linewidth - 3] + &#39;...&#39;
            yield from separators
            yield metavar

    def _trydecode(self, data, codec: Optional[str], width: int, linecount: int) -&gt; str:
        remaining = linecount
        result = []
        wrap = self.args.decode &gt; 1
        if codec is None:
            from refinery.units.encoding.esc import esc
            decoded = data[:abs(width * linecount)]
            decoded = str(decoded | -esc(bare=True))
            limit = abs(min(linecount * width, len(decoded)))
            for k in range(0, limit, width):
                result.append(decoded[k:k + width])
            return result
        try:
            import unicodedata
            unprintable = {&#39;Cc&#39;, &#39;Cf&#39;, &#39;Co&#39;, &#39;Cs&#39;}
            self.log_info(F&#39;trying to decode as {codec}.&#39;)
            decoded = codecs.decode(data, codec, errors=&#39;strict&#39;)
            count = sum(unicodedata.category(c) not in unprintable for c in decoded)
            ratio = count / len(decoded)
        except UnicodeDecodeError as DE:
            self.log_info(&#39;decoding failed:&#39;, DE.reason)
            return None
        except ValueError as V:
            self.log_info(&#39;decoding failed:&#39;, V)
            return None
        if ratio &lt; 0.8:
            self.log_info(F&#39;data contains {ratio * 100:.2f}% printable characters, this is too low.&#39;)
            return None
        decoded = decoded.splitlines(False)
        if not wrap:
            for k, line in enumerate(decoded):
                line = line.replace(&#39;\t&#39;, &#39;\x20&#39; * 4)
                if len(line) &lt;= width:
                    continue
                clipped = line[:width - 3]
                if self.args.gray:
                    color = &#39;&#39;
                    reset = &#39;&#39;
                else:
                    colorama = self._colorama
                    color = colorama.Fore.LIGHTRED_EX
                    reset = colorama.Style.RESET_ALL
                decoded[k] = F&#39;{clipped}{color}...{reset}&#39;
            return decoded[:abs(linecount)]
        for paragraph in decoded:
            if not remaining:
                break
            wrapped = [
                line for chunk in textwrap.wrap(
                    paragraph,
                    width,
                    break_long_words=True,
                    break_on_hyphens=False,
                    drop_whitespace=False,
                    expand_tabs=True,
                    max_lines=abs(remaining + 1),
                    replace_whitespace=False,
                    tabsize=4,
                )
                for line in chunk.splitlines(keepends=False)
            ]
            remaining -= len(wrapped)
            result.extend(wrapped)
        return result[:abs(linecount)]

    def _peeklines(self, data: bytearray, colorize: bool) -&gt; Generator[str, None, None]:

        meta = metavars(data)

        codec = None
        lines = None
        final = data.temp or False
        empty = True

        if not self.args.index:
            meta.discard(&#39;index&#39;)
            index = None
        else:
            index = meta.get(&#39;index&#39;, None)

        if not self.args.brief:
            padding = 0
        else:
            padding = SizeInt.width + 2
            if index is not None:
                padding += 6

        metrics = self._get_metrics(len(data), self.args.lines, padding)

        if self.args.brief:
            metrics.address_width = 0
            metrics.fit_to_width(allow_increase=True)

        sepsize = metrics.hexdump_width
        txtsize = self.args.width or sepsize

        if self.args.lines and data:
            if self.args.escape:
                lines = self._trydecode(data, None, txtsize, metrics.line_count)
            if self.args.decode &gt; 0:
                for codec in (&#39;utf8&#39;, &#39;utf-16le&#39;, &#39;utf-16&#39;, &#39;utf-16be&#39;):
                    lines = self._trydecode(data, codec, txtsize, metrics.line_count)
                    if lines:
                        codec = codec
                        break
                else:
                    codec = None
            if lines is None:
                lines = list(self.hexdump(data, metrics, colorize))
            else:
                sepsize = txtsize

        def separator(title=None):
            if title is None or sepsize &lt;= len(title) + 8:
                return sepsize * &#39;-&#39;
            return &#39;-&#39; * (sepsize - len(title) - 5) + F&#39;[{title}]---&#39;

        if self.args.brief:
            final = False
        elif not self.args.bare:
            peek = repr(meta.size)
            line = separator()
            if len(data) &lt;= 5_000_000:
                peek = F&#39;{peek}; {meta.entropy!r} entropy&#39;
            peek = F&#39;{peek}; {meta.magic!s}&#39;
            if self.args.lines == 0:
                peek = None
            elif not data:
                peek = None
                line = separator(&#39;empty chunk&#39;)
            if self.args.meta &gt; 0:
                meta.derive(&#39;size&#39;)
                meta.derive(&#39;magic&#39;)
                meta.derive(&#39;entropy&#39;)
                peek = None
            if self.args.meta &gt; 1:
                meta.derive(&#39;crc32&#39;)
                meta.derive(&#39;sha256&#39;)
            if self.args.meta &gt; 2:
                for name in meta.derivations:
                    meta[name]
            for line in self._peekmeta(metrics.hexdump_width, line, meta, peek=peek):
                empty = False
                yield line

        if lines:
            empty = False
            if not self.args.brief:
                yield separator(codec or None)
                yield from lines
            else:
                brief = next(iter(lines))
                brief = F&#39;{SizeInt(len(data))!r}: {brief}&#39;
                if index is not None:
                    brief = F&#39;#{index:03d}: {brief}&#39;
                yield brief

        if final and (self.args.bare or not empty):
            yield separator()

    def filter(self, chunks):
        try:
            self._colorama.init(wrap=False)
        except ImportError:
            pass
        discarded = 0
        it = iter(chunks)
        buffer = collections.deque(itertools.islice(it, 0, 2))
        buffer.reverse()

        while buffer:
            if self.isatty and not buffer[0].visible:
                buffer.popleft()
                discarded += 1
            else:
                item = buffer.pop()
                last = not bool(buffer)
                item.temp = last
                if not item.visible and self.isatty:
                    discarded += 1
                else:
                    yield item
            try:
                buffer.appendleft(next(it))
            except StopIteration:
                pass

        if discarded:
            self.log_warn(F&#39;discarded {discarded} invisible chunks to prevent them from leaking into the terminal.&#39;)</code></pre>
</details>
</dd>
<dt id="refinery.shell.pemeta"><code class="flex name class">
<span>class <span class="ident">pemeta</span></span>
<span>(</span><span>custom=False, debug=False, dotnet=False, signatures=False, timestamps=0, version=False, header=False, exports=0, imports=0, tabular=False, timeraw=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Extract metadata from PE files. By default, all information except for imports and exports are
extracted.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/formats/pe/pemeta.py#L145-L706" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class pemeta(Unit):
    &#34;&#34;&#34;
    Extract metadata from PE files. By default, all information except for imports and exports are
    extracted.
    &#34;&#34;&#34;
    def __init__(
        self, custom : Arg(&#39;-c&#39;, &#39;--custom&#39;,
            help=&#39;Unless enabled, all default categories will be extracted.&#39;) = False,
        debug      : Arg.Switch(&#39;-D&#39;, help=&#39;Parse the PDB path from the debug directory.&#39;) = False,
        dotnet     : Arg.Switch(&#39;-N&#39;, help=&#39;Parse the .NET header.&#39;) = False,
        signatures : Arg.Switch(&#39;-S&#39;, help=&#39;Parse digital signatures.&#39;) = False,
        timestamps : Arg.Counts(&#39;-T&#39;, help=&#39;Extract time stamps. Specify twice for more detail.&#39;) = 0,
        version    : Arg.Switch(&#39;-V&#39;, help=&#39;Parse the VERSION resource.&#39;) = False,
        header     : Arg.Switch(&#39;-H&#39;, help=&#39;Parse base data from the PE header.&#39;) = False,
        exports    : Arg.Counts(&#39;-E&#39;, help=&#39;List all exported functions. Specify twice to include addresses.&#39;) = 0,
        imports    : Arg.Counts(&#39;-I&#39;, help=&#39;List all imported functions. Specify twice to include addresses.&#39;) = 0,
        tabular    : Arg.Switch(&#39;-t&#39;, help=&#39;Print information in a table rather than as JSON&#39;) = False,
        timeraw    : Arg.Switch(&#39;-r&#39;, help=&#39;Extract time stamps as numbers instead of human-readable format.&#39;) = False,
    ):
        if not custom and not any((debug, dotnet, signatures, timestamps, version, header)):
            debug = dotnet = signatures = timestamps = version = header = True
        super().__init__(
            debug=debug,
            dotnet=dotnet,
            signatures=signatures,
            timestamps=timestamps,
            version=version,
            header=header,
            imports=imports,
            exports=exports,
            timeraw=timeraw,
            tabular=tabular,
        )

    @classmethod
    def _ensure_string(cls, x):
        if not isinstance(x, str):
            x = repr(x) if not isinstance(x, bytes) else x.decode(cls.codec, &#39;backslashreplace&#39;)
        return x

    @classmethod
    def _parse_pedict(cls, bin):
        return dict((
            cls._ensure_string(key).replace(&#34; &#34;, &#34;&#34;),
            cls._ensure_string(val)
        ) for key, val in bin.items() if val)

    @classmethod
    def parse_signature(cls, data: bytearray) -&gt; dict:
        &#34;&#34;&#34;
        Extracts a JSON-serializable and human-readable dictionary with information about
        time stamp and code signing certificates that are attached to the input PE file.
        &#34;&#34;&#34;
        from refinery.units.formats.pkcs7 import pkcs7

        try:
            signature = data | pkcs7 | json.loads
        except Exception as E:
            raise ValueError(F&#39;PKCS7 parser failed with error: {E!s}&#39;)

        info = {}

        def _value(doc: dict, require_type=None):
            if require_type is not None:
                if doc.get(&#39;type&#39;, None) != require_type:
                    raise LookupError
            value = doc.get(&#39;value&#39;, None)
            value = [value] if value else doc.get(&#39;values&#39;, [])
            if not value:
                raise LookupError
            return value[0]

        def find_timestamps(entry) -&gt; dict:
            if isinstance(entry, dict):
                try:
                    return {&#39;Timestamp&#39;: _value(entry, &#39;signing_time&#39;)}
                except LookupError:
                    pass
                for value in entry.values():
                    result = find_timestamps(value)
                    if result is None:
                        continue
                    with suppress(KeyError):
                        result.setdefault(&#39;TimestampIssuer&#39;, entry[&#39;sid&#39;][&#39;issuer&#39;][&#39;common_name&#39;])
                    return result
            elif isinstance(entry, list):
                for value in entry:
                    result = find_timestamps(value)
                    if result is None:
                        continue
                    return result

        timestamp_info = find_timestamps(signature)
        if timestamp_info is not None:
            info.update(timestamp_info)

        try:
            certificates = signature[&#39;content&#39;][&#39;certificates&#39;]
        except KeyError:
            return info

        if len(certificates) == 1:
            main_certificate = certificates[0]
        else:
            certificates_with_extended_use = []
            main_certificate = None
            for certificate in certificates:
                with suppress(Exception):
                    crt = certificate[&#39;tbs_certificate&#39;]
                    ext = [e for e in crt[&#39;extensions&#39;] if e[&#39;extn_id&#39;] == &#39;extended_key_usage&#39; and e[&#39;extn_value&#39;] != [&#39;time_stamping&#39;]]
                    key = [e for e in crt[&#39;extensions&#39;] if e[&#39;extn_id&#39;] == &#39;key_usage&#39;]
                    if ext:
                        certificates_with_extended_use.append(certificate)
                    if any(&#39;key_cert_sign&#39; in e[&#39;extn_value&#39;] for e in key):
                        continue
                    if any(&#39;code_signing&#39; in e[&#39;extn_value&#39;] for e in ext):
                        main_certificate = certificate
                        break
            if main_certificate is None and len(certificates_with_extended_use) == 1:
                main_certificate = certificates_with_extended_use[0]
        if main_certificate:
            crt = main_certificate[&#39;tbs_certificate&#39;]
            serial = crt[&#39;serial_number&#39;]
            if isinstance(serial, int):
                serial = F&#39;{serial:x}&#39;
            if len(serial) % 2 != 0:
                serial = F&#39;0{serial}&#39;
            assert bytes.fromhex(serial) in data
            subject = crt[&#39;subject&#39;]
            location = [subject.get(t, &#39;&#39;) for t in (&#39;locality_name&#39;, &#39;state_or_province_name&#39;, &#39;country_name&#39;)]
            info.update(Subject=subject[&#39;common_name&#39;])
            if any(location):
                info.update(SubjectLocation=&#39;, &#39;.join(filter(None, location)))
            for signer_info in signature[&#39;content&#39;].get(&#39;signer_infos&#39;, ()):
                try:
                    if signer_info[&#39;sid&#39;][&#39;serial_number&#39;] != crt[&#39;serial_number&#39;]:
                        continue
                    for attr in signer_info[&#39;signed_attrs&#39;]:
                        if attr[&#39;type&#39;] == &#39;authenticode_info&#39;:
                            auth = _value(attr)
                            info.update(ProgramName=auth[&#39;programName&#39;])
                            info.update(MoreInfo=auth[&#39;moreInfo&#39;])
                except KeyError:
                    continue
            try:
                valid_from = crt[&#39;validity&#39;][&#39;not_before&#39;]
                valid_until = crt[&#39;validity&#39;][&#39;not_after&#39;]
            except KeyError:
                pass
            else:
                info.update(ValidFrom=valid_from, ValidUntil=valid_until)
            info.update(
                Issuer=crt[&#39;issuer&#39;][&#39;common_name&#39;], Fingerprint=main_certificate[&#39;fingerprint&#39;], Serial=serial)
            return info
        return info

    def _pe_characteristics(self, pe: PE):
        return {name for name, mask in image_characteristics
            if pe.FILE_HEADER.Characteristics &amp; mask}

    def _pe_address_width(self, pe: PE, default=16) -&gt; int:
        if &#39;IMAGE_FILE_16BIT_MACHINE&#39; in self._pe_characteristics(pe):
            return 4
        elif MACHINE_TYPE[pe.FILE_HEADER.Machine] in [&#39;IMAGE_FILE_MACHINE_I386&#39;]:
            return 8
        elif MACHINE_TYPE[pe.FILE_HEADER.Machine] in [
            &#39;IMAGE_FILE_MACHINE_AMD64&#39;,
            &#39;IMAGE_FILE_MACHINE_IA64&#39;,
        ]:
            return 16
        else:
            return default

    def _vint(self, pe: PE, value: int):
        if not self.args.tabular:
            return value
        aw = self._pe_address_width(pe)
        return F&#39;0x{value:0{aw}X}&#39;

    def parse_version(self, pe: PE, data=None) -&gt; dict:
        &#34;&#34;&#34;
        Extracts a JSON-serializable and human-readable dictionary with information about
        the version resource of an input PE file, if available.
        &#34;&#34;&#34;
        pe.parse_data_directories(directories=[DIRECTORY_ENTRY[&#39;IMAGE_DIRECTORY_ENTRY_RESOURCE&#39;]])
        string_table_entries = []
        for FileInfo in pe.FileInfo:
            for FileInfoEntry in FileInfo:
                with suppress(AttributeError):
                    for StringTableEntry in FileInfoEntry.StringTable:
                        StringTableEntryParsed = self._parse_pedict(StringTableEntry.entries)
                        with suppress(AttributeError):
                            LangID = StringTableEntry.entries.get(&#39;LangID&#39;, None) or StringTableEntry.LangID
                            LangID = int(LangID, 0x10) if not isinstance(LangID, int) else LangID
                            LangHi = LangID &gt;&gt; 0x10
                            LangLo = LangID &amp; 0xFFFF
                            Language = LCID.get(LangHi, &#39;Language Neutral&#39;)
                            Charset = self._CHARSET.get(LangLo, &#39;Unknown Charset&#39;)
                            StringTableEntryParsed.update(
                                LangID=F&#39;{LangID:08X}&#39;,
                                Charset=Charset,
                                Language=Language
                            )
                        for key in StringTableEntryParsed:
                            if key.endswith(&#39;Version&#39;):
                                value = StringTableEntryParsed[key]
                                separator = &#39;, &#39;
                                if re.match(F&#39;\\d+({re.escape(separator)}\\d+){{3}}&#39;, value):
                                    StringTableEntryParsed[key] = &#39;.&#39;.join(value.split(separator))
                        string_table_entries.append(StringTableEntryParsed)
        if not string_table_entries:
            return None
        elif len(string_table_entries) == 1:
            return string_table_entries[0]
        else:
            return string_table_entries

    def parse_exports(self, pe: PE, data=None, include_addresses=False) -&gt; list:
        pe.parse_data_directories(directories=[DIRECTORY_ENTRY[&#39;IMAGE_DIRECTORY_ENTRY_EXPORT&#39;]])
        base = pe.OPTIONAL_HEADER.ImageBase
        info = []
        for k, exp in enumerate(pe.DIRECTORY_ENTRY_EXPORT.symbols):
            if not exp.name:
                name = F&#39;@{k}&#39;
            else:
                name = exp.name.decode(&#39;ascii&#39;)
            item = {&#39;Name&#39;: name, &#39;Address&#39;: self._vint(pe, exp.address + base)} if include_addresses else name
            info.append(item)
        return info

    def parse_imports(self, pe: PE, data=None, include_addresses=False) -&gt; list:
        info = {}
        dirs = []
        for name in [
            &#39;DIRECTORY_ENTRY_IMPORT&#39;,
            &#39;DIRECTORY_ENTRY_DELAY_IMPORT&#39;,
        ]:
            pe.parse_data_directories(directories=[DIRECTORY_ENTRY[F&#39;IMAGE_{name}&#39;]])
            with suppress(AttributeError):
                dirs.append(getattr(pe, name))
        self.log_warn(dirs)
        for idd in itertools.chain(*dirs):
            dll: bytes = idd.dll
            dll = dll.decode(&#39;ascii&#39;)
            if dll.lower().endswith(&#39;.dll&#39;):
                dll = dll[:~3]
            imports: list[str] = info.setdefault(dll, [])
            with suppress(AttributeError):
                symbols = idd.imports
            with suppress(AttributeError):
                symbols = idd.entries
            try:
                for imp in symbols:
                    name: bytes = imp.name
                    name = name and name.decode(&#39;ascii&#39;) or F&#39;@{imp.ordinal}&#39;
                    if not include_addresses:
                        imports.append(name)
                    else:
                        imports.append(dict(Name=name, Address=self._vint(pe, imp.address)))
            except Exception as e:
                self.log_warn(F&#39;error parsing {name}: {e!s}&#39;)
        return info

    def parse_header(self, pe: PE, data=None) -&gt; dict:
        def format_macro_name(name: str, prefix, convert=True):
            name = name.split(&#39;_&#39;)[prefix:]
            if convert:
                for k, part in enumerate(name):
                    name[k] = part.upper() if len(part) &lt;= 3 else part.capitalize()
            return &#39; &#39;.join(name)

        major = pe.OPTIONAL_HEADER.MajorOperatingSystemVersion
        minor = pe.OPTIONAL_HEADER.MinorOperatingSystemVersion
        version = self._WINVER.get(major, {0: &#39;Unknown&#39;})

        try:
            MinimumOS = version[minor]
        except LookupError:
            MinimumOS = version[0]
        header_information = {
            &#39;Machine&#39;: format_macro_name(MACHINE_TYPE[pe.FILE_HEADER.Machine], 3, False),
            &#39;Subsystem&#39;: format_macro_name(SUBSYSTEM_TYPE[pe.OPTIONAL_HEADER.Subsystem], 2),
            &#39;MinimumOS&#39;: MinimumOS,
        }

        pe.parse_data_directories(directories=[
            DIRECTORY_ENTRY[&#39;IMAGE_DIRECTORY_ENTRY_EXPORT&#39;],
        ])

        try:
            export_name = pe.DIRECTORY_ENTRY_EXPORT.name
            if isinstance(export_name, bytes):
                export_name = export_name.decode(&#39;utf8&#39;)
            if not export_name.isprintable():
                export_name = None
        except Exception:
            export_name = None
        if export_name:
            header_information[&#39;ExportName&#39;] = export_name

        rich_header = pe.parse_rich_header()
        rich = []

        if rich_header:
            it = rich_header.get(&#39;values&#39;, [])
            if self.args.tabular:
                cw = max(len(F&#39;{c:d}&#39;) for c in it[1::2])
            for idv, count in zip(it[0::2], it[1::2]):
                info = get_rich_info(idv)
                if not info:
                    continue
                pid = info.pid.upper()
                if self.args.tabular:
                    short_pid = get_rich_short_pid(pid)
                    rich.append(F&#39;[{idv:08x}] {count:&gt;0{cw}d} {short_pid!s} {info.ver}&#39;)
                else:
                    rich.append({
                        &#39;Counter&#39;: count,
                        &#39;Encoded&#39;: F&#39;{idv:08x}&#39;,
                        &#39;Library&#39;: pid,
                        &#39;Product&#39;: info.ver,
                    })
            header_information[&#39;RICH&#39;] = rich

        characteristics = self._pe_characteristics(pe)
        for typespec, flag in {
            &#39;EXE&#39;: &#39;IMAGE_FILE_EXECUTABLE_IMAGE&#39;,
            &#39;DLL&#39;: &#39;IMAGE_FILE_DLL&#39;,
            &#39;SYS&#39;: &#39;IMAGE_FILE_SYSTEM&#39;
        }.items():
            if flag in characteristics:
                header_information[&#39;Type&#39;] = typespec

        base = pe.OPTIONAL_HEADER.ImageBase
        header_information[&#39;ImageBase&#39;] = self._vint(pe, base)
        header_information[&#39;ImageSize&#39;] = get_pe_size(pe)
        header_information[&#39;Bits&#39;] = 4 * self._pe_address_width(pe, 16)
        header_information[&#39;EntryPoint&#39;] = self._vint(pe, pe.OPTIONAL_HEADER.AddressOfEntryPoint + base)
        return header_information

    def parse_time_stamps(self, pe: PE, raw_time_stamps: bool, more_detail: bool) -&gt; dict:
        &#34;&#34;&#34;
        Extracts time stamps from the PE header (link time), as well as from the imports,
        exports, debug, and resource directory. The resource time stamp is also parsed as
        a DOS time stamp and returned as the &#34;Delphi&#34; time stamp.
        &#34;&#34;&#34;
        if raw_time_stamps:
            def dt(ts): return ts
        else:
            dt = date_from_timestamp

        pe.parse_data_directories(directories=[
            DIRECTORY_ENTRY[&#39;IMAGE_DIRECTORY_ENTRY_IMPORT&#39;],
            DIRECTORY_ENTRY[&#39;IMAGE_DIRECTORY_ENTRY_EXPORT&#39;],
            DIRECTORY_ENTRY[&#39;IMAGE_DIRECTORY_ENTRY_BOUND_IMPORT&#39;],
            DIRECTORY_ENTRY[&#39;IMAGE_DIRECTORY_ENTRY_DELAY_IMPORT&#39;],
            DIRECTORY_ENTRY[&#39;IMAGE_DIRECTORY_ENTRY_DEBUG&#39;],
            DIRECTORY_ENTRY[&#39;IMAGE_DIRECTORY_ENTRY_RESOURCE&#39;]
        ])

        info = {}

        with suppress(AttributeError):
            info.update(Linker=dt(pe.FILE_HEADER.TimeDateStamp))

        for dir_name, _dll, info_key in [
            (&#39;DIRECTORY_ENTRY_IMPORT&#39;,       &#39;dll&#39;,  &#39;Import&#39;), # noqa
            (&#39;DIRECTORY_ENTRY_DELAY_IMPORT&#39;, &#39;dll&#39;,  &#39;Symbol&#39;), # noqa
            (&#39;DIRECTORY_ENTRY_BOUND_IMPORT&#39;, &#39;name&#39;, &#39;Module&#39;), # noqa
        ]:
            impts = {}
            for entry in getattr(pe, dir_name, []):
                ts = 0
                with suppress(AttributeError):
                    ts = entry.struct.dwTimeDateStamp
                with suppress(AttributeError):
                    ts = entry.struct.TimeDateStamp
                if ts == 0 or ts == 0xFFFFFFFF:
                    continue
                name = getattr(entry, _dll, B&#39;&#39;).decode()
                if name.lower().endswith(&#39;.dll&#39;):
                    name = name[:-4]
                impts[name] = dt(ts)
            if not impts:
                continue
            if not more_detail:
                dmin = min(impts.values())
                dmax = max(impts.values())
                small_delta = 2 * 60 * 60
                if not raw_time_stamps:
                    small_delta = timedelta(seconds=small_delta)
                if dmax - dmin &lt; small_delta:
                    impts = dmin
            info[info_key] = impts

        with suppress(AttributeError):
            Export = pe.DIRECTORY_ENTRY_EXPORT.struct.TimeDateStamp
            if Export: info.update(Export=dt(Export))

        with suppress(AttributeError):
            res_timestamp = pe.DIRECTORY_ENTRY_RESOURCE.struct.TimeDateStamp
            if res_timestamp:
                with suppress(ValueError):
                    from refinery.units.misc.datefix import datefix
                    dos = datefix.dostime(res_timestamp)
                    info.update(Delphi=dos)
                    info.update(RsrcTS=dt(res_timestamp))

        def norm(value):
            if isinstance(value, list):
                return [norm(v) for v in value]
            if isinstance(value, dict):
                return {k: norm(v) for k, v in value.items()}
            if isinstance(value, int):
                return value
            return str(value)

        return {key: norm(value) for key, value in info.items()}

    def parse_dotnet(self, pe: PE, data):
        &#34;&#34;&#34;
        Extracts a JSON-serializable and human-readable dictionary with information about
        the .NET metadata of an input PE file.
        &#34;&#34;&#34;
        header = DotNetHeader(data, pe=pe)
        tables = header.meta.Streams.Tables
        info = dict(
            RuntimeVersion=F&#39;{header.head.MajorRuntimeVersion}.{header.head.MinorRuntimeVersion}&#39;,
            Version=F&#39;{header.meta.MajorVersion}.{header.meta.MinorVersion}&#39;,
            VersionString=header.meta.VersionString
        )

        info[&#39;Flags&#39;] = [name for name, check in header.head.KnownFlags.items() if check]

        if len(tables.Assembly) == 1:
            assembly = tables.Assembly[0]
            info.update(
                AssemblyName=assembly.Name,
                Release=&#39;{}.{}.{}.{}&#39;.format(
                    assembly.MajorVersion,
                    assembly.MinorVersion,
                    assembly.BuildNumber,
                    assembly.RevisionNumber
                )
            )

        try:
            entry = self._vint(pe, header.head.EntryPointToken + pe.OPTIONAL_HEADER.ImageBase)
            info.update(EntryPoint=entry)
        except AttributeError:
            pass

        if len(tables.Module) == 1:
            module = tables.Module[0]
            info.update(ModuleName=module.Name)

        return info

    def parse_debug(self, pe: PE, data=None):
        result = {}
        pe.parse_data_directories(directories=[
            DIRECTORY_ENTRY[&#39;IMAGE_DIRECTORY_ENTRY_DEBUG&#39;]])
        for dbg in pe.DIRECTORY_ENTRY_DEBUG:
            if DEBUG_TYPE.get(dbg.struct.Type, None) != &#39;IMAGE_DEBUG_TYPE_CODEVIEW&#39;:
                continue
            with suppress(Exception):
                pdb = dbg.entry.PdbFileName
                if 0 in pdb:
                    pdb = pdb[:pdb.index(0)]
                result.update(
                    PdbPath=pdb.decode(self.codec),
                    PdbAge=dbg.entry.Age
                )
        return result

    def process(self, data):
        result = {}
        pe = PE(data=data, fast_load=True)

        for switch, resolver, name in [
            (self.args.debug,   self.parse_debug,    &#39;Debug&#39;),    # noqa
            (self.args.dotnet,  self.parse_dotnet,   &#39;DotNet&#39;),   # noqa
            (self.args.header,  self.parse_header,   &#39;Header&#39;),   # noqa
            (self.args.version, self.parse_version,  &#39;Version&#39;),  # noqa
            (self.args.imports, self.parse_imports,  &#39;Imports&#39;),  # noqa
            (self.args.exports, self.parse_exports,  &#39;Exports&#39;),  # noqa
        ]:
            if not switch:
                continue
            self.log_debug(F&#39;parsing: {name}&#39;)
            args = pe, data
            if switch &gt; 1:
                args = *args, True
            try:
                info = resolver(*args)
            except Exception as E:
                self.log_info(F&#39;failed to obtain {name}: {E!s}&#39;)
                continue
            if info:
                result[name] = info

        signature = {}

        if self.args.timestamps or self.args.signatures:
            with suppress(Exception):
                from refinery.units.formats.pe.pesig import pesig
                signature = self.parse_signature(next(data | pesig))

        if self.args.timestamps:
            ts = self.parse_time_stamps(pe, self.args.timeraw, self.args.timestamps &gt; 1)
            with suppress(KeyError):
                ts.update(Signed=signature[&#39;Timestamp&#39;])
            result.update(TimeStamp=ts)

        if signature and self.args.signatures:
            result[&#39;Signature&#39;] = signature

        if result:
            yield from ppjson(tabular=self.args.tabular)._pretty_output(result, indent=4, ensure_ascii=False)

    _CHARSET = {
        0x0000: &#39;7-bit ASCII&#39;,
        0x03A4: &#39;Japan (Shift ? JIS X-0208)&#39;,
        0x03B5: &#39;Korea (Shift ? KSC 5601)&#39;,
        0x03B6: &#39;Taiwan (Big5)&#39;,
        0x04B0: &#39;Unicode&#39;,
        0x04E2: &#39;Latin-2 (Eastern European)&#39;,
        0x04E3: &#39;Cyrillic&#39;,
        0x04E4: &#39;Multilingual&#39;,
        0x04E5: &#39;Greek&#39;,
        0x04E6: &#39;Turkish&#39;,
        0x04E7: &#39;Hebrew&#39;,
        0x04E8: &#39;Arabic&#39;,
    }

    _WINVER = {
        3: {
            0x00: &#39;Windows NT 3&#39;,
            0x0A: &#39;Windows NT 3.1&#39;,
            0x32: &#39;Windows NT 3.5&#39;,
            0x33: &#39;Windows NT 3.51&#39;,
        },
        4: {
            0x00: &#39;Windows 95&#39;,
            0x0A: &#39;Windows 98&#39;,
        },
        5: {
            0x00: &#39;Windows 2000&#39;,
            0x5A: &#39;Windows Me&#39;,
            0x01: &#39;Windows XP&#39;,
            0x02: &#39;Windows Server 2003&#39;,
        },
        6: {
            0x00: &#39;Windows Vista&#39;,
            0x01: &#39;Windows 7&#39;,
            0x02: &#39;Windows 8&#39;,
            0x03: &#39;Windows 8.1&#39;,
        },
        10: {
            0x00: &#39;Windows 10&#39;,
        }
    }</code></pre>
</details>
</dd>
<dt id="refinery.shell.peoverlay"><code class="flex name class">
<span>class <span class="ident">peoverlay</span></span>
<span>(</span><span>certificate=False, directories=False, memdump=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Returns the overlay of a PE file, i.e. anything that may have been appended to the file.
This does not include digital signatures. Use <code><a title="refinery.pestrip" href="index.html#refinery.pestrip">pestrip</a></code> to obtain only the body
of the PE file after removing the overlay.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/formats/pe/peoverlay.py#L6-L19" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class peoverlay(OverlayUnit):
    &#34;&#34;&#34;
    Returns the overlay of a PE file, i.e. anything that may have been appended to the file.
    This does not include digital signatures. Use `refinery.pestrip` to obtain only the body
    of the PE file after removing the overlay.
    &#34;&#34;&#34;
    def process(self, data: bytearray) -&gt; bytearray:
        size = self._get_size(data)
        try:
            data[:size] = []
        except Exception:
            return data[size:]
        else:
            return data</code></pre>
</details>
</dd>
<dt id="refinery.shell.perc"><code class="flex name class">
<span>class <span class="ident">perc</span></span>
<span>(</span><span>*paths, pretty=False, path=b'path', regex=False, exact=False, fuzzy=0, drop_path=False, join_path=False, list=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Extract PE file resources.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/formats/pe/perc.py#L61-L425" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class perc(PathExtractorUnit):
    &#34;&#34;&#34;
    Extract PE file resources.
    &#34;&#34;&#34;
    def __init__(
        self, *paths,
        pretty: Arg.Switch(&#39;-p&#39;, help=&#39;Add missing headers to bitmap and icon resources.&#39;) = False,
        **kwargs
    ):
        super().__init__(*paths, pretty=pretty, **kwargs)

    def _get_icon_dir(self, pe: pefile.PE):
        try:
            group, = (e for e in pe.DIRECTORY_ENTRY_RESOURCE.entries if e.id == RSRC.ICON_GROUP.value)
            group = group.directory.entries[0].directory.entries[0].data.struct
            return GRPICONDIR(pe.get_data(group.OffsetToData, group.Size))
        except Exception:
            return None

    def _search(self, pe: pefile.PE, directory, level=0, *parts):
        if level &gt;= 3:
            self.log_warn(F&#39;unexpected resource tree level {level + 1:d}&#39;)
        for entry in directory.entries:
            if entry.name:
                identifier = str(entry.name)
            elif level == 0 and entry.id in iter(RSRC):
                identifier = RSRC(entry.id)
            elif entry.id is not None:
                identifier = entry.id
            else:
                self.log_warn(F&#39;resource entry has name {entry.name} and id {entry.id} at level {level + 1:d}&#39;)
                continue
            if entry.struct.DataIsDirectory:
                yield from self._search(pe, entry.directory, level + 1, *parts, identifier)
            else:
                rva = entry.data.struct.OffsetToData
                size = entry.data.struct.Size
                path = &#39;/&#39;.join(str(p) for p in (*parts, identifier))
                extract = None
                if self.args.pretty:
                    if parts[0] is RSRC.BITMAP:
                        extract = self._handle_bitmap(pe, rva, size)
                    elif parts[0] is RSRC.ICON:
                        extract = self._handle_icon(pe, parts, rva, size)
                    elif parts[0] is RSRC.STRING:
                        extract = self._handle_strings(pe, parts, rva, size)
                if extract is None:
                    def extract(pe=pe):
                        return pe.get_data(rva, size)
                yield UnpackResult(
                    path,
                    extract,
                    offset=pe.get_offset_from_rva(rva),
                    lcid=self._get_lcid(entry.data),
                )

    def _get_lcid(self, node_data) -&gt; Optional[str]:
        try:
            pid = node_data.lang or 0
            sid = node_data.sublang or 0
        except AttributeError:
            return None
        try:
            pid = self._LANG_ID_TO_LCID[pid]
        except KeyError:
            return None
        lcid = pid.get(sid, 0)
        return LCID.get(lcid)

    def _handle_strings(self, pe: pefile.PE, parts: Tuple[RSRC, int, int], rva: int, size: int):
        def extract(pe=pe):
            self.log_debug(parts)
            base = (parts[1] - 1) &lt;&lt; 4
            reader = StructReader(pe.get_data(rva, size))
            table = {}
            index = 0
            while not reader.eof:
                string = reader.read_exactly(reader.u16() * 2)
                if not string:
                    break
                key = F&#39;{base + index:04X}&#39;
                table[key] = string.decode(&#39;utf-16le&#39;)
                index += 1
            return json.dumps(table, indent=4).encode(self.codec)
        return extract

    def _handle_bitmap(self, pe: pefile.PE, rva: int, size: int):
        def extract(pe=pe):
            bitmap = pe.get_data(rva, size)
            total = (len(bitmap) + 14).to_bytes(4, &#39;little&#39;)
            return B&#39;BM&#39; + total + B&#39;\0\0\0\0\x36\0\0\0&#39; + bitmap
        return extract

    def _handle_icon(self, pe: pefile.PE, parts: Tuple[RSRC, int, int], rva: int, size: int):
        try:
            icondir = self._get_icon_dir(pe)
            index = int(parts[1]) - 1
            info = icondir.entries[index]
            icon = pe.get_data(rva, size)
        except Exception:
            return None
        if icon.startswith(B&#39;(\0\0\0&#39;):
            header = struct.pack(&#39;&lt;HHHBBBBHHII&#39;,
                0,
                1,
                1,
                info.width,
                info.height,
                info.color_count,
                0,
                info.planes,
                info.bit_count,
                len(icon),
                0x16
            )
            icon = header + icon
        return icon

    def unpack(self, data):
        pe = pefile.PE(data=data, fast_load=True)
        pe.parse_data_directories(
            directories=pefile.DIRECTORY_ENTRY[&#39;IMAGE_DIRECTORY_ENTRY_RESOURCE&#39;])
        try:
            rsrc = pe.DIRECTORY_ENTRY_RESOURCE
        except AttributeError:
            pass
        else:
            yield from self._search(pe, rsrc)

    def _mktbl(ids: List[Tuple[int, int, int]]) -&gt; Dict[int, Dict[int, int]]:
        table = {}
        for pid, sid, lcid in ids:
            if pid not in table:
                table[pid] = {0: lcid}
            table[pid][sid] = lcid
        return table

    _LANG_ID_TO_LCID = _mktbl([
        (0x00, 0x03, 0x0C00),
        (0x00, 0x05, 0x1400),
        (0x7F, 0x00, 0x007F),
        (0x00, 0x00, 0x0000),
        (0x02, 0x02, 0x0800),
        (0x00, 0x04, 0x1000),
        (0x00, 0x01, 0x0400),
        (0x36, 0x01, 0x0436),
        (0x1c, 0x01, 0x041C),
        (0x84, 0x01, 0x0484),
        (0x5E, 0x01, 0x045E),
        (0x01, 0x05, 0x1401),
        (0x01, 0x0f, 0x3C01),
        (0x01, 0x03, 0x0C01),
        (0x01, 0x02, 0x0801),
        (0x01, 0x0B, 0x2C01),
        (0x01, 0x0D, 0x3401),
        (0x01, 0x0C, 0x3001),
        (0x01, 0x04, 0x1001),
        (0x01, 0x06, 0x1801),
        (0x01, 0x08, 0x2001),
        (0x01, 0x10, 0x4001),
        (0x01, 0x01, 0x0401),
        (0x01, 0x0A, 0x2801),
        (0x01, 0x07, 0x1C01),
        (0x01, 0x0E, 0x3801),
        (0x01, 0x09, 0x2401),
        (0x2B, 0x01, 0x042B),
        (0x4D, 0x01, 0x044D),
        (0x2C, 0x02, 0x082C),
        (0x2C, 0x01, 0x042C),
        (0x45, 0x02, 0x0445),
        (0x6D, 0x01, 0x046D),
        (0x2d, 0x01, 0x042D),
        (0x23, 0x01, 0x0423),
        (0x1A, 0x08, 0x201A),
        (0x1A, 0x05, 0x141A),
        (0x7E, 0x01, 0x047E),
        (0x02, 0x01, 0x0402),
        (0x92, 0x01, 0x0492),
        (0x5C, 0x01, 0x045C),
        (0x03, 0x01, 0x0403),
        (0x04, 0x03, 0x0C04),
        (0x04, 0x05, 0x1404),
        (0x04, 0x04, 0x1004),
        (0x04, 0x02, 0x0004),
        (0x04, 0x01, 0x7C04),
        (0x83, 0x01, 0x0483),
        (0x1A, None, 0x001A),
        (0x1a, 0x04, 0x101A),
        (0x1a, 0x01, 0x041A),
        (0x05, 0x01, 0x0405),
        (0x06, 0x01, 0x0406),
        (0x8C, 0x01, 0x048C),
        (0x65, 0x01, 0x0465),
        (0x13, 0x02, 0x0813),
        (0x13, 0x01, 0x0413),
        (0x09, 0x03, 0x0C09),
        (0x09, 0x0A, 0x2809),
        (0x09, 0x04, 0x1009),
        (0x09, 0x09, 0x2409),
        (0x09, 0x10, 0x4009),
        (0x09, 0x06, 0x1809),
        (0x09, 0x08, 0x2009),
        (0x09, 0x11, 0x4409),
        (0x09, 0x05, 0x1409),
        (0x09, 0x0D, 0x3409),
        (0x09, 0x12, 0x4809),
        (0x09, 0x07, 0x1c09),
        (0x09, 0x0B, 0x2C09),
        (0x09, 0x02, 0x0809),
        (0x09, 0x01, 0x0409),
        (0x09, 0x0C, 0x3009),
        (0x25, 0x01, 0x0425),
        (0x38, 0x01, 0x0438),
        (0x64, 0x01, 0x0464),
        (0x0B, 0x01, 0x040B),
        (0x0C, 0x02, 0x080c),
        (0x0C, 0x03, 0x0C0C),
        (0x0C, 0x01, 0x040c),
        (0x0C, 0x05, 0x140C),
        (0x0C, 0x06, 0x180C),
        (0x0C, 0x04, 0x100C),
        (0x62, 0x01, 0x0462),
        (0x56, 0x01, 0x0456),
        (0x37, 0x01, 0x0437),
        (0x07, 0x03, 0x0C07),
        (0x07, 0x01, 0x0407),
        (0x07, 0x05, 0x1407),
        (0x07, 0x04, 0x1007),
        (0x07, 0x02, 0x0807),
        (0x08, 0x01, 0x0408),
        (0x6F, 0x01, 0x046F),
        (0x47, 0x01, 0x0447),
        (0x68, 0x01, 0x0468),
        (0x75, 0x01, 0x0475),
        (0x0D, 0x01, 0x040D),
        (0x39, 0x01, 0x0439),
        (0x0E, 0x01, 0x040E),
        (0x0F, 0x01, 0x040F),
        (0x70, 0x01, 0x0470),
        (0x21, 0x01, 0x0421),
        (0x5D, 0x02, 0x085D),
        (0x5D, 0x01, 0x045D),
        (0x3C, 0x02, 0x083C),
        (0x34, 0x01, 0x0434),
        (0x35, 0x01, 0x0435),
        (0x10, 0x01, 0x0410),
        (0x10, 0x02, 0x0810),
        (0x11, 0x01, 0x0411),
        (0x4B, 0x01, 0x044B),
        (0x3F, 0x01, 0x043F),
        (0x53, 0x01, 0x0453),
        (0x86, 0x01, 0x0486),
        (0x87, 0x01, 0x0487),
        (0x57, 0x01, 0x0457),
        (0x12, 0x01, 0x0412),
        (0x40, 0x01, 0x0440),
        (0x54, 0x01, 0x0454),
        (0x26, 0x01, 0x0426),
        (0x27, 0x01, 0x0427),
        (0x2E, 0x02, 0x082E),
        (0x6E, 0x01, 0x046E),
        (0x2F, 0x01, 0x042F),
        (0x3E, 0x02, 0x083E),
        (0x3E, 0x01, 0x043e),
        (0x4C, 0x01, 0x044C),
        (0x3A, 0x01, 0x043A),
        (0x81, 0x01, 0x0481),
        (0x7A, 0x01, 0x047A),
        (0x4E, 0x01, 0x044E),
        (0x7C, 0x01, 0x047C),
        (0x50, 0x01, 0x0450),
        (0x50, 0x02, 0x0850),
        (0x61, 0x01, 0x0461),
        (0x14, 0x01, 0x0414),
        (0x14, 0x02, 0x0814),
        (0x82, 0x01, 0x0482),
        (0x48, 0x01, 0x0448),
        (0x63, 0x01, 0x0463),
        (0x29, 0x01, 0x0429),
        (0x15, 0x01, 0x0415),
        (0x16, 0x01, 0x0416),
        (0x16, 0x02, 0x0816),
        (0x67, 0x02, 0x0867),
        (0x46, 0x01, 0x0446),
        (0x46, 0x02, 0x0846),
        (0x6B, 0x01, 0x046B),
        (0x6B, 0x02, 0x086B),
        (0x6B, 0x03, 0x0C6B),
        (0x18, 0x01, 0x0418),
        (0x17, 0x01, 0x0417),
        (0x19, 0x01, 0x0419),
        (0x85, 0x01, 0x0485),
        (0x3B, 0x09, 0x243B),
        (0x3B, 0x04, 0x103B),
        (0x3B, 0x05, 0x143B),
        (0x3B, 0x03, 0x0C3B),
        (0x3B, 0x01, 0x043B),
        (0x3B, 0x02, 0x083B),
        (0x3B, 0x08, 0x203B),
        (0x3B, 0x06, 0x183B),
        (0x3B, 0x07, 0x1C3B),
        (0x4F, 0x01, 0x044F),
        (0x1a, 0x07, 0x1C1A),
        (0x1a, 0x06, 0x181A),
        (0x1a, 0x03, 0x0C1A),
        (0x1a, 0x02, 0x081A),
        (0x6C, 0x01, 0x046C),
        (0x32, 0x02, 0x0832),
        (0x32, 0x01, 0x0432),
        (0x32, 0x01, 0x0459),
        (0x32, 0x02, 0x0859),
        (0x5B, 0x01, 0x045B),
        (0x1b, 0x01, 0x041B),
        (0x24, 0x01, 0x0424),
        (0x0A, 0x0b, 0x2C0A),
        (0x0A, 0x10, 0x400A),
        (0x0A, 0x0D, 0x340A),
        (0x0A, 0x09, 0x240A),
        (0x0A, 0x05, 0x140A),
        (0x0A, 0x07, 0x1C0A),
        (0x0A, 0x0C, 0x300A),
        (0x0A, 0x11, 0x440A),
        (0x0A, 0x04, 0x100A),
        (0x0A, 0x12, 0x480A),
        (0x0A, 0x02, 0x080A),
        (0x0A, 0x13, 0x4C0A),
        (0x0A, 0x06, 0x180A),
        (0x0A, 0x0F, 0x3C0A),
        (0x0A, 0x0A, 0x280A),
        (0x0A, 0x14, 0x500A),
        (0x0A, 0x03, 0x0C0A),
        (0x0A, 0x01, 0x040A),
        (0x0A, 0x15, 0x540A),
        (0x0A, 0x0E, 0x380A),
        (0x0A, 0x08, 0x200A),
        (0x41, 0x01, 0x0441),
        (0x1D, 0x02, 0x081D),
        (0x1D, 0x01, 0x041D),
        (0x5A, 0x01, 0x045A),
        (0x28, 0x01, 0x0428),
        (0x5F, 0x02, 0x085F),
        (0x49, 0x01, 0x0449),
        (0x49, 0x02, 0x0849),
        (0x44, 0x01, 0x0444),
        (0x4A, 0x01, 0x044A),
        (0x1E, 0x01, 0x041E),
        (0x51, 0x01, 0x0451),
        (0x73, 0x02, 0x0873),
        (0x73, 0x01, 0x0473),
        (0x1F, 0x01, 0x041F),
        (0x42, 0x01, 0x0442),
        (0x22, 0x01, 0x0422),
        (0x2E, 0x01, 0x042E),
        (0x20, 0x02, 0x0820),
        (0x20, 0x01, 0x0420),
        (0x80, 0x01, 0x0480),
        (0x43, 0x02, 0x0843),
        (0x43, 0x01, 0x0443),
        (0x03, 0x02, 0x0803),
        (0x2A, 0x01, 0x042A),
        (0x52, 0x01, 0x0452),
        (0x88, 0x01, 0x0488),
        (0x78, 0x01, 0x0478),
        (0x6A, 0x01, 0x046A),
    ])</code></pre>
</details>
</dd>
<dt id="refinery.shell.pesig"><code class="flex name class">
<span>class <span class="ident">pesig</span></span>
</code></dt>
<dd>
<section class="desc"><p>Extracts the contents of the IMAGE_DIRECTORY_ENTRY_SECURITY entry of a PE file,
i.e. the digital signatures in DER format.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/formats/pe/pesig.py#L9-L37" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class pesig(Unit):
    &#34;&#34;&#34;
    Extracts the contents of the IMAGE_DIRECTORY_ENTRY_SECURITY entry of a PE file,
    i.e. the digital signatures in DER format.
    &#34;&#34;&#34;

    _SECDIRID = DIRECTORY_ENTRY[&#39;IMAGE_DIRECTORY_ENTRY_SECURITY&#39;]

    def __init__(self): pass

    def process(self, data: bytearray) -&gt; bytearray:
        pe = PE(data=data, fast_load=True)
        pe.parse_data_directories(directories=[self._SECDIRID])
        security = pe.OPTIONAL_HEADER.DATA_DIRECTORY[self._SECDIRID]
        self.log_info(F&#39;signature offset: 0x{security.VirtualAddress:08X}&#39;)
        self.log_info(F&#39;signature length: 0x{security.Size:08X}&#39;)
        if security.VirtualAddress == 0 or security.Size == 0:
            raise ValueError(F&#39;IMAGE_DIRECTORY_ENTRY_SECURITY ({self._SECDIRID}) is corrupt.&#39;)
        sgnoff = security.VirtualAddress + 8
        sgnend = sgnoff + security.Size
        length, revision, certtype = unpack(&#39;&lt;IHH&#39;, data[sgnoff - 8:sgnoff])
        signature = data[sgnoff:sgnend]

        if len(signature) + 8 != length:
            raise RefineryPartialResult(
                F&#39;Found {len(signature) + 8} bytes of signature, but length should be {length}.&#39;,
                partial=signature)

        return signature</code></pre>
</details>
</dd>
<dt id="refinery.shell.pestrip"><code class="flex name class">
<span>class <span class="ident">pestrip</span></span>
<span>(</span><span>certificate=False, directories=False, memdump=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Removes the overlay of a PE file and returns the main executable. Use <code><a title="refinery.peoverlay" href="index.html#refinery.peoverlay">peoverlay</a></code> to
extract the overlay.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/formats/pe/pestrip.py#L6-L19" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class pestrip(OverlayUnit):
    &#34;&#34;&#34;
    Removes the overlay of a PE file and returns the main executable. Use `refinery.peoverlay` to
    extract the overlay.
    &#34;&#34;&#34;

    def process(self, data: bytearray) -&gt; bytearray:
        size = self._get_size(data)
        try:
            data[size:] = []
        except Exception:
            data = data[:size]
        else:
            return data</code></pre>
</details>
</dd>
<dt id="refinery.shell.pick"><code class="flex name class">
<span>class <span class="ident">pick</span></span>
<span>(</span><span>*bounds)</span>
</code></dt>
<dd>
<section class="desc"><p>Picks sequences from the array of multiple inputs. For example, <code>pick 0 2:</code>
will return all but the second ingested input (which has index <code>1</code>).</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/meta/pick.py#L43-L115" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class pick(Unit):
    &#34;&#34;&#34;
    Picks sequences from the array of multiple inputs. For example, `pick 0 2:`
    will return all but the second ingested input (which has index `1`).
    &#34;&#34;&#34;
    def __init__(self, *bounds: Arg.Bounds(nargs=&#39;*&#39;, default=[0])):
        super().__init__(bounds=[sliceobj(s) for s in bounds])

    def process(self, data: Chunk):
        if not data.visible:
            yield data
            return

        state: _PickState = data.temp
        a = state.accessor
        lower = a.start
        upper = a.stop

        if lower is not None:
            lower -= state.discarded
        if upper is not None:
            upper -= state.discarded
        if state.consumed:
            yield from state.remaining[slice(lower, upper, a.step)]
            return

        while lower:
            try:
                chunk = next(state.chunks)
            except StopIteration:
                upper = None
                break
            if chunk.visible:
                lower -= 1
                upper -= 1
                state.discarded += 1
            else:
                yield chunk
        if upper is None:
            yield from state.chunks
            return
        while upper:
            try:
                chunk = next(state.chunks)
            except StopIteration:
                break
            if chunk.visible:
                upper -= 1
                state.discarded += 1
            yield chunk

    def filter(self, chunks: Iterable[Chunk]):
        chunks = begin(chunks)
        if chunks is None:
            return
        container, chunks = chunks
        if container.scope &lt; 1:
            raise RuntimeError(F&#39;{self.__class__.__name__} cannot be used outside a frame; maybe you meant to use snip?&#39;)
        container = container.copy()
        container.visible = True
        state = _PickState(deque(self.args.bounds), chunks)
        while state.next():
            if not state.consumed:
                if not state.discardable():
                    self.log_debug(F&#39;consumed input into buffer after {state.discarded} skips&#39;)
                    for chunk in state.chunks:
                        if not chunk.visible:
                            yield chunk
                            continue
                        state.remaining.append(chunk)
                    state.consumed = True
            container.temp = state
            yield container</code></pre>
</details>
</dd>
<dt id="refinery.shell.pkcs7"><code class="flex name class">
<span>class <span class="ident">pkcs7</span></span>
</code></dt>
<dd>
<section class="desc"><p>Converts PKCS7 encoded data to a JSON representation.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/formats/pkcs7.py#L12-L147" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class pkcs7(Unit):
    &#34;&#34;&#34;
    Converts PKCS7 encoded data to a JSON representation.
    &#34;&#34;&#34;
    @Unit.Requires(&#39;asn1crypto&#39;, &#39;default&#39;, &#39;extended&#39;)
    def _asn1crypto():
        import asn1crypto
        import asn1crypto.cms
        import asn1crypto.core
        import asn1crypto.x509
        return asn1crypto

    def process(self, data: bytes):
        asn1 = self._asn1crypto.core
        cms = self._asn1crypto.cms
        signature = cms.ContentInfo.load(data)

        def unsign(data):
            if isinstance(data, int):
                size = data.bit_length()
                if data &lt; 0:
                    data = (1 &lt;&lt; (size + 1)) - ~data - 1
                if data &gt; 0xFFFFFFFF_FFFFFFFF:
                    size, r = divmod(size, 8)
                    size += bool(r)
                    data = data.to_bytes(size, &#39;big&#39;).hex()
                return data
            elif isinstance(data, dict):
                return {key: unsign(value) for key, value in data.items()}
            elif isinstance(data, list):
                return [unsign(x) for x in data]
            else:
                return data

        class SpcString(asn1.Choice):
            _alternatives = [
                (&#39;unicode&#39;, asn1.BMPString, {&#39;implicit&#39;: 0}),
                (&#39;ascii&#39;, asn1.IA5String, {&#39;implicit&#39;: 1})
            ]

        SpcUuid = asn1.OctetString

        class SpcSerializedObject(asn1.Sequence):
            _fields = [
                (&#39;classId&#39;, SpcUuid),
                (&#39;serializedData&#39;, asn1.OctetString),
            ]

        class SpcLink(asn1.Choice):
            _alternatives = [
                (&#39;url&#39;, asn1.IA5String, {&#39;implicit&#39;: 0}),
                (&#39;monikier&#39;, SpcSerializedObject, {&#39;implicit&#39;: 1}),
                (&#39;file&#39;, SpcString, {&#39;explicit&#39;: 2})
            ]

        class SpcSpOpusInfo(asn1.Sequence):
            _fields = [
                (&#39;programName&#39;, SpcString, {&#39;optional&#39;: True, &#39;explicit&#39;: 0}),
                (&#39;moreInfo&#39;, SpcLink, {&#39;optional&#39;: True, &#39;explicit&#39;: 1}),
            ]

        class SetOfInfos(asn1.SetOf):
            _child_spec = SpcSpOpusInfo

        cms.CMSAttributeType._map[&#39;1.3.6.1.4.1.311.2.1.12&#39;] = &#39;authenticode_info&#39;
        cms.CMSAttribute._oid_specs[&#39;authenticode_info&#39;] = SetOfInfos

        class ParsedASN1ToJSON(BytesAsStringEncoder):
            unit = self

            @classmethod
            def _is_keyval(cls, obj):
                return (
                    isinstance(obj, dict)
                    and set(obj.keys()) == {&#39;type&#39;, &#39;values&#39;}
                    and len(obj[&#39;values&#39;]) == 1
                )

            @classmethod
            def handled(cls, obj) -&gt; bool:
                return BytesAsStringEncoder.handled(obj) or cls._is_keyval(obj)

            def encode_bytes(self, obj: bytes):
                with suppress(Exception):
                    string = obj.decode(&#39;latin1&#39;)
                    if string.isprintable():
                        return string
                return super().encode_bytes(obj)

            def default(self, obj):
                if self._is_keyval(obj):
                    return dict(type=obj[&#39;type&#39;], value=obj[&#39;values&#39;][0])
                with suppress(TypeError):
                    return super().default(obj)
                if isinstance(obj, (set, tuple)):
                    return list(obj)
                if isinstance(obj, datetime):
                    return str(obj)
                dict_result = {}
                list_result = None
                if isinstance(obj, self.unit._asn1crypto.x509.Certificate):
                    dict_result.update(fingerprint=obj.sha1.hex())
                if isinstance(obj, asn1.BitString):
                    return {&#39;bit_string&#39;: obj.native}
                with suppress(Exception):
                    list_result = list(obj)
                    if all(isinstance(k, str) for k in list_result):
                        dict_result.update((key, obj[key]) for key in list_result)
                if dict_result:
                    return dict_result
                if list_result is not None:
                    return list_result
                if isinstance(obj, self.unit._asn1crypto.cms.CertificateChoices):
                    return obj.chosen
                if isinstance(obj, asn1.Sequence):
                    children = obj.children
                    if children:
                        return children
                    return obj.dump()
                with suppress(Exception):
                    return obj.native
                if isinstance(obj, asn1.Any):
                    parsed = None
                    with suppress(Exception):
                        parsed = obj.parse()
                    if parsed:
                        return parsed
                    return obj.dump()
                if isinstance(obj, asn1.Asn1Value):
                    return obj.dump()
                raise ValueError(F&#39;Unable to determine JSON encoding of {obj.__class__.__name__} object.&#39;)

        with ParsedASN1ToJSON as encoder:
            encoded = encoder.dumps(signature)
            converted = unsign(json.loads(encoded))
            return json.dumps(converted, indent=4).encode(self.codec)</code></pre>
</details>
</dd>
<dt id="refinery.shell.pkcs7sig"><code class="flex name class">
<span>class <span class="ident">pkcs7sig</span></span>
<span>(</span><span>tabular=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Converts PKCS7 encoded signatures into a human-readable JSON representation. This can be used
to parse authenticode signatures appended to files that are not PE files to get the same output
that is produced by the pemeta unit.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/formats/pkcs7sig.py#L8-L19" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class pkcs7sig(Unit):
    &#34;&#34;&#34;
    Converts PKCS7 encoded signatures into a human-readable JSON representation. This can be used
    to parse authenticode signatures appended to files that are not PE files to get the same output
    that is produced by the pemeta unit.
    &#34;&#34;&#34;
    def __init__(self, tabular: Arg(&#39;-t&#39;, help=&#39;Print information in a table rather than as JSON&#39;) = False):
        super().__init__(tabular=tabular)

    def process(self, data: bytes):
        json = pemeta.parse_signature(data)
        yield from ppjson(tabular=self.args.tabular)._pretty_output(json, indent=4, ensure_ascii=False)</code></pre>
</details>
</dd>
<dt id="refinery.shell.pop"><code class="flex name class">
<span>class <span class="ident">pop</span></span>
<span>(</span><span>*names)</span>
</code></dt>
<dd>
<section class="desc"><p>In processing order, remove visible chunks from the current frame and store their contents in the given
meta variables. All chunks in the input stream are consequently made visible again. If pop is used at
the end of a frame, then variables will be local to the parent frame.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/meta/pop.py#L61-L128" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class pop(Unit):
    &#34;&#34;&#34;
    In processing order, remove visible chunks from the current frame and store their contents in the given
    meta variables. All chunks in the input stream are consequently made visible again. If pop is used at
    the end of a frame, then variables will be local to the parent frame.
    &#34;&#34;&#34;
    def __init__(
        self,
        *names: Arg(type=str, metavar=F&#39;[name[:conversion]|count|{_popcount._MERGE_SYMBOL}]&#39;, help=(
            R&#39;Specify either the name of a single variable to receive the contents of an input chunk, or &#39;
            R&#39;an integer expression that specifies a number of values to be removed from the input without &#39;
            F&#39;storing them. Additionally, it is possible to specify the symbol &#34;{_popcount._MERGE_SYMBOL}&#34; &#39;
            R&#39;to remove a single chunk from the input and merge its meta data into the following ones. By &#39;
            R&#39;default, a single merge is performed. When a variable name is specified, a sequence of &#39;
            R&#39;transformations can be appended to be applied before storing it. For example, the argument &#39;
            R&#39;k:le:b64 would first decode the chunk using base64, then convert it to an integer in little &#39;
            R&#39;endian format, and store the integer result in the variable `k`. The visual aid is that the &#39;
            R&#39;content is passed from right to left through all conversions, into the variable `k`.&#39;
        ))
    ):
        if not names:
            names = _popcount._MERGE_SYMBOL,
        super().__init__(names=[_popcount(n) for n in names])

    def process(self, data):
        return data

    def filter(self, chunks: Iterable[Chunk]):
        invisible = []
        variables = {}
        remaining: Iterator[_popcount] = iter(self.args.names)

        it = iter(chunks)
        pop = next(remaining).reset()
        done = False

        for chunk in it:
            if not chunk.visible:
                self.log_debug(&#39;buffering invisible chunk&#39;)
                invisible.append(chunk)
                continue
            try:
                while not pop.into(variables, chunk):
                    pop = next(remaining).reset()
            except StopIteration:
                done = True
                invisible.append(chunk)
                break

        if not done and pop.done:
            try:
                next(remaining)
            except StopIteration:
                done = True

        if not done:
            raise ValueError(&#39;Not all variables could be assigned.&#39;)

        nesting = self.args.nesting

        for chunk in chain(invisible, it):
            meta = chunk.meta
            meta.update(variables)
            if nesting &lt; 0:
                for name in variables:
                    meta.set_scope(name, chunk.scope + nesting)
            chunk.visible = True
            yield chunk</code></pre>
</details>
</dd>
<dt id="refinery.shell.ppjscript"><code class="flex name class">
<span>class <span class="ident">ppjscript</span></span>
<span>(</span><span>indent=4)</span>
</code></dt>
<dd>
<section class="desc"><p>Pretty-prints JavaScript without any reflection or evaluation.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/sinks/ppjscript.py#L7-L27" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class ppjscript(Unit):
    &#34;&#34;&#34;
    Pretty-prints JavaScript without any reflection or evaluation.
    &#34;&#34;&#34;
    def __init__(self, indent: Arg.Number(&#39;-i&#39;, help=(
        &#39;Controls the amount of space characters used for indentation in the output. Default is 4.&#39;)) = 4
    ):
        return super().__init__(indent=indent)

    @Unit.Requires(&#39;jsbeautifier&#39;, &#39;display&#39;, &#39;extended&#39;)
    def _jsb():
        import jsbeautifier
        import jsbeautifier.unpackers.javascriptobfuscator
        # TODO: This is a workaround for the following bug:
        # https://github.com/beautify-web/js-beautify/issues/1350
        jsbeautifier.unpackers.javascriptobfuscator.detect = lambda *_: False
        return jsbeautifier

    @unicoded
    def process(self, data: str) -&gt; str:
        return self._jsb.beautify(data, dict(eval_code=False, indent_size=self.args.indent))</code></pre>
</details>
</dd>
<dt id="refinery.shell.ppjson"><code class="flex name class">
<span>class <span class="ident">ppjson</span></span>
<span>(</span><span>tabular=False, indent=4)</span>
</code></dt>
<dd>
<section class="desc"><p>Expects JSON input data and outputs it in a neatly formatted manner.
If the indentation is set to zero, the output is minified.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/sinks/ppjson.py#L12-L54" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class ppjson(Unit):
    &#34;&#34;&#34;
    Expects JSON input data and outputs it in a neatly formatted manner.
    If the indentation is set to zero, the output is minified.
    &#34;&#34;&#34;
    _TRAILING_COMMA = re.compile(BR&#39;,\s*(}|])&#39;)

    def __init__(
        self,
        tabular: Arg.Switch(&#39;-t&#39;, group=&#39;OUT&#39;, help=&#39;Convert JSON input into a flattened table.&#39;) = False,
        indent : Arg.Number(&#39;-i&#39;, group=&#39;OUT&#39;, help=&#39;Number of spaces used for indentation. Default is {default}.&#39;) = 4
    ):
        return super().__init__(indent=indent, tabular=tabular)

    def _pretty_output(self, parsed, **kwargs):
        if self.args.tabular:
            table = list(flattened(parsed))
            width = max(len(key) for key, _ in table)
            tsize = get_terminal_size(80) - width - 4
            for key, value in table:
                value = str(value).rstrip()
                value = textwrap.wrap(value, tsize)
                it = iter(value)
                try:
                    item = next(it)
                except StopIteration:
                    continue
                yield F&#39;{key:&lt;{width}} : {item}&#39;.encode(self.codec)
                for wrap in it:
                    yield F&#39;{&#34;&#34;:&lt;{width + 3}}{wrap}&#39;.encode(self.codec)
        else:
            yield json.dumps(parsed, **kwargs).encode(self.codec)

    def process(self, data):
        if self._TRAILING_COMMA.search(data):
            def smartfix(match):
                k = match.start()
                return match.group(0 if any(k in s for s in strings) else 1)
            from refinery.lib.patterns import formats
            strings = {range(*m.span()) for m in formats.string.finditer(data)}
            data = self._TRAILING_COMMA.sub(smartfix, data)
        kwargs = {&#39;indent&#39;: self.args.indent} if self.args.indent else {&#39;separators&#39;: (&#39;,&#39;, &#39;:&#39;)}
        yield from self._pretty_output(json.loads(data), **kwargs)</code></pre>
</details>
</dd>
<dt id="refinery.shell.ppxml"><code class="flex name class">
<span>class <span class="ident">ppxml</span></span>
<span>(</span><span>indent=4, header=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Expects XML input data and outputs it in a neatly formatted manner.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/sinks/ppxml.py#L9-L76" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class ppxml(Unit):
    &#34;&#34;&#34;
    Expects XML input data and outputs it in a neatly formatted manner.
    &#34;&#34;&#34;

    def __init__(self,
        indent: Arg.Number(&#39;-i&#39;, help=(
            &#39;Controls the amount of space characters used for indentation in the output. Default is 4.&#39;)) = 4,
        header: Arg.Switch(&#39;-x&#39;, help=&#39;Add an XML header to the formatted output.&#39;) = False
    ):
        super().__init__(indent=indent, header=header)

    def process(self, data):

        pad = self.args.indent * &#39; &#39;
        etm = {}

        try:
            dom = ForgivingParse(data, etm)
        except Exception:
            from refinery.lib.meta import metavars
            msg = &#39;error parsing as XML, returning original content&#39;
            path = metavars(data).get(&#39;path&#39;)
            if path:
                msg = F&#39;{msg}: {path}&#39;
            self.log_warn(msg)
            return data

        def indent(element, level=0, more_sibs=False):
            &#34;&#34;&#34;
            The credit for this one goes to:
            https://stackoverflow.com/a/12940014
            &#34;&#34;&#34;
            indentation = &#39;\n&#39;
            if level:
                indentation += (level - 1) * pad
            childcount = len(element)
            if childcount:
                if not element.text or not element.text.strip():
                    element.text = indentation + pad
                    if level:
                        element.text += pad
                for count, child in enumerate(element):
                    indent(child, level + 1, count &lt; childcount - 1)
                if level and (not element.tail or element.tail.isspace()):
                    element.tail = indentation
                    if more_sibs:
                        element.tail += pad
            elif level and (not element.tail or element.tail.isspace()):
                element.tail = indentation
                if more_sibs: element.tail += pad

        indent(dom.getroot())

        with io.BytesIO() as output:
            dom.write(output, encoding=self.codec, xml_declaration=self.args.header)
            result = output.getvalue()

        for uid, key in etm.items():
            entity = F&#39;&amp;{key};&#39;.encode(self.codec)
            needle = uid.encode(self.codec)
            result = result.replace(needle, entity)

        return result

    @classmethod
    def handles(cls, data):
        return is_xml(data)</code></pre>
</details>
</dd>
<dt id="refinery.shell.ps1str"><code class="flex name class">
<span>class <span class="ident">ps1str</span></span>
</code></dt>
<dd>
<section class="desc"><p>Escapes and unescapes PowerShell strings.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/encoding/ps1str.py#L9-L72" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class ps1str(Unit):
    &#34;&#34;&#34;
    Escapes and unescapes PowerShell strings.
    &#34;&#34;&#34;
    UNESCAPE = {
        &#39;`0&#39;: &#39;\0&#39;,
        &#39;`a&#39;: &#39;\a&#39;,
        &#39;`b&#39;: &#39;\b&#39;,
        &#39;`f&#39;: &#39;\f&#39;,
        &#39;`n&#39;: &#39;\n&#39;,
        &#39;`r&#39;: &#39;\r&#39;,
        &#39;`t&#39;: &#39;\t&#39;,
        &#39;`v&#39;: &#39;\v&#39;,
        &#39;``&#39;: &#39;`&#39;,
        &#34;`&#39;&#34;: &#39;\&#39;&#39;,
        &#39;`&#34;&#39;: &#39;\&#34;&#39;,
    }
    ESCAPE = {
        &#39;`&#39; : &#39;``&#39;,
        &#39;$&#39; : &#39;`$&#39;,
        &#39;\0&#39;: &#39;`0&#39;,
        &#39;\a&#39;: &#39;`a&#39;,
        &#39;\b&#39;: &#39;`b&#39;,
        &#39;\f&#39;: &#39;`f&#39;,
        &#39;\n&#39;: &#39;`n&#39;,
        &#39;\r&#39;: &#39;`r&#39;,
        &#39;\t&#39;: &#39;`t&#39;,
        &#39;\v&#39;: &#39;`v&#39;,
        &#39;\&#39;&#39;: &#34;`&#39;&#34;,
        &#39;\&#34;&#39;: &#39;&#34;&#34;&#39;,
    }

    def __init__(self): pass

    @unicoded
    def process(self, data):
        match = re.fullmatch(R&#39;&#39;&#39;@([&#39;&#34;])\s*?[\r\n](.*?)[\r\n]\1@&#39;&#39;&#39;, data, flags=re.DOTALL)
        if match:
            return match.group(2)
        if data[0] not in &#39;\&#39;\&#34;&#39; or data[-1] != data[0]:
            raise ValueError(
                &#39;No quotes found at beginning of input. To escape a PowerShell string, the &#39;
                &#39;quotes must be included because quote escaping depends on whether a single &#39;
                &#39;or double quote was used.&#39;)

        quote, data = data[0], data[1:-1]

        def unescape(match):
            string = match[0]
            return self.UNESCAPE.get(string, string[1:])

        if quote == &#39;&#34;&#39;:
            if re.search(R&#39;(?&lt;!`)\$(?=[\w\(\{\$\?\^:])&#39;, data):
                self.log_warn(&#39;Loss of information: double quoted string contains variable substitutions.&#39;)
            data = re.sub(&#39;`.&#39;, unescape, data)

        return data.replace(quote + quote, quote)

    @unicoded
    def reverse(self, data):
        def escaper(match):
            char = match[0]
            return ps1str.ESCAPE.get(char, char)
        return &#39;&#34;{}&#34;&#39;.format(re.sub(R&#39;&#39;&#39;[\x00\x07-\x0D`$&#39;&#34;]&#39;&#39;&#39;, escaper, data))</code></pre>
</details>
</dd>
<dt id="refinery.shell.push"><code class="flex name class">
<span>class <span class="ident">push</span></span>
<span>(</span><span>data=b'')</span>
</code></dt>
<dd>
<section class="desc"><p>The unit inserts an additional chunk before each input chunk and moves the original
data out of scope. This chunk is considered the "original" data, while the one inserted
in front of it is used as an intermediate result. By default, this intermediate data is
a copy of the input data. For example:</p>
<pre><code>emit key=value | push [[| rex =(.*)$ {1} | pop v ]| repl var:v censored ]
</code></pre>
<p>will output <code>key=censored</code>. The application of <code><a title="refinery.rex" href="index.html#refinery.rex">rex</a></code> turns the (duplicated)
data into just the value, which is then stored in the variable <code>v</code>. The application
of <code><a title="refinery.repl" href="index.html#refinery.repl">repl</a></code> replaces this value with the hard-coded string <code>censored</code>.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/meta/push.py#L6-L34" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class push(Unit):
    &#34;&#34;&#34;
    The unit inserts an additional chunk before each input chunk and moves the original
    data out of scope. This chunk is considered the &#34;original&#34; data, while the one inserted
    in front of it is used as an intermediate result. By default, this intermediate data is
    a copy of the input data. For example:

        emit key=value | push [[| rex =(.*)$ {1} | pop v ]| repl var:v censored ]

    will output `key=censored`. The application of `refinery.rex` turns the (duplicated)
    data into just the value, which is then stored in the variable `v`. The application
    of `refinery.repl` replaces this value with the hard-coded string `censored`.
    &#34;&#34;&#34;
    def __init__(self, data: Arg(help=&#39;The data to be pushed, by default a copy of the input.&#39;) = B&#39;&#39;):
        super().__init__(data=data)

    def process(self, data: Chunk):
        src = self.args.data
        tos = data.copy(meta=True, data=False)
        tos[:] = src or data
        if self.args.nesting &gt; 0:
            data.set_next_scope(False)
        else:
            try:
                data.visible = False
            except AttributeError:
                self.log_warn(&#39;application has no effect outside frame.&#39;)
        yield data
        yield tos</code></pre>
</details>
</dd>
<dt id="refinery.shell.put"><code class="flex name class">
<span>class <span class="ident">put</span></span>
<span>(</span><span>name, value=&lt;object object&gt;)</span>
</code></dt>
<dd>
<section class="desc"><p>Can be used to add a meta variable to the processed chunk. Note that meta variables
cease to exist outside a frame.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/meta/put.py#L14-L44" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class put(Unit):
    &#34;&#34;&#34;
    Can be used to add a meta variable to the processed chunk. Note that meta variables
    cease to exist outside a frame.
    &#34;&#34;&#34;
    def __init__(
        self,
        name : Arg(help=&#39;The name of the variable to be used.&#39;, type=str),
        value: Arg(help=&#39;The value for the variable. If no value is given, the entire current chunk is stored.&#39;,
            type=functools.partial(numseq, typecheck=False)) = _EMPTY
    ):
        super().__init__(name=check_variable_name(name), value=value)

    def process(self, data: Chunk):
        value = self.args.value
        if value is _EMPTY:
            value = data
        if not isinstance(value, (int, float)) and not isbuffer(value):
            try:
                len(value)
            except TypeError:
                if isinstance(value, itertools.repeat):
                    value = next(value)
                if not isinstance(value, (int, float)):
                    raise NotImplementedError(F&#39;put does not support {value.__class__.__name__} values.&#39;)
            else:
                if not isinstance(value, (dict, list)):
                    value = list(value)
        self.log_debug(F&#39;storing {typename(value)}:&#39;, value, clip=True)
        data.meta[self.args.name] = value
        return data</code></pre>
</details>
</dd>
<dt id="refinery.shell.pyc"><code class="flex name class">
<span>class <span class="ident">pyc</span></span>
<span>(</span><span>*paths, list=False, join_path=False, drop_path=False, fuzzy=0, exact=False, regex=False, path=b'path', date=b'date', pwd=b'')</span>
</code></dt>
<dd>
<section class="desc"><p>Decompiles Python bytecode (PYC) files back to source code. A known limitation is that it does
not work on recent Python versions, but anything below 3.9 should work.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/formats/pyc.py#L12-L24" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class pyc(ArchiveUnit):
    &#34;&#34;&#34;
    Decompiles Python bytecode (PYC) files back to source code. A known limitation is that it does
    not work on recent Python versions, but anything below 3.9 should work.
    &#34;&#34;&#34;

    def unpack(self, data):
        input_path = metavars(data).get(self.args.path.decode(self.codec))
        for k, code in enumerate(extract_code_from_buffer(bytes(data), input_path)):
            path = code.container.co_filename or F&#39;__unknown_name_{k:02d}.py&#39;
            date = datetime.fromtimestamp(code.timestamp)
            data = decompile_buffer(code)
            yield self._pack(path, date, data)</code></pre>
</details>
</dd>
<dt id="refinery.shell.pym"><code class="flex name class">
<span>class <span class="ident">pym</span></span>
</code></dt>
<dd>
<section class="desc"><p>Converts Python-Marshaled code objects to the PYC (Python Bytecode) format. If it is an
older Python version, you can use the <code><a title="refinery.pyc" href="index.html#refinery.pyc">pyc</a></code> unit to then decompile the code, but
for more recent versions a separate Python decompiler will be required.</p>
<p>WARNING: This unit will invoke the <code>marshal.loads</code> function, which may be unsafe. Please
refer to the official Python documentation for more details.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/formats/pym.py#L9-L53" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class pym(Unit):
    &#34;&#34;&#34;
    Converts Python-Marshaled code objects to the PYC (Python Bytecode) format. If it is an
    older Python version, you can use the `refinery.pyc` unit to then decompile the code, but
    for more recent versions a separate Python decompiler will be required.

    WARNING: This unit will invoke the `marshal.loads` function, which may be unsafe. Please
    refer to the official Python documentation for more details.
    &#34;&#34;&#34;

    def reverse(self, data):
        return marshal.dumps(data)

    def process(self, data):
        data = marshal.loads(data)
        code = (lambda: 0).__code__.__class__

        def toblob(data):
            if isinstance(data, (bytes, bytearray)):
                self.log_info(U&#39;unmarshalled a byte string, returning as is&#39;)
                return data
            if isinstance(data, str):
                self.log_info(F&#39;unmarshalled a string object, encoding as {self.codec}&#39;)
                return data.encode(self.codec)
            if isinstance(data, code):
                self.log_info(U&#39;unmarshalled a code object, converting to pyc&#39;)
                import importlib
                return importlib._bootstrap_external._code_to_timestamp_pyc(data)
            if isinstance(data, int):
                self.log_info(U&#39;unmarshalled an integer, returning big endian encoding&#39;)
                q, r = divmod(data.bit_length(), 8)
                q += int(bool(r))
                return data.to_bytes(q, &#39;big&#39;)
            if isinstance(data, dict):
                with BytesAsStringEncoder as encoder:
                    return encoder.dumps(data).encode(self.codec)
            raise NotImplementedError(
                F&#39;No serialization implemented for object of type {data.__class__.__name__}&#39;)

        if isinstance(data, list):
            self.log_info(&#39;object is a list, converting each item individually&#39;)
            for item in data:
                yield toblob(item)
        else:
            yield toblob(data)</code></pre>
</details>
</dd>
<dt id="refinery.shell.qb"><code class="flex name class">
<span>class <span class="ident">qb</span></span>
<span>(</span><span>*data)</span>
</code></dt>
<dd>
<section class="desc"><p>Short for "queue back": Insert new chunks at the end of the current frame.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/meta/queue.py#L57-L62" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class qb(QueueUnit):
    &#34;&#34;&#34;
    Short for &#34;queue back&#34;: Insert new chunks at the end of the current frame.
    &#34;&#34;&#34;
    def filter(self, chunks: Iterable[Chunk]):
        yield from self._queue(chunks, False)</code></pre>
</details>
</dd>
<dt id="refinery.shell.qf"><code class="flex name class">
<span>class <span class="ident">qf</span></span>
<span>(</span><span>*data)</span>
</code></dt>
<dd>
<section class="desc"><p>Short for "queue front": Insert new chunks at the beginning of the current frame.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/meta/queue.py#L49-L54" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class qf(QueueUnit):
    &#34;&#34;&#34;
    Short for &#34;queue front&#34;: Insert new chunks at the beginning of the current frame.
    &#34;&#34;&#34;
    def filter(self, chunks: Iterable[Chunk]):
        yield from self._queue(chunks, True)</code></pre>
</details>
</dd>
<dt id="refinery.shell.qlz"><code class="flex name class">
<span>class <span class="ident">qlz</span></span>
</code></dt>
<dd>
<section class="desc"><p>This unit implements QuickLZ decompression levels 1 and 3.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/compression/qlz.py#L13-L137" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class qlz(Unit):
    &#34;&#34;&#34;
    This unit implements QuickLZ decompression levels 1 and 3.
    &#34;&#34;&#34;

    def process(self, data):
        source = memoryview(data)
        head = source[0]
        clvl = (head &gt;&gt; 2) &amp; 0x3

        if head &amp; 2:
            self.log_info(&#39;long header detected&#39;)
            size = int.from_bytes(source[5:9], &#39;little&#39;)
            source = source[9:]
        else:
            self.log_info(&#39;short header detected&#39;)
            size = source[3]
            source = source[3:]
        if head &amp; 1 != 1:
            self.log_warn(&#39;header indicates that data is uncompressed, returning remaining data&#39;)
            return source
        else:
            self.log_info(F&#39;compression level {clvl}, decompressed size {SizeInt(size)!r}&#39;)

        def fetchhash():
            return int.from_bytes(destination[hashvalue + 1:hashvalue + 4], byteorder=&#39;little&#39;)

        codeword = 1
        destination = bytearray()
        hashtable = [0] * _HASH_VALUES
        hashvalue = -1
        last_matchstart = size - _UNCONDITIONAL_MATCHLEN - _UNCOMPRESSED_END - 1
        fetch = 0

        if clvl == 2:
            raise ValueError(&#34;This version only supports level 1 and 3&#34;)
        while source:
            if codeword == 1:
                codeword = int.from_bytes(source[:4], byteorder=&#39;little&#39;)
                source = source[4:]
                if len(destination) &lt;= last_matchstart:
                    c = 3 if clvl == 1 else 4
                    fetch = int.from_bytes(source[:c], byteorder=&#39;little&#39;)
            if codeword &amp; 1:
                codeword = codeword &gt;&gt; 1
                if clvl == 1:
                    hash = (fetch &gt;&gt; 4) &amp; 0xFFF
                    offset = hashtable[hash]
                    if fetch &amp; 0xF:
                        matchlen = (fetch &amp; 0xF) + 2
                        source = source[2:]
                    else:
                        matchlen = source[2]
                        source = source[3:]
                else:
                    if (fetch &amp; 3) == 0:
                        delta = (fetch &amp; 0xFF) &gt;&gt; 2
                        matchlen = 3
                        source = source[1:]
                    elif (fetch &amp; 2) == 0:
                        delta = (fetch &amp; 0xFFFF) &gt;&gt; 2
                        matchlen = 3
                        source = source[2:]
                    elif (fetch &amp; 1) == 0:
                        delta = (fetch &amp; 0xFFFF) &gt;&gt; 6
                        matchlen = ((fetch &gt;&gt; 2) &amp; 15) + 3
                        source = source[2:]
                    elif (fetch &amp; 127) != 3:
                        delta = (fetch &gt;&gt; 7) &amp; 0x1FFFF
                        matchlen = ((fetch &gt;&gt; 2) &amp; 0x1F) + 2
                        source = source[3:]
                    else:
                        delta = fetch &gt;&gt; 15
                        matchlen = ((fetch &gt;&gt; 7) &amp; 255) + 3
                        source = source[4:]
                    offset = (len(destination) - delta) &amp; 0xFFFFFFFF

                for i in range(offset, offset + matchlen):
                    destination.append(destination[i])

                if clvl == 1:
                    fetch = fetchhash()
                    while hashvalue &lt; len(destination) - matchlen:
                        hashvalue += 1
                        hash = ((fetch &gt;&gt; 12) ^ fetch) &amp; _HASH_MASK
                        hashtable[hash] = hashvalue
                        fetch = fetch &gt;&gt; 8 &amp; 0xFFFF
                        try:
                            fetch |= destination[hashvalue + 3] &lt;&lt; 16
                        except IndexError:
                            pass
                    fetch = int.from_bytes(source[:3], byteorder=&#39;little&#39;)
                else:
                    fetch = int.from_bytes(source[:4], byteorder=&#39;little&#39;)
                hashvalue = len(destination) - 1
            else:
                if len(destination) &lt;= last_matchstart:
                    destination.append(source[0])
                    source = source[1:]
                    codeword = codeword &gt;&gt; 1
                    if clvl == 1:
                        while hashvalue &lt; len(destination) - 3:
                            fetch2 = fetchhash()
                            hashvalue += 1
                            hash = ((fetch2 &gt;&gt; 12) ^ fetch2) &amp; _HASH_MASK
                            hashtable[hash] = hashvalue
                        fetch = fetch &gt;&gt; 8 &amp; 0xFFFF | source[2] &lt;&lt; 16
                    else:
                        fetch = fetch &gt;&gt; 8 &amp; 0xFFFF
                        fetch |= source[2] &lt;&lt; 16
                        fetch |= source[3] &lt;&lt; 24
                else:
                    while len(destination) &lt;= size - 1:
                        if codeword == 1:
                            source = source[4:]
                            codeword = 0x80000000
                        destination.append(source[0])
                        source = source[1:]
                        codeword = codeword &gt;&gt; 1
                    break
        if len(destination) != size:
            raise RefineryPartialResult(
                F&#39;Header indicates decompressed size 0x{size:X}, but 0x{len(destination):X} bytes &#39;
                F&#39;were decompressed.&#39;, destination)
        return destination</code></pre>
</details>
</dd>
<dt id="refinery.shell.rabbit"><code class="flex name class">
<span>class <span class="ident">rabbit</span></span>
<span>(</span><span>key, discard=0, stateful=False, iv=b'')</span>
</code></dt>
<dd>
<section class="desc"><p>RABBIT encryption and decryption.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/crypto/cipher/rabbit.py#L88-L100" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class rabbit(StreamCipherUnit):
    &#34;&#34;&#34;
    RABBIT encryption and decryption.
    &#34;&#34;&#34;
    key_size = {16}

    def __init__(self, key, discard=0, stateful=False, iv: Arg(&#39;-i&#39;, &#39;--iv&#39;, help=&#39;Optional initialization vector.&#39;) = B&#39;&#39;):
        super().__init__(key=key, iv=iv, stateful=stateful, discard=discard)

    def keystream(self) -&gt; Iterable[int]:
        if len(self.args.iv) not in (0, 8):
            raise ValueError(&#39;The IV length must be exactly 8 bytes.&#39;)
        return RabbitCipher(self.args.key, self.args.iv)</code></pre>
</details>
</dd>
<dt id="refinery.shell.rc2"><code class="flex name class">
<span>class <span class="ident">rc2</span></span>
<span>(</span><span>key, iv=b'', *, eks=1024, derive_eks=False, padding=None, mode=None, raw=False, little_endian=False, segment_size=0, mac_len=0, assoc_len=0)</span>
</code></dt>
<dd>
<section class="desc"><p>RC2 encryption and decryption.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/crypto/cipher/rc2.py#L10-L48" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class rc2(StandardBlockCipherUnit, cipher=PyCryptoFactoryWrapper(ARC2)):
    &#34;&#34;&#34;
    RC2 encryption and decryption.
    &#34;&#34;&#34;

    def __init__(
        self, key, iv=b&#39;&#39;, *,
        eks: Arg.Number(&#39;-k&#39;, &#39;--eks&#39;, group=&#39;EKS&#39;,
            help=&#39;Set the effective key size. Default is {default}.&#39;) = 1024,
        derive_eks: Arg.Switch(&#39;-d&#39;, &#39;--dks&#39;, group=&#39;EKS&#39;,
            help=&#39;Act as .NET and derive the effective key size from the key length.&#39;) = False,
        padding=None,
        mode=None,
        raw=False,
        little_endian=False,
        segment_size=0,
        mac_len=0,
        assoc_len=0,
        **keywords
    ):
        super().__init__(
            key,
            iv,
            eks=eks,
            derive_eks=derive_eks,
            padding=padding,
            mode=mode,
            raw=raw,
            little_endian=little_endian,
            segment_size=segment_size,
            mac_len=mac_len,
            assoc_len=assoc_len,
            **keywords
        )

    def _new_cipher(self, **optionals) -&gt; CipherInterface:
        eks = len(self.args.key) * 8 if self.args.derive_eks else self.args.eks
        optionals.update(effective_keylen=eks)
        return super()._new_cipher(**optionals)</code></pre>
</details>
</dd>
<dt id="refinery.shell.rc4"><code class="flex name class">
<span>class <span class="ident">rc4</span></span>
<span>(</span><span>key, discard=0)</span>
</code></dt>
<dd>
<section class="desc"><p>RC4 encryption and decryption.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/crypto/cipher/rc4.py#L11-L22" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class rc4(StandardCipherUnit, cipher=PyCryptoFactoryWrapper(ARC4)):
    &#34;&#34;&#34;
    RC4 encryption and decryption.
    &#34;&#34;&#34;
    def __init__(
        self, key,
        discard: Arg.Number(&#39;-d&#39;, help=&#39;Discard the first {varname} bytes of the keystream, {default} by default.&#39;) = 0,
    ):
        super().__init__(key, discard=discard)

    def _new_cipher(self, **optionals):
        return super()._new_cipher(drop=self.args.discard, **optionals)</code></pre>
</details>
</dd>
<dt id="refinery.shell.rc4mod"><code class="flex name class">
<span>class <span class="ident">rc4mod</span></span>
<span>(</span><span>key, stateful=False, discard=0, *, size=256)</span>
</code></dt>
<dd>
<section class="desc"><p>Implements a modified version of the RC4 stream cipher where the size of the RC4 SBox can be altered.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/crypto/cipher/rc4mod.py#L8-L36" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class rc4mod(StreamCipherUnit):
    &#34;&#34;&#34;
    Implements a modified version of the RC4 stream cipher where the size of the RC4 SBox can be altered.
    &#34;&#34;&#34;

    def __init__(
        self, key, stateful=False, discard=0, *,
        size: Arg.Number(&#39;-t&#39;, help=&#39;Table size, {default} by default.&#39;, bound=(1, None)) = 0x100
    ):
        super().__init__(key=key, stateful=stateful, discard=discard, size=size)

    def keystream(self):
        size = self.args.size
        tablerange = range(max(size, 0x100))
        b, table = 0, bytearray(k &amp; 0xFF for k in tablerange)
        for a, keybyte in zip(tablerange, cycle(self.args.key)):
            t = table[a]
            b = (b + keybyte + t) % size
            table[a] = table[b]
            table[b] = t
        self.log_debug(lambda: F&#39;SBOX = {table.hex(&#34; &#34;).upper()}&#39;, clip=True)
        b, a = 0, 0
        while True:
            a = (a + 1) % size
            t = table[a]
            b = (b + t) % size
            table[a] = table[b]
            table[b] = t
            yield table[(table[a] + t) % size]</code></pre>
</details>
</dd>
<dt id="refinery.shell.rc5"><code class="flex name class">
<span>class <span class="ident">rc5</span></span>
<span>(</span><span>key, iv=b'', *, padding=None, mode=None, raw=False, little_endian=False, segment_size=0, rounds=12, word_size=32, assoc_len=0, mac_len=0)</span>
</code></dt>
<dd>
<section class="desc"><p>RC5 encryption and decryption.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/crypto/cipher/rc5.py#L117-L149" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class rc5(StandardBlockCipherUnit, cipher=BlockCipherFactory(RC5)):
    &#34;&#34;&#34;
    RC5 encryption and decryption.
    &#34;&#34;&#34;
    def __init__(
        self, key, iv=b&#39;&#39;, *, padding=None, mode=None, raw=False, little_endian=False, segment_size=0,
        rounds    : Arg.Number(&#39;-k&#39;, help=&#39;Number of rounds to use, the default is {default}&#39;) = _R,
        word_size : Arg.Number(&#39;-w&#39;, help=&#39;The word size in bits, {default} by default.&#39;) = _W,
        **more
    ):
        super().__init__(
            key,
            iv,
            padding=padding,
            mode=mode,
            raw=raw,
            little_endian=little_endian,
            segment_size=segment_size,
            rounds=rounds,
            word_size=word_size,
            **more
        )

    @property
    def block_size(self):
        return self.args.word_size // 4

    def _new_cipher(self, **optionals) -&gt; CipherInterface:
        return super()._new_cipher(
            rounds=self.args.rounds,
            word_size=self.args.word_size,
            **optionals
        )</code></pre>
</details>
</dd>
<dt id="refinery.shell.rc6"><code class="flex name class">
<span>class <span class="ident">rc6</span></span>
<span>(</span><span>key, iv=b'', *, padding=None, mode=None, raw=False, little_endian=False, segment_size=0, rounds=20, word_size=32)</span>
</code></dt>
<dd>
<section class="desc"><p>RC6 encryption and decryption. The parameter defaults are the RC6 parameters that were chosen
for the AES candidacy. Only key sizes of 128, 192, and 256 bits are used for AES candidates, but
the unit will allow any key size up to 256 bits.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/crypto/cipher/rc6.py#L118-L150" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class rc6(StandardBlockCipherUnit, cipher=BlockCipherFactory(RC6)):
    &#34;&#34;&#34;
    RC6 encryption and decryption. The parameter defaults are the RC6 parameters that were chosen
    for the AES candidacy. Only key sizes of 128, 192, and 256 bits are used for AES candidates, but
    the unit will allow any key size up to 256 bits.
    &#34;&#34;&#34;
    def __init__(
        self, key, iv=b&#39;&#39;, *, padding=None, mode=None, raw=False, little_endian=False, segment_size=0,
        rounds    : Arg.Number(&#39;-k&#39;, help=&#39;Number of rounds to use, the default is {default}&#39;) = _R,
        word_size : Arg.Number(&#39;-w&#39;, help=&#39;The word size in bits, {default} by default.&#39;) = _W,
    ):
        super().__init__(
            key,
            iv,
            padding=padding,
            mode=mode,
            raw=raw,
            little_endian=little_endian,
            segment_size=segment_size,
            rounds=rounds,
            word_size=word_size
        )

    @property
    def block_size(self):
        return self.args.word_size // 2

    def _new_cipher(self, **optionals) -&gt; CipherInterface:
        return super()._new_cipher(
            rounds=self.args.rounds,
            word_size=self.args.word_size,
            **optionals
        )</code></pre>
</details>
</dd>
<dt id="refinery.shell.recode"><code class="flex name class">
<span>class <span class="ident">recode</span></span>
<span>(</span><span>decode=None, encode='UTF8', decerr=None, encerr=None, errors=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Expects input string data encoded in the <code>from</code> encoding and encodes it in
the <code>to</code> encoding, then outputs the result.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/encoding/recode.py#L18-L65" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class recode(Unit):
    &#34;&#34;&#34;
    Expects input string data encoded in the `from` encoding and encodes it in
    the `to` encoding, then outputs the result.
    &#34;&#34;&#34;

    def __init__(
        self,
        decode: Arg(metavar=&#39;decode-as&#39;, type=str, help=&#39;Input encoding; Guess encoding by default.&#39;) = None,
        encode: Arg(metavar=&#39;encode-as&#39;, type=str, help=F&#39;Output encoding; The default is {Unit.codec}.&#39;) = Unit.codec,
        decerr: Arg.Option(&#39;-d&#39;, choices=Handler,
            help=&#39;Specify an error handler for decoding.&#39;) = None,
        encerr: Arg.Option(&#39;-e&#39;, choices=Handler,
            help=&#39;Specify an error handler for encoding.&#39;) = None,
        errors: Arg.Option(&#39;-E&#39;, choices=Handler, help=(
            &#39;Specify an error handler for both encoding and decoding. &#39;
            &#39;The possible choices are the following: {choices}&#39;)) = None,
    ):
        super().__init__(
            decode=decode,
            encode=encode,
            decerr=Arg.AsOption(decerr or errors or &#39;STRICT&#39;, Handler).value,
            encerr=Arg.AsOption(encerr or errors or &#39;STRICT&#39;, Handler).value
        )

    @Unit.Requires(&#39;chardet&#39;, &#39;default&#39;, &#39;extended&#39;)
    def _chardet():
        import chardet
        return chardet

    def _detect(self, data):
        mv = memoryview(data)
        if not any(mv[1::2]): return &#39;utf-16le&#39;
        if not any(mv[0::2]): return &#39;utf-16be&#39;
        detection = self._chardet.detect(data)
        codec = detection[&#39;encoding&#39;]
        self.log_info(lambda: F&#39;Using input encoding: {codec}, detected with {int(detection[&#34;confidence&#34;] * 100)}% confidence.&#39;)
        return codec

    def _recode(self, enc, dec, encerr, decerr, data):
        dec = dec or self._detect(data)
        return codecs.encode(codecs.decode(data, dec, errors=decerr), enc, errors=encerr)

    def reverse(self, data):
        return self._recode(self.args.decode, self.args.encode, self.args.decerr, self.args.encerr, data)

    def process(self, data):
        return self._recode(self.args.encode, self.args.decode, self.args.encerr, self.args.decerr, data)</code></pre>
</details>
</dd>
<dt id="refinery.shell.reduce"><code class="flex name class">
<span>class <span class="ident">reduce</span></span>
<span>(</span><span>suffix, just=0, temp='t')</span>
</code></dt>
<dd>
<section class="desc"><p>The reduce unit applies an arbitrary multibin suffix repeatedly to reduce a complete frame to a
single chunk. The first chunk in the frame serves as initialization.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/meta/reduce.py#L9-L45" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class reduce(Unit):
    &#34;&#34;&#34;
    The reduce unit applies an arbitrary multibin suffix repeatedly to reduce a complete frame to a
    single chunk. The first chunk in the frame serves as initialization.
    &#34;&#34;&#34;

    def __init__(self,
        suffix: Arg(type=str, help=(
            &#39;The remaining command line is a multibin suffix. The reduction accumulator is initialized &#39;
            &#39;with the first chunk in the frame. Then, each remaining chunk is processed with the given &#39;
            &#39;suffix and the result is used to overwrite the accumulator.&#39;
        )),
        just: Arg.Number(&#39;-j&#39;,
            help=&#39;Optionally specify a maximum number of chunks to process beyond the first.&#39;) = 0,
        temp: Arg.String(&#39;-t&#39;, metavar=&#39;name&#39;,
            help=&#39;The name of the accumulator variable. The default is &#34;{default}&#34;.&#39;) = &#39;t&#39;,
    ):
        super().__init__(suffix=suffix, temp=temp, just=just)

    def filter(self, chunks: Iterable[Chunk]):
        it = iter(chunks)
        just = self.args.just
        name = self.args.temp
        accu = next(it)
        if not just:
            scope = it
        else:
            import itertools
            self.log_info(F&#39;reducing only the next {just} chunks&#39;)
            scope = itertools.islice(it, 0, just)
        for chunk in scope:
            chunk.meta[name] = accu
            accu[:] = DelayedBinaryArgument(self.args.suffix, reverse=True, seed=chunk)(chunk)
            self.log_debug(&#39;reduced:&#39;, accu, clip=True)
        accu.meta.discard(name)
        yield accu
        yield from it</code></pre>
</details>
</dd>
<dt id="refinery.shell.rep"><code class="flex name class">
<span>class <span class="ident">rep</span></span>
<span>(</span><span>count=2, label=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Duplicates the given input a given number of times. It is also possible to specify
an iterable instead of a number, in which case the input will be replicated once for
each item in this iterable.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/strings/rep.py#L8-L50" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class rep(Unit):
    &#34;&#34;&#34;
    Duplicates the given input a given number of times. It is also possible to specify
    an iterable instead of a number, in which case the input will be replicated once for
    each item in this iterable.
    &#34;&#34;&#34;

    def __init__(
        self,
        count: Arg.NumSeq(help=(
            &#39;Defines the number of outputs to generate for each input. The default is {default}. &#39;
            &#39;You can specify any multibin expression that defines an integer iterable here: Each &#39;
            &#39;input chunk will be replicated once for each element of that sequence.&#39;)) = 2,
        label: Arg(type=str, help=(
            &#39;If specified, the meta variable with this name will be populated with the index of &#39;
            &#39;the replicated chunk. When the count parameter is an integer, this label will be &#39;
            &#39;equivalent to the index meta variable.&#39;)) = None
    ):
        super().__init__(count=count, label=label)

    def process(self, data: bytes):
        def count():
            count = self.args.count
            if isinstance(count, int):
                return count
            return sum(1 for _ in count)

        if self.args.squeeze or not self._framed:
            self.log_debug(&#39;compressing all repeated items into a single chunk&#39;)
            yield data * count()
            return

        self.log_debug(&#39;emitting each repeated item as an individual chunk&#39;)

        label = self.args.label
        if label is None:
            yield from repeat(data, count())
            return

        meta = {}
        for counter in self.args.count:
            meta[label] = counter
            yield self.labelled(data, **meta)</code></pre>
</details>
</dd>
<dt id="refinery.shell.repl"><code class="flex name class">
<span>class <span class="ident">repl</span></span>
<span>(</span><span>search, replace=b'', count=-1)</span>
</code></dt>
<dd>
<section class="desc"><p>Performs a simple binary string replacement on the input data.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/strings/repl.py#L6-L24" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class repl(Unit):
    &#34;&#34;&#34;
    Performs a simple binary string replacement on the input data.
    &#34;&#34;&#34;

    def __init__(
        self,
        search : Arg(help=&#39;This is the search term.&#39;),
        replace: Arg(help=&#39;The substitution string. Leave this empty to remove all occurrences of the search term.&#39;) = B&#39;&#39;,
        count  : Arg.Number(&#39;-n&#39;, help=&#39;Only replace the given number of occurrences&#39;) = -1
    ):
        super().__init__(search=search, replace=replace, count=count)

    def process(self, data: bytes):
        return data.replace(
            self.args.search,
            self.args.replace,
            self.args.count
        )</code></pre>
</details>
</dd>
<dt id="refinery.shell.resplit"><code class="flex name class">
<span>class <span class="ident">resplit</span></span>
<span>(</span><span>regex=b'\\r?\\n', multiline=False, ignorecase=False, count=0)</span>
</code></dt>
<dd>
<section class="desc"><p>Splits the data at the given regular expression and returns the sequence of
chunks between the separators. By default, the input is split along line breaks.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/pattern/resplit.py#L6-L27" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class resplit(SingleRegexUnit):
    &#34;&#34;&#34;
    Splits the data at the given regular expression and returns the sequence of
    chunks between the separators. By default, the input is split along line breaks.
    &#34;&#34;&#34;

    def __init__(
        self, regex=RB&#39;\r?\n&#39;, multiline=False, ignorecase=False, count=0
    ):
        super().__init__(regex=regex, multiline=multiline, ignorecase=ignorecase, count=count)

    def process(self, data):
        view = memoryview(data)
        cursor = 0
        count = self.args.count
        for k, match in enumerate(self.regex.finditer(view), 2):
            yield view[cursor:match.start()]
            cursor = match.end()
            yield from match.groups()
            if k &gt; count &gt; 0:
                break
        yield view[cursor:]</code></pre>
</details>
</dd>
<dt id="refinery.shell.resub"><code class="flex name class">
<span>class <span class="ident">resub</span></span>
<span>(</span><span>regex='\\s+', subst=b'', multiline=False, ignorecase=False, count=0)</span>
</code></dt>
<dd>
<section class="desc"><p>A unit for performing substitutions based on a binary regular expression pattern. Besides the
syntax <code>{k}</code> to insert the <code>k</code>-th match group, the unit supports processing the contents of
match groups with arbitrary refinery units. To do so, use the following F-string-like syntax:</p>
<pre><code>{match-group:handlers}
</code></pre>
<p>where <code>:handlers</code> is an optional reverse multibin expression that is used to post-process the
binary data from the match. For example, <code>{2:hex:b64}</code> represents the base64-decoding of the
hex-decoding of the second match group.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/pattern/resub.py#L9-L45" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class resub(SingleRegexUnit):
    &#34;&#34;&#34;
    A unit for performing substitutions based on a binary regular expression pattern. Besides the
    syntax `{k}` to insert the `k`-th match group, the unit supports processing the contents of
    match groups with arbitrary refinery units. To do so, use the following F-string-like syntax:

        {match-group:handlers}

    where `:handlers` is an optional reverse multibin expression that is used to post-process the
    binary data from the match. For example, `{2:hex:b64}` represents the base64-decoding of the
    hex-decoding of the second match group.
    &#34;&#34;&#34;
    def __init__(
        self,
        regex: Arg(help=&#39;Regular expression to be searched and replaced. The default is &#34;{default}&#34;.&#39;) = &#39;\\s+&#39;,
        subst: Arg(&#39;subst&#39;, help=(
            &#39;Substitution value: use {1} for group 1, {0} for entire match. Matches are removed &#39;
            &#39;(replaced by an empty string) by default.&#39;
        )) = B&#39;&#39;,
        multiline=False,
        ignorecase=False,
        count=0
    ):
        super().__init__(regex=regex, subst=subst, multiline=multiline, ignorecase=ignorecase, count=count)

    def process(self, data):
        def repl(match: Match):
            return meta.format_bin(spec, self.codec, [match[0], *match.groups()], match.groupdict())
        self.log_info(&#39;pattern:&#39;, getattr(self.regex, &#39;pattern&#39;, self.regex))
        self.log_info(&#39;replace:&#39;, self.args.subst)
        meta = metavars(data)
        spec = self.args.subst.decode(&#39;ascii&#39;, &#39;backslashreplace&#39;)
        substitute = self.regex.sub
        if self.args.count:
            from functools import partial
            substitute = partial(substitute, count=self.args.count)
        return substitute(repl, data)</code></pre>
</details>
</dd>
<dt id="refinery.shell.rev"><code class="flex name class">
<span>class <span class="ident">rev</span></span>
<span>(</span><span>blocksize=None)</span>
</code></dt>
<dd>
<section class="desc"><p>The blocks of the input data are output in reverse order. If the length of
the input data is not a multiple of the block size, the data is truncated.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/blockwise/rev.py#L12-L49" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class rev(UnaryOperation):
    &#34;&#34;&#34;
    The blocks of the input data are output in reverse order. If the length of
    the input data is not a multiple of the block size, the data is truncated.
    &#34;&#34;&#34;

    def __init__(self, blocksize=None):
        super().__init__(blocksize=blocksize, _truncate=2)

    def inplace(self, block: ndarray):
        return self._numpy.flip(block)

    operate = NotImplemented

    def process(self, data: bytearray):
        if self.bytestream:
            data.reverse()
            return data
        try:
            return self._fastblock(data)
        except FastBlockError:
            b = self.blocksize
            n = len(data)
            q = n // b
            m = q * b
            view = memoryview(data)
            temp = bytearray(b)
            for k in range(0, (q // 2) * b, b):
                lhs = slice(k, k + b)
                rhs = slice(m - k - b, m - k)
                temp[:] = view[rhs]
                data[rhs] = view[lhs]
                data[lhs] = temp
            if m &lt; n:
                del view
                del temp
                del data[m:]
            return data</code></pre>
</details>
</dd>
<dt id="refinery.shell.rex"><code class="flex name class">
<span>class <span class="ident">rex</span></span>
<span>(</span><span>regex, *transformation, unicode=False, unique=False, multiline=False, ignorecase=False, min=1, max=None, len=None, stripspace=False, longest=False, take=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Short for Regular Expression eXtractor: A binary grep which can apply a transformation to each
match. Each match is an individual output. Besides the syntax <code>{k}</code> to insert the <code>k</code>-th match
group, the unit supports processing the contents of match groups with arbitrary refinery units.
To do so, use the following F-string-like syntax:</p>
<pre><code>{match-group:pipeline}
</code></pre>
<p>where <code>:pipeline</code> is an optional pipeline of refinery commands as it would be specified on
the command line. The value of the corresponding match is post-processed with this command.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/pattern/rex.py#L11-L80" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class rex(SingleRegexUnit, PatternExtractor):
    &#34;&#34;&#34;
    Short for Regular Expression eXtractor: A binary grep which can apply a transformation to each
    match. Each match is an individual output. Besides the syntax `{k}` to insert the `k`-th match
    group, the unit supports processing the contents of match groups with arbitrary refinery units.
    To do so, use the following F-string-like syntax:

        {match-group:pipeline}

    where `:pipeline` is an optional pipeline of refinery commands as it would be specified on
    the command line. The value of the corresponding match is post-processed with this command.
    &#34;&#34;&#34;
    def __init__(
        self, regex,
        # TODO: Use positional only in Python 3.8
        # /,
        *transformation: Arg(type=utf8, help=(
            &#39;An optional sequence of transformations to be applied to each match. &#39;
            &#39;Each transformation produces one output in the order in which they   &#39;
            &#39;are given. The default transformation is {0}, i.e. the entire match.  &#39;
        )),
        unicode: Arg.Switch(&#39;-u&#39;, help=&#39;Also find unicode strings.&#39;) = False,
        unique: Arg.Switch(&#39;-q&#39;, help=&#39;Yield every (transformed) match only once.&#39;) = False,
        multiline=False, ignorecase=False, min=1, max=None, len=None, stripspace=False,
        longest=False, take=None
    ):
        super().__init__(
            regex=regex,
            transformation=transformation,
            unicode=unicode,
            unique=unique,
            multiline=multiline,
            ignorecase=ignorecase,
            min=min,
            max=max,
            len=len,
            stripspace=stripspace,
            longest=longest,
            take=take,
            utf16=unicode,
            ascii=True,
            duplicates=not unique
        )

    def process(self, data):
        meta = metavars(data)
        self.log_debug(&#39;regular expression:&#39;, getattr(self.regex, &#39;pattern&#39;, self.regex))
        transformations = []
        specs: List[bytes] = list(self.args.transformation)
        if not specs:
            specs.append(B&#39;{0}&#39;)
        for spec in specs:
            def transformation(match: Match, s=spec.decode(self.codec)):
                symb: dict = match.groupdict()
                args: list = [match.group(0), *match.groups()]
                used = set()
                for key, value in symb.items():
                    if value is None:
                        symb[key] = B&#39;&#39;
                item = meta.format(s, self.codec, args, symb, True, True, used)
                used.update(key for key, value in symb.items() if not value)
                for variable in used:
                    symb.pop(variable, None)
                symb.update(offset=match.start())
                chunk = Chunk(item)
                chunk.meta.update(meta)
                chunk.meta.update(symb)
                return chunk
            transformations.append(transformation)
        yield from self.matches_filtered(memoryview(data), self.regex, *transformations)</code></pre>
</details>
</dd>
<dt id="refinery.shell.rijndael"><code class="flex name class">
<span>class <span class="ident">rijndael</span></span>
<span>(</span><span>key, iv=b'', block_size=16, *, assoc_len=0, mac_len=0, segment_size=0, little_endian=False, raw=False, mode=None, padding=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Rijndael encryption and decryption. Note that there is also a <code><a title="refinery.aes" href="index.html#refinery.aes">aes</a></code> unit which has
much better performance because it calls into the PyCryptodome library. You would have to
use this specific Rijndael unit only if Rijndael is used with a block size that is different
from 16 bytes, in which case it is equivalent to AES.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/crypto/cipher/rijndael.py#L601-L620" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class rijndael(StandardBlockCipherUnit, cipher=BlockCipherFactory(Rijndael)):
    &#34;&#34;&#34;
    Rijndael encryption and decryption. Note that there is also a `refinery.aes` unit which has
    much better performance because it calls into the PyCryptodome library. You would have to
    use this specific Rijndael unit only if Rijndael is used with a block size that is different
    from 16 bytes, in which case it is equivalent to AES.
    &#34;&#34;&#34;
    def __init__(
        self, key, iv=b&#39;&#39;,
        block_size: Arg.Number(&#39;-b&#39;, help=&#39;Cipher block size, default is {default}. Valid choices are 16, 24, and 32.&#39;) = 16,
        **more
    ):
        return super().__init__(key, iv, block_size=block_size, **more)

    @property
    def block_size(self):
        return self.args.block_size

    def _new_cipher(self, **optionals) -&gt; CipherInterface:
        return super()._new_cipher(block_size=self.args.block_size, **optionals)</code></pre>
</details>
</dd>
<dt id="refinery.shell.ripemd128"><code class="flex name class">
<span>class <span class="ident">ripemd128</span></span>
<span>(</span><span>text=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Returns the RIPEMD-128 hash of the input data.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/crypto/hash/cryptographic.py#L59-L65" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class ripemd128(HashUnit):
    &#34;&#34;&#34;
    Returns the RIPEMD-128 hash of the input data.
    &#34;&#34;&#34;
    def _algorithm(self, data):
        from refinery.lib.ripemd128 import ripemd128
        return ripemd128(data)</code></pre>
</details>
</dd>
<dt id="refinery.shell.ripemd160"><code class="flex name class">
<span>class <span class="ident">ripemd160</span></span>
<span>(</span><span>text=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Returns the RIPEMD160 hash of the input data.</p></section>
</dd>
<dt id="refinery.shell.rmv"><code class="flex name class">
<span>class <span class="ident">rmv</span></span>
<span>(</span><span>*names)</span>
</code></dt>
<dd>
<section class="desc"><p>Short for "ReMove Variable": Removes meta variables that were created in the current frame. If no
variable names are given, the unit removes all of them. Note that this can recover variables from
outer frames that were previously shadowed.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/meta/rmv.py#L7-L21" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class rmv(Unit):
    &#34;&#34;&#34;
    Short for &#34;ReMove Variable&#34;: Removes meta variables that were created in the current frame. If no
    variable names are given, the unit removes all of them. Note that this can recover variables from
    outer frames that were previously shadowed.
    &#34;&#34;&#34;
    def __init__(self, *names: Arg(type=str, metavar=&#39;name&#39;, help=&#39;Name of a variable to be removed.&#39;)):
        super().__init__(names=names)

    def process(self, data: Chunk):
        meta = metavars(data)
        keys = self.args.names or list(meta.variable_names())
        for key in keys:
            meta.discard(key)
        return data</code></pre>
</details>
</dd>
<dt id="refinery.shell.rncrypt"><code class="flex name class">
<span>class <span class="ident">rncrypt</span></span>
<span>(</span><span>password)</span>
</code></dt>
<dd>
<section class="desc"><p>Implements encryption and decryption using the RNCryptor specification.
See also: <a href="https://github.com/RNCryptor">https://github.com/RNCryptor</a></p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/crypto/cipher/rncrypt.py#L46-L94" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class rncrypt(Unit):
    &#34;&#34;&#34;
    Implements encryption and decryption using the RNCryptor specification.
    See also: https://github.com/RNCryptor
    &#34;&#34;&#34;
    def __init__(self, password: bytearray):
        super().__init__(password=password)

    def process(self, data: bytes) -&gt; bytes:
        encryption_salt = data[2:10]
        hmac_salt = data[10:18]
        iv = data[18:34]
        cipher_text = data[34:-32]
        hmac_signature = data[-32:]
        encryption_key = self._pbkdf2(self.args.password, encryption_salt)
        hmac_key = self._pbkdf2(self.args.password, hmac_salt)
        if not hmac.compare_digest(self._hmac(hmac_key, data[:-32]), hmac_signature):
            raise ValueError(&#34;Failed to verify signature.&#34;)
        return unpad(
            self._aes_decrypt(encryption_key, iv, cipher_text),
            block_size=AES.block_size
        )

    def reverse(self, data: bytes) -&gt; bytes:
        prng = Random.new()
        data = pad(data, block_size=AES.block_size)
        encryption_salt = prng.read(8)
        encryption_key = self._pbkdf2(self.args.password, encryption_salt)
        hmac_salt = prng.read(8)
        hmac_key = self._pbkdf2(self.args.password, hmac_salt)
        iv = prng.read(AES.block_size)
        cipher_text = self._aes_encrypt(encryption_key, iv, data)
        new_data = b&#39;\x03\x01&#39; + encryption_salt + hmac_salt + iv + cipher_text
        return new_data + self._hmac(hmac_key, new_data)

    def _aes_encrypt(self, key, iv, text):
        return AES.new(key, AES.MODE_CBC, iv).encrypt(text)

    def _aes_decrypt(self, key, iv, text):
        return AES.new(key, AES.MODE_CBC, iv).decrypt(text)

    def _hmac(self, key, data):
        return hmac.new(key, data, hashlib.sha256).digest()

    def _prf(self, secret, salt):
        return hmac.new(secret, salt, hashlib.sha1).digest()

    def _pbkdf2(self, password, salt, iterations=10000, key_length=32):
        return KDF.PBKDF2(password, salt, dkLen=key_length, count=iterations, prf=self._prf)</code></pre>
</details>
</dd>
<dt id="refinery.shell.rot"><code class="flex name class">
<span>class <span class="ident">rot</span></span>
<span>(</span><span>amount=13)</span>
</code></dt>
<dd>
<section class="desc"><p>Rotate the characters of the alphabet by the given amount. The default
amount is 13, providing the common (and weak) string obfuscation method.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/crypto/cipher/rot.py#L9-L26" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class rot(Unit):
    &#34;&#34;&#34;
    Rotate the characters of the alphabet by the given amount. The default
    amount is 13, providing the common (and weak) string obfuscation method.
    &#34;&#34;&#34;

    def __init__(self, amount: Arg.Number(help=&#39;Number of letters to rotate by; Default is 13.&#39;) = 13):
        super().__init__(amount=amount)

    def process(self, data: bytearray):
        rot = self.args.amount % 26
        for index, byte in enumerate(data):
            for alphabet in _LCASE, _UCASE:
                if byte in alphabet:
                    zero = alphabet[0]
                    data[index] = zero + (byte - zero + rot) % 26
                    break
        return data</code></pre>
</details>
</dd>
<dt id="refinery.shell.rotl"><code class="flex name class">
<span>class <span class="ident">rotl</span></span>
<span>(</span><span>argument, bigendian=False, blocksize=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Rotate the bits of each block left.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/blockwise/rotl.py#L6-L18" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class rotl(BinaryOperation):
    &#34;&#34;&#34;
    Rotate the bits of each block left.
    &#34;&#34;&#34;
    def operate(self, value, shift):
        shift %= self.fbits
        return (value &lt;&lt; shift) | (value &gt;&gt; (self.fbits - shift))

    def inplace(self, value, shift):
        shift %= self.fbits
        lower = value &gt;&gt; (self.fbits - shift)
        value &lt;&lt;= shift
        value |= lower</code></pre>
</details>
</dd>
<dt id="refinery.shell.rotr"><code class="flex name class">
<span>class <span class="ident">rotr</span></span>
<span>(</span><span>argument, bigendian=False, blocksize=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Rotate the bits of each block right.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/blockwise/rotr.py#L6-L18" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class rotr(BinaryOperation):
    &#34;&#34;&#34;
    Rotate the bits of each block right.
    &#34;&#34;&#34;
    def operate(self, value, shift):
        shift %= self.fbits
        return (value &gt;&gt; shift) | (value &lt;&lt; (self.fbits - shift))

    def inplace(self, value, shift):
        shift %= self.fbits
        lower = value &gt;&gt; shift
        value &lt;&lt;= self.fbits - shift
        value |= lower</code></pre>
</details>
</dd>
<dt id="refinery.shell.rsa"><code class="flex name class">
<span>class <span class="ident">rsa</span></span>
<span>(</span><span>key, swapkeys=False, textbook=False, padding=PAD.AUTO, rsautl=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Implements single block RSA encryption and decryption. This unit can be used to encrypt
and decrypt blocks generated by openssl's <code>rsautl</code> tool when using the mode <code>-verify</code>.
When it is executed with a public key for decryption or with a private key for encryption,
it will perform a raw RSA operation. The result of these operations are (un)padded using
EMSA-PKCS1-v1_5.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/crypto/cipher/rsa.py#L82-L271" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class rsa(Unit):
    &#34;&#34;&#34;
    Implements single block RSA encryption and decryption. This unit can be used to encrypt
    and decrypt blocks generated by openssl&#39;s `rsautl` tool when using the mode `-verify`.
    When it is executed with a public key for decryption or with a private key for encryption,
    it will perform a raw RSA operation. The result of these operations are (un)padded using
    EMSA-PKCS1-v1_5.
    &#34;&#34;&#34;
    def __init__(
        self,
        key: Arg(help=&#39;RSA key in PEM, DER, or Microsoft BLOB format.&#39;),
        swapkeys: Arg.Switch(&#39;-s&#39;, help=&#39;Swap public and private exponent.&#39;) = False,
        textbook: Arg.Switch(&#39;-t&#39;, group=&#39;PAD&#39;, help=&#39;Equivalent to --padding=NONE.&#39;) = False,
        padding : Arg.Option(&#39;-p&#39;, group=&#39;PAD&#39;, choices=PAD,
            help=&#39;Choose one of the following padding modes: {choices}. The default is AUTO.&#39;) = PAD.AUTO,
        rsautl  : Arg.Switch(&#39;-r&#39;, group=&#39;PAD&#39;,
            help=&#39;Act as rsautl from OpenSSH; This is equivalent to --swapkeys --padding=PKCS10&#39;) = False,
    ):
        padding = Arg.AsOption(padding, PAD)
        if textbook:
            if padding != PAD.AUTO:
                raise ValueError(&#39;Conflicting padding options!&#39;)
            padding = padding.NONE
        if rsautl:
            if padding and padding != PAD.PKCS10:
                raise ValueError(&#39;Conflicting padding options!&#39;)
            swapkeys = True
            padding = PAD.PKCS10

        super().__init__(key=key, textbook=textbook, padding=padding, swapkeys=swapkeys)

        self._key_hash = None
        self._key_data = None

    @property
    def blocksize(self) -&gt; int:
        return self.key.size_in_bytes()

    @property
    def _blocksize_plain(self) -&gt; int:
        # PKCS#1 v1.5 padding is at least 11 bytes.
        return self.blocksize - 11

    @property
    def pub(self):
        return self.key.d if self.args.swapkeys else self.key.e

    @property
    def prv(self):
        return self.key.e if self.args.swapkeys else self.key.d

    def _get_msg(self, data):
        msg = int.from_bytes(data, byteorder=&#39;big&#39;)
        if msg &gt; self.key.n:
            raise ValueError(F&#39;This key can only handle messages of size {self.blocksize}.&#39;)
        return msg

    def _encrypt_raw(self, data):
        return pow(
            self._get_msg(data),
            self.pub,
            self.key.n
        ).to_bytes(self.blocksize, byteorder=&#39;big&#39;)

    def _decrypt_raw(self, data):
        return pow(
            self._get_msg(data),
            self.prv,
            self.key.n
        ).to_bytes(self.blocksize, byteorder=&#39;big&#39;)

    def _unpad(self, data, head, padbyte=None):
        if len(data) &gt; self.blocksize:
            raise ValueError(F&#39;This key can only handle messages of size {self.blocksize}.&#39;)
        if data.startswith(head):
            pos = data.find(B&#39;\0&#39;, 2)
            if pos &gt; 0:
                pad = data[2:pos]
                if padbyte is None or all(b == padbyte for b in pad):
                    return data[pos + 1:]
        raise ValueError(&#39;Incorrect padding&#39;)

    def _pad(self, data, head, padbyte=None):
        if len(data) &gt; self._blocksize_plain:
            raise ValueError(F&#39;This key can only encrypt messages of size at most {self._blocksize_plain}.&#39;)
        pad = self.blocksize - len(data) - len(head) - 1
        if padbyte is not None:
            padding = pad * bytes((padbyte,))
        else:
            padding = bytearray(1)
            while not all(padding):
                padding = bytearray(filter(None, padding))
                padding.extend(get_random_bytes(pad - len(padding)))
        return head + padding + B&#39;\0&#39; + data

    def _unpad_pkcs10(self, data):
        return self._unpad(data, B&#39;\x00\x01&#39;, 0xFF)

    def _unpad_pkcs15(self, data):
        return self._unpad(data, B&#39;\x00\x02&#39;, None)

    def _pad_pkcs10(self, data):
        return self._pad(data, B&#39;\x00\x01&#39;, 0xFF)

    def _pad_pkcs15(self, data):
        return self._pad(data, B&#39;\x00\x02&#39;, None)

    def _decrypt_block_OAEP(self, data):
        self.log_debug(&#39;Attempting decryption with PyCrypto PKCS1 OAEP.&#39;)
        return PKCS1_OAEP.new(self.key).decrypt(data)

    def _encrypt_block_OAEP(self, data):
        self.log_debug(&#39;Attempting encryption with PyCrypto PKCS1 OAEP.&#39;)
        return PKCS1_OAEP.new(self.key).encrypt(data)

    def _decrypt_block(self, data):
        if self._oaep and self._pads in {PAD.AUTO, PAD.OAEP}:
            try:
                return self._decrypt_block_OAEP(data)
            except ValueError as E:
                if self._pads:
                    raise
                self.log_debug(F&#39;{E!s} No longer attempting OAEP.&#39;)
                self._oaep = False

        data = self._decrypt_raw(data)
        return self._unpad_per_argument(data)

    def _unpad_per_argument(self, data):
        if self._pads == PAD.NONE:
            return data
        elif self._pads == PAD.PKCS10:
            return self._unpad_pkcs10(data)
        elif self._pads == PAD.PKCS15:
            return self._unpad_pkcs15(data)
        elif self._pads == PAD.AUTO:
            with suppress(ValueError):
                data = self._unpad_pkcs10(data)
                self.log_info(&#39;Detected PKCS1.0 padding.&#39;)
                self._pads = PAD.PKCS10
                return data
            with suppress(ValueError):
                data = self._unpad_pkcs15(data)
                self.log_info(&#39;Detected PKCS1.5 padding.&#39;)
                self._pads = PAD.PKCS15
                return data
            raise RefineryPartialResult(&#39;No padding worked, returning raw decrypted blocks.&#39;, data)
        else:
            raise ValueError(F&#39;Invalid padding value: {self._pads!r}&#39;)

    def _encrypt_block(self, data):
        if self._pads in {PAD.AUTO, PAD.OAEP}:
            try:
                return self._encrypt_block_OAEP(data)
            except ValueError:
                if self._pads: raise
                self.log_debug(&#39;PyCrypto primitives for OAEP failed, falling back to PKCS1.5.&#39;)
                self._pads = PAD.PKCS15

        if self._pads == PAD.PKCS15:
            data = self._pad_pkcs15(data)
        elif self._pads == PAD.PKCS10:
            data = self._pad_pkcs10(data)

        return self._encrypt_raw(data)

    @property
    def key(self) -&gt; RSA.RsaKey:
        key_blob = self.args.key
        key_hash = hash(key_blob)
        if key_hash != self._key_hash:
            fmt, key_data = normalize_rsa_key(key_blob)
            self.log_info(F&#39;successfully parsed RSA key as {fmt.value}&#39;)
            self._key_hash = key_hash
            self._key_data = key_data
        return self._key_data

    def process(self, data):
        self._oaep = True
        self._pads = self.args.padding
        if not self.key.has_private():
            try:
                return self._unpad_per_argument(self._encrypt_raw(data))
            except Exception as E:
                raise ValueError(F&#39;A public key was given for decryption and rsautl mode resulted in an error: {E}&#39;) from E
        return B&#39;&#39;.join(self._decrypt_block(block) for block in splitchunks(data, self.blocksize))

    def reverse(self, data):
        self._pads = self.args.padding
        return B&#39;&#39;.join(self._encrypt_block(block) for block in splitchunks(data, self._blocksize_plain))</code></pre>
</details>
</dd>
<dt id="refinery.shell.rsakey"><code class="flex name class">
<span>class <span class="ident">rsakey</span></span>
<span>(</span><span>public=False, output=RSAFormat.PEM)</span>
</code></dt>
<dd>
<section class="desc"><p>Parse RSA keys in various formats; PEM, DER, Microsoft BLOB, and W3C-XKMS (XML) format are supported.
The same formats are supported for the input format, but you can also specify a key in the following
format, where both modulus and exponent have to be hex-encoded: <code>[modulus]:[exponent]</code></p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/crypto/cipher/rsakey.py#L26-L115" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class rsakey(Unit):
    &#34;&#34;&#34;
    Parse RSA keys in various formats; PEM, DER, Microsoft BLOB, and W3C-XKMS (XML) format are supported.
    The same formats are supported for the input format, but you can also specify a key in the following
    format, where both modulus and exponent have to be hex-encoded: `[modulus]:[exponent]`
    &#34;&#34;&#34;
    def __init__(
        self,
        public: Arg.Switch(&#39;-p&#39;, help=&#39;Force public key output even if the input is private.&#39;) = False,
        output: Arg.Option(help=&#39;Select an output format ({choices}), default is {default}.&#39;, choices=RSAFormat) = RSAFormat.PEM
    ):
        super().__init__(public=public, output=Arg.AsOption(output, RSAFormat))

    def _xkms_wrap(self, number: int):
        size, r = divmod(number.bit_length(), 8)
        size += int(bool(r))
        return base64.b64encode(number.to_bytes(size, &#39;big&#39;))

    def process(self, data):
        from refinery.lib.mscrypto import TYPES, ALGORITHMS
        fmt, key = normalize_rsa_key(data, force_public=self.args.public)
        self.log_info(F&#39;parsing input as {fmt.value} format&#39;)
        out = self.args.output
        if out is RSAFormat.PEM:
            yield key.export_key(&#39;PEM&#39;)
            return
        if out is RSAFormat.DER:
            yield key.export_key(&#39;DER&#39;)
            return
        if out is RSAFormat.BLOB:
            def le(v: int, s: int):
                return v.to_bytes(s, &#39;little&#39;)
            buffer = bytearray()
            buffer.append(TYPES.PRIVATEKEYBLOB if key.has_private() else TYPES.PUBLICKEYBLOB)
            buffer.extend(le(2, 3))
            buffer.extend(le(ALGORITHMS.CALG_RSA_KEYX, 4))
            buffer.extend(B&#39;RSA2&#39; if key.has_private() else B&#39;RSA1&#39;)
            size = 2
            while size &lt; key.n.bit_length():
                size &lt;&lt;= 1
            self.log_info(F&#39;using bit size {size}&#39;)
            buffer.extend(le(size, 4))
            size //= 8
            buffer.extend(le(key.e, 4))
            buffer.extend(le(key.n, size))
            if key.has_private():
                exp_1 = key.d % (key.p - 1)
                exp_2 = key.d % (key.q - 1)
                coeff = pow(key.q, -1, key.p)
                half = size // 2
                buffer.extend(le(key.p, half))
                buffer.extend(le(key.q, half))
                buffer.extend(le(exp_1, half))
                buffer.extend(le(exp_2, half))
                buffer.extend(le(coeff, half))
                buffer.extend(le(key.d, size))
            yield buffer
            return
        components = {
            &#39;Modulus&#39; : key.n,
            &#39;Exponent&#39;: key.e,
        }
        if key.has_private():
            decoded = DerSequence()
            decoded.decode(key.export_key(&#39;DER&#39;))
            it = itertools.islice(decoded, 3, None)
            for v in (&#39;D&#39;, &#39;P&#39;, &#39;Q&#39;, &#39;DP&#39;, &#39;DQ&#39;, &#39;InverseQ&#39;):
                try:
                    components[v] = next(it)
                except StopIteration:
                    break
        if out is RSAFormat.XKMS:
            for tag in components:
                components[tag] = base64.b64encode(number.long_to_bytes(components[tag])).decode(&#39;ascii&#39;)
            tags = &#39;\n&#39;.join(F&#39;\t&lt;{tag}&gt;{value}&lt;/{tag}&gt;&#39; for tag, value in components.items())
            yield F&#39;&lt;RSAKeyPair&gt;\n{tags}\n&lt;/RSAKeyPair&gt;&#39;.encode(self.codec)
            return
        components[&#39;BitSize&#39;] = key.n.bit_length()
        for tag, value in components.items():
            if value.bit_length() &gt; 32:
                components[tag] = F&#39;{value:X}&#39;
        if out is RSAFormat.JSON:
            yield json.dumps(components, indent=4).encode(self.codec)
            return
        if out is RSAFormat.TEXT:
            table = list(flattened(components))
            for key, value in table:
                value = F&#39;0x{value}&#39; if isinstance(value, str) else str(value)
                value = &#39;\n&#39;.join(F&#39;{L}&#39; for L in textwrap.wrap(value, 80))
                yield F&#39;-- {key + &#34; &#34;:-&lt;77}\n{value!s}&#39;.encode(self.codec)</code></pre>
</details>
</dd>
<dt id="refinery.shell.salsa"><code class="flex name class">
<span>class <span class="ident">salsa</span></span>
<span>(</span><span>key, stateful=False, discard=0, nonce=b'REFINERY', magic=b'', offset=0, rounds=20)</span>
</code></dt>
<dd>
<section class="desc"><p>Salsa encryption and decryption. The nonce must be 8 bytes long. When 64 bytes are provided
as the key, this data is interpreted as the initial state box and all other parameters are
ignored.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/crypto/cipher/salsa.py#L149-L167" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class salsa(LatinCipherUnit):
    &#34;&#34;&#34;
    Salsa encryption and decryption. The nonce must be 8 bytes long. When 64 bytes are provided
    as the key, this data is interpreted as the initial state box and all other parameters are
    ignored.
    &#34;&#34;&#34;
    def keystream(self) -&gt; Iterable[int]:
        key = self.args.key
        if len(key) == 64:
            it = SalsaCipher.FromState(key)
        else:
            it = SalsaCipher(
                key,
                self.args.nonce,
                self.args.magic,
                self.args.rounds,
                self.args.offset,
            )
        yield from it</code></pre>
</details>
</dd>
<dt id="refinery.shell.salsa20"><code class="flex name class">
<span>class <span class="ident">salsa20</span></span>
<span>(</span><span>key, nonce=b'REFINERY')</span>
</code></dt>
<dd>
<section class="desc"><p>Salsa20 encryption and decryption. This unit is functionally equivalent to <code><a title="refinery.salsa" href="index.html#refinery.salsa">salsa</a></code>
with 20 rounds, but it uses the PyCryptodome library C implementation rather than the pure
Python implementation used by <code><a title="refinery.salsa" href="index.html#refinery.salsa">salsa</a></code>.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/crypto/cipher/salsa.py#L189-L195" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class salsa20(LatinCipherStandardUnit, cipher=PyCryptoFactoryWrapper(Salsa20)):
    &#34;&#34;&#34;
    Salsa20 encryption and decryption. This unit is functionally equivalent to `refinery.salsa`
    with 20 rounds, but it uses the PyCryptodome library C implementation rather than the pure
    Python implementation used by `refinery.salsa`.
    &#34;&#34;&#34;
    pass</code></pre>
</details>
</dd>
<dt id="refinery.shell.scope"><code class="flex name class">
<span>class <span class="ident">scope</span></span>
<span>(</span><span>*slice, visible=True)</span>
</code></dt>
<dd>
<section class="desc"><p>After using <code><a title="refinery.scope" href="index.html#refinery.scope">scope</a></code> within in a <code><a title="refinery.lib.frame" href="lib/frame.html">refinery.lib.frame</a></code>, all the
following operations will be applied only to the selected indices. All
remaining chunks still exist, they are just not operated on. When the
frame closes or the frame is being rescoped by a second application of
this unit, they become visible again.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/meta/scope.py#L13-L62" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class scope(FrameSlicer):
    &#34;&#34;&#34;
    After using `refinery.scope` within in a `refinery.lib.frame`, all the
    following operations will be applied only to the selected indices. All
    remaining chunks still exist, they are just not operated on. When the
    frame closes or the frame is being rescoped by a second application of
    this unit, they become visible again.
    &#34;&#34;&#34;
    def __init__(self, *slice, visible: Arg.Switch(&#39;-n&#39;, &#39;--not&#39;, off=True, help=(
        &#39;Hide the given chunks instead of making them the only ones visible.&#39;)) = True
    ):
        super().__init__(*slice, visible=visible)
        # Sort any slices with negative arguments to the back so we check
        # them last. This delays potential consumption of the chunks iterator
        # as much as possible.
        self.args.slice.sort(
            key=lambda s: (s.start or 0, s.stop or 0), reverse=True)

    def filter(self, chunks):
        it = iter(chunks)
        consumed = None
        size = None

        def buffered() -&gt; Generator[Chunk, None, None]:
            yield from it
            while consumed:
                yield consumed.popleft()

        def shift(offset, default):
            nonlocal consumed, it, size
            if offset is None:
                return default
            if offset &gt;= 0:
                return offset
            if consumed is None:
                from collections import deque
                self.log_info(F&#39;consuming iterator to compute negative offset {offset}.&#39;)
                consumed = deque(it)
                size = len(consumed) + k + 1
            return max(0, offset + size)

        for k, chunk in enumerate(buffered()):
            for s in self.args.slice:
                if k in range(shift(s.start, 0), shift(s.stop, k + 1), s.step or 1):
                    chunk.visible = self.args.visible
                    break
            else:
                chunk.visible = not self.args.visible
            self.log_debug(chunk)
            yield chunk</code></pre>
</details>
</dd>
<dt id="refinery.shell.seal"><code class="flex name class">
<span>class <span class="ident">seal</span></span>
<span>(</span><span>key, discard=0, stateful=False)</span>
</code></dt>
<dd>
<section class="desc"><p>SEAL encryption and decryption.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/crypto/cipher/seal.py#L190-L197" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class seal(StreamCipherUnit):
    &#34;&#34;&#34;
    SEAL encryption and decryption.
    &#34;&#34;&#34;
    key_size = {20}

    def keystream(self) -&gt; Iterable[bytes]:
        return SEAL_Cipher(self.args.key)</code></pre>
</details>
</dd>
<dt id="refinery.shell.secstr"><code class="flex name class">
<span>class <span class="ident">secstr</span></span>
<span>(</span><span>key=b'\x01\x02\x03\x04\x05\x06\x07\x08\t\n\x0b\x0c\r\x0e\x0f\x10', iv=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Implements the AES-based encryption scheme used by the PowerShell commands
<code>ConvertFrom-SecureString</code> and <code>ConvertTo-SecureString</code>.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/crypto/cipher/secstr.py#L12-L84" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class secstr(Unit):
    &#34;&#34;&#34;
    Implements the AES-based encryption scheme used by the PowerShell commands
    `ConvertFrom-SecureString` and `ConvertTo-SecureString`.
    &#34;&#34;&#34;

    # This is a magic header value used for PowerShell secure strings.
    _MAGIC = bytes((
        0xEF, 0xAE, 0x3D, 0xD9, 0xDD, 0x75, 0xD7, 0xAE, 0xF8, 0xDD, 0xFD, 0x38,
        0xDB, 0x7E, 0x35, 0xDD, 0xBD, 0x7A, 0xD3, 0x9D, 0x1A, 0xE7, 0x7E, 0x39))

    # Secure strings include a decimal number formatted as a string directly
    # following the header. Presumably, this is the PowerShell version.
    _PSVER = 2

    def __init__(
        self, key: Arg(
            help=&#39;Secure string encryption 16-byte AES key; the default are the bytes from 1 to 16.&#39;
        ) = bytes(range(1, 17)),
        iv: Arg(&#39;-i&#39;, help=&#39;Optionally specify an IV to use for encryption.&#39;) = None
    ):
        super().__init__(key=key, iv=iv)

    @property
    def key(self):
        key = self.args.key
        if len(key) not in (0x10, 0x18, 0x20):
            raise ValueError(&#39;The encryption key has to be 16 bytes long.&#39;)
        return key

    @property
    def iv(self):
        iv = self.args.iv
        if iv is not None and len(iv) != 0x10:
            raise ValueError(&#39;The IV has to be 16 bytes long.&#39;)
        return iv

    def reverse(self, data):
        ivec = self.iv or urandom(0x10)
        if len(ivec) != 0x10:
            raise ValueError(self._IVERR)
        cipher = AES.new(self.key, AES.MODE_CBC, ivec)
        data = data.decode(&#39;latin-1&#39;).encode(&#39;utf-16LE&#39;)
        data = cipher.encrypt(pad(data, block_size=0x10))
        data = base64.b16encode(data).lower().decode(&#39;ascii&#39;)
        ivec = base64.b64encode(ivec).decode(&#39;ascii&#39;)
        data = &#39;|&#39;.join((&#39;%d&#39; % self._PSVER, ivec, data)).encode(&#39;utf-16LE&#39;)
        return base64.b64encode(self._MAGIC + data)

    def process(self, data):
        head, ivec, data = base64.b64decode(data).split(b&#39;|\0&#39;)
        self.log_info(&#39;head:&#39;, head.hex())
        ivec = base64.b64decode(ivec.decode(&#39;utf-16LE&#39;))
        self.log_info(&#39;ivec:&#39;, ivec.hex())
        data = base64.b16decode(data.decode(&#39;utf-16LE&#39;), casefold=True)
        if len(data) % 0x10 != 0:
            self.log_info(&#39;data not block-aligned, padding with zeros&#39;)
            data += B&#39;\0&#39; * (0x10 - len(data) % 0x10)
        cipher = AES.new(self.key, AES.MODE_CBC, ivec)
        data = cipher.decrypt(data)
        try:
            data = unpad(data, block_size=0x10)
        except Exception:
            self.log_warn(&#39;decrypted data does not have PKCS7 padding&#39;)
        for p in range(0x10):
            try:
                return data[-p:].decode(&#39;utf-16LE&#39;).encode(&#39;latin-1&#39;)
            except UnicodeDecodeError:
                pass
            except UnicodeEncodeError:
                pass
        self.log_warn(&#39;result is not a padded unicode string, key is likely wrong&#39;)
        return data</code></pre>
</details>
</dd>
<dt id="refinery.shell.sep"><code class="flex name class">
<span>class <span class="ident">sep</span></span>
<span>(</span><span>separator=b'\n', scoped=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Multiple inputs are joined along a specified separator. If any of the input
<code><a title="refinery.lib.frame.Chunk" href="lib/frame.html#refinery.lib.frame.Chunk">Chunk</a></code>s is currently out of scope, <code><a title="refinery.sep" href="index.html#refinery.sep">sep</a></code> turns
makes them visible by default. This can be prevented by using the <code>-s</code> flag.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/meta/sep.py#L6-L39" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class sep(Unit):
    &#34;&#34;&#34;
    Multiple inputs are joined along a specified separator. If any of the input
    `refinery.lib.frame.Chunk`s is currently out of scope, `refinery.sep` turns
    makes them visible by default. This can be prevented by using the `-s` flag.
    &#34;&#34;&#34;

    def __init__(
        self, separator: Arg(help=&#39;Separator; the default is a line break.&#39;) = B&#39;\n&#39;,
        scoped: Arg.Switch(&#39;-s&#39;, help=(
            &#39;Maintain chunk scope; i.e. do not turn all input chunks visible.&#39;)) = False
    ):
        super().__init__(separator=separator, scoped=scoped)
        self.separate = False

    def filter(self, chunks):
        it = iter(chunks)
        try:
            chunk = next(it)
        except StopIteration:
            return
        self.separate = True
        for upcoming in it:
            if not self.args.scoped:
                chunk.visible = True
            yield chunk
            chunk = upcoming
        self.separate = False
        yield chunk

    def process(self, data):
        yield data
        if self.separate:
            yield self.args.separator</code></pre>
</details>
</dd>
<dt id="refinery.shell.serpent"><code class="flex name class">
<span>class <span class="ident">serpent</span></span>
<span>(</span><span>key, iv=b'', padding=None, mode=None, raw=False, swap=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Serpent encryption and decryption. Some Serpent implementations read the bytes of each block
in one direction, some in the other. When decryption results with this unit do not yield the
expected result, try using the <code>--swap</code> (or <code>-s</code>) option to swap the bytes in each block.
Furthermore, it is sometimes necessary to swap the bytes of the input key, which can be done
by prefixing the input key by the multibin handler <code>snip[::-1]</code>.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/crypto/cipher/serpent.py#L56-L73" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class serpent(StandardBlockCipherUnit, cipher=BlockCipherFactory(Serpent)):
    &#34;&#34;&#34;
    Serpent encryption and decryption. Some Serpent implementations read the bytes of each block
    in one direction, some in the other. When decryption results with this unit do not yield the
    expected result, try using the `--swap` (or `-s`) option to swap the bytes in each block.
    Furthermore, it is sometimes necessary to swap the bytes of the input key, which can be done
    by prefixing the input key by the multibin handler `snip[::-1]`.
    &#34;&#34;&#34;
    def __init__(
        self, key, iv=b&#39;&#39;, padding=None, mode=None, raw=False,
        swap: Arg.Switch(&#39;-s&#39;, help=&#39;Read the bytes in each block in reverse order.&#39;) = False
    ):
        super().__init__(key, iv, padding=padding, mode=mode, raw=raw, swap=swap)

    def _new_cipher(self, **optionals) -&gt; CipherInterface:
        instance: Serpent = super()._new_cipher()
        instance.swap = self.args.swap
        return instance</code></pre>
</details>
</dd>
<dt id="refinery.shell.sha1"><code class="flex name class">
<span>class <span class="ident">sha1</span></span>
<span>(</span><span>text=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Returns the SHA1 hash of the input data.</p></section>
</dd>
<dt id="refinery.shell.sha224"><code class="flex name class">
<span>class <span class="ident">sha224</span></span>
<span>(</span><span>text=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Returns the SHA224 hash of the input data.</p></section>
</dd>
<dt id="refinery.shell.sha256"><code class="flex name class">
<span>class <span class="ident">sha256</span></span>
<span>(</span><span>text=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Returns the SHA256 hash of the input data.</p></section>
</dd>
<dt id="refinery.shell.sha384"><code class="flex name class">
<span>class <span class="ident">sha384</span></span>
<span>(</span><span>text=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Returns the SHA384 hash of the input data.</p></section>
</dd>
<dt id="refinery.shell.sha3_224"><code class="flex name class">
<span>class <span class="ident">sha3_224</span></span>
<span>(</span><span>text=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Returns the SHA3-224 hash of the input data.</p></section>
</dd>
<dt id="refinery.shell.sha3_256"><code class="flex name class">
<span>class <span class="ident">sha3_256</span></span>
<span>(</span><span>text=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Returns the SHA3-256 hash of the input data.</p></section>
</dd>
<dt id="refinery.shell.sha3_384"><code class="flex name class">
<span>class <span class="ident">sha3_384</span></span>
<span>(</span><span>text=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Returns the SHA3-384 hash of the input data.</p></section>
</dd>
<dt id="refinery.shell.sha3_512"><code class="flex name class">
<span>class <span class="ident">sha3_512</span></span>
<span>(</span><span>text=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Returns the SHA3-512 hash of the input data.</p></section>
</dd>
<dt id="refinery.shell.sha512"><code class="flex name class">
<span>class <span class="ident">sha512</span></span>
<span>(</span><span>text=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Returns the SHA512 hash of the input data.</p></section>
</dd>
<dt id="refinery.shell.shl"><code class="flex name class">
<span>class <span class="ident">shl</span></span>
<span>(</span><span>argument, bigendian=False, blocksize=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Shift the bits of each block left, filling with zero bits.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/blockwise/shl.py#L6-L13" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class shl(BinaryOperation):
    &#34;&#34;&#34;
    Shift the bits of each block left, filling with zero bits.
    &#34;&#34;&#34;
    @staticmethod
    def operate(a, b): return a &lt;&lt; b
    @staticmethod
    def inplace(a, b): a &lt;&lt;= b</code></pre>
</details>
</dd>
<dt id="refinery.shell.shr"><code class="flex name class">
<span>class <span class="ident">shr</span></span>
<span>(</span><span>argument, bigendian=False, blocksize=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Shift the bits of each block right, filling with zero bits.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/blockwise/shr.py#L6-L13" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class shr(BinaryOperation):
    &#34;&#34;&#34;
    Shift the bits of each block right, filling with zero bits.
    &#34;&#34;&#34;
    @staticmethod
    def operate(a, b): return a &gt;&gt; b
    @staticmethod
    def inplace(a, b): a &gt;&gt;= b</code></pre>
</details>
</dd>
<dt id="refinery.shell.sm4"><code class="flex name class">
<span>class <span class="ident">sm4</span></span>
<span>(</span><span>key, iv=b'', *, padding=None, mode=None, raw=False, little_endian=False, segment_size=0, mac_len=0, assoc_len=0)</span>
</code></dt>
<dd>
<section class="desc"><p>The SM4 symmetric blockcipher algorithm published as GB/T 32907-2016 by the State Cryptography
Administration of China (SCA).</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/crypto/cipher/sm4.py#L107-L112" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class sm4(StandardBlockCipherUnit, cipher=BlockCipherFactory(SM4)):
    &#34;&#34;&#34;
    The SM4 symmetric blockcipher algorithm published as GB/T 32907-2016 by the State Cryptography
    Administration of China (SCA).
    &#34;&#34;&#34;
    pass</code></pre>
</details>
</dd>
<dt id="refinery.shell.snip"><code class="flex name class">
<span>class <span class="ident">snip</span></span>
<span>(</span><span>slices=[slice(None, None, None)], length=False, stream=False, remove=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Snips the input data based on a Python slice expression. For example, the
initialization <code>slice 0::1 1::1</code> would yield a unit that first extracts
every byte at an even position and then, every byte at an odd position. In
this case, multiple outputs are produced. The unit can be used in reverse
mode, in which case the specified ranges are deleted sequentially from the
input.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/strings/snip.py#L8-L61" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class snip(Unit):
    &#34;&#34;&#34;
    Snips the input data based on a Python slice expression. For example, the
    initialization `slice 0::1 1::1` would yield a unit that first extracts
    every byte at an even position and then, every byte at an odd position. In
    this case, multiple outputs are produced. The unit can be used in reverse
    mode, in which case the specified ranges are deleted sequentially from the
    input.
    &#34;&#34;&#34;
    def __init__(
        self,
        slices: Arg(help=&#39;Specify start:stop:step in Python slice syntax.&#39;) = [slice(None, None)],
        length: Arg.Switch(&#39;-l&#39;, help=(
            &#39;Interpret the end of a slice as a length rather than as an offset.&#39;)) = False,
        stream: Arg.Switch(&#39;-s&#39;, help=(
            &#39;After each slice, consider only the data that follows after it for subsequent &#39;
            &#39;slicing.&#39;)) = False,
        remove: Arg.Switch(&#39;-r&#39;, help=(
            &#39;Remove the slices from the input rather than selecting them.&#39;)) = False,
    ):
        super().__init__(slices=slices, length=length, stream=stream, remove=remove)

    def process(self, data: bytearray):
        slices: list[slice] = list(self.args.slices)
        stream = self.args.stream
        remove = self.args.remove
        length = self.args.length
        cursor = 0
        view = memoryview(data)

        for k, bounds in enumerate(slices):
            upper = bounds.stop
            lower = bounds.start or 0
            if upper is None:
                upper = len(data)
            else:
                upper += cursor
            if length:
                upper += lower
            bounds = slice(
                lower + cursor, upper, bounds.step)
            if stream:
                cursor = upper
            if not remove:
                temp = view[bounds]
            else:
                if k + 1 &gt;= len(slices):
                    view.release()
                    del view
                    temp = data
                else:
                    temp = bytearray(data)
                del temp[bounds]
            yield temp</code></pre>
</details>
</dd>
<dt id="refinery.shell.sorted"><code class="flex name class">
<span>class <span class="ident">sorted</span></span>
<span>(</span><span>key=None, ascending=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Sorts all elements of the input <code><a title="refinery.lib.frame" href="lib/frame.html">refinery.lib.frame</a></code> lexicographically.
This unit is a <code><a title="refinery.nop" href="index.html#refinery.nop">nop</a></code> on single inputs.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/meta/sorted.py#L9-L51" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class sorted(Unit):
    &#34;&#34;&#34;
    Sorts all elements of the input `refinery.lib.frame` lexicographically.
    This unit is a `refinery.nop` on single inputs.
    &#34;&#34;&#34;

    def __init__(
        self,
        key: Arg(&#39;key&#39;, type=str, help=&#39;A meta variable expression to sort by instead of sorting the content.&#39;) = None,
        ascending: Arg.Switch(&#39;-a&#39;, help=&#39;Sort in ascending order, the default is descending.&#39;) = False
    ):
        super().__init__(key=key, ascending=ascending)

    def filter(self, chunks):
        sortbuffer = []
        invisibles = []
        key = self.args.key
        rev = not self.args.ascending

        if key is not None:
            def _key(chunk):
                return expression(metavars(chunk)), chunk
            expression = PythonExpression(key, all_variables_allowed=True)
            key = _key

        def sorted():
            if not sortbuffer:
                return
            sortbuffer.sort(key=key, reverse=rev)
            yield from sortbuffer
            sortbuffer.clear()

        for chunk in chunks:
            if chunk.visible:
                yield from invisibles
                invisibles.clear()
                sortbuffer.append(chunk)
            else:
                yield from sorted()
                invisibles.append(chunk)

        yield from invisibles
        yield from sorted()</code></pre>
</details>
</dd>
<dt id="refinery.shell.sosemanuk"><code class="flex name class">
<span>class <span class="ident">sosemanuk</span></span>
<span>(</span><span>key, stateful=False, discard=0, nonce=b'')</span>
</code></dt>
<dd>
<section class="desc"></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/crypto/cipher/sosemanuk.py#L2220-L2229" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class sosemanuk(StreamCipherUnit):

    def __init__(
        self, key, stateful=False, discard=0,
        nonce: Arg(help=&#39;The nonce. Default is empty, which is equivalent to 16 null bytes.&#39;) = B&#39;&#39;,
    ):
        super().__init__(key=key, nonce=nonce, stateful=stateful, discard=discard)

    def keystream(self):
        yield from Sosemanuk(self.args.key, self.args.nonce)</code></pre>
</details>
</dd>
<dt id="refinery.shell.speck"><code class="flex name class">
<span>class <span class="ident">speck</span></span>
<span>(</span><span>key, iv=b'', padding=None, mode=None, raw=False, block_size=16, *, assoc_len=0, mac_len=0, segment_size=0, little_endian=False)</span>
</code></dt>
<dd>
<section class="desc"><p>SPECK encryption and decryption. It supports block sizes of 8 and 16 bytes.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/crypto/cipher/speck.py#L87-L103" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class speck(StandardBlockCipherUnit, cipher=BlockCipherFactory(Speck)):
    &#34;&#34;&#34;
    SPECK encryption and decryption. It supports block sizes of 8 and 16 bytes.
    &#34;&#34;&#34;
    def __init__(
        self, key, iv=b&#39;&#39;, padding=None, mode=None, raw=False,
        block_size: Arg.Number(&#39;-b&#39;, help=&#39;Cipher block size, default is {default}. Valid choices are 8 and 16.&#39;) = 16,
        **more
    ):
        return super().__init__(key, iv, padding=padding, mode=mode, raw=raw, block_size=block_size, **more)

    @property
    def block_size(self):
        return self.args.block_size

    def _new_cipher(self, **optionals) -&gt; CipherInterface:
        return super()._new_cipher(block_size=self.args.block_size, **optionals)</code></pre>
</details>
</dd>
<dt id="refinery.shell.stego"><code class="flex name class">
<span>class <span class="ident">stego</span></span>
<span>(</span><span>transpose, split=False, parts='RGB')</span>
</code></dt>
<dd>
<section class="desc"><p>Decodes the RGBA (red/green/blue/alpha) values of the pixels of a given image file and outputs
these values as bytes. By default, the pixels are converted left to right, top to bottom.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/formats/stego.py#L16-L64" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class stego(Unit):
    &#34;&#34;&#34;
    Decodes the RGBA (red/green/blue/alpha) values of the pixels of a given image file and outputs
    these values as bytes. By default, the pixels are converted left to right, top to bottom.
    &#34;&#34;&#34;
    def __init__(
        self,
        transpose: Arg.Switch(&#39;-t&#39;, help=&#39;Return the columns of the image rather than the rows.&#39;),
        split: Arg.Switch(&#39;-m&#39;, help=&#39;Emit the individual rows or columns as separate outputs.&#39;) = False,
        parts: Arg(&#39;parts&#39;, nargs=&#39;?&#39;, type=str, help=(
            &#39;A string containing any ordering of the letters R, G, B, and A (case-insensitive). &#39;
            &#39;These pixel components will be extracted from every pixel in the given order. The &#39;
            &#39;default value is {default}.&#39;
        )) = &#39;RGB&#39;
    ):
        super().__init__(
            transpose=transpose,
            split=split,
            parts=tuple(Arg.AsOption(p, PIXEL_PART) for p in parts)
        )

    @Unit.Requires(&#39;Pillow&#39;, &#39;formats&#39;)
    def _image():
        from PIL import Image
        return Image

    def process(self, data):
        split = self.args.split
        parts = self.args.parts
        image = self._image.open(MemoryFile(data))
        if self.args.transpose:
            image = image.transpose(self._image.Transpose.ROTATE_90)
        width, height = image.size
        chunk_size = len(parts)
        output = MemoryFile()
        buffer = bytearray(chunk_size * width)
        for y in range(height):
            offset = 0
            for x in range(width):
                pixel = image.getpixel((x, y))
                next_offset = offset + chunk_size
                buffer[offset:next_offset] = (pixel[p] for p in parts)
                offset = next_offset
            if split:
                yield buffer
            else:
                output.write(buffer)
        if not split:
            yield output.getvalue()</code></pre>
</details>
</dd>
<dt id="refinery.shell.stretch"><code class="flex name class">
<span>class <span class="ident">stretch</span></span>
<span>(</span><span>*count)</span>
</code></dt>
<dd>
<section class="desc"><p>Stretch the input data by repeating every byte a number of times.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/strings/stretch.py#L8-L39" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class stretch(Unit):
    &#34;&#34;&#34;
    Stretch the input data by repeating every byte a number of times.
    &#34;&#34;&#34;
    def __init__(self, *count: Arg.Number(metavar=&#39;count&#39;, help=(
        &#39;The number of times every byte should be repeated. By default,  &#39;
        &#39;every byte is repeated once.&#39;
    ))):
        count = count or (2,)
        if any(k &lt;= 0 for k in count):
            raise ValueError(&#39;You can not use a stretching factor of less than 1.&#39;)
        super().__init__(count=count or (2,))

    def process(self, data):
        def stretched(it):
            factor = cycle(self.args.count)
            for byte in it:
                yield from repeat(byte, next(factor))
        return bytearray(stretched(iter(data)))

    def reverse(self, data):
        # one-sided inverse
        def clinched(it):
            factor = cycle(self.args.count)
            while True:
                try:
                    take = islice(it, next(factor))
                    yield next(take)
                    for _ in take: pass
                except StopIteration:
                    break
        return bytearray(clinched(iter(data)))</code></pre>
</details>
</dd>
<dt id="refinery.shell.struct"><code class="flex name class">
<span>class <span class="ident">struct</span></span>
<span>(</span><span>spec, *outputs, multi=False, count=, until=None, field=None, more=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Read structured data from the beginning of a chunk and store the extracted fields in chunk meta
variables. The structure format is specified in extended Python struct format, and all
remaining arguments to this unit are the names of the variables that receive the values from
this struct. The extended struct format supports all field types supported by Python, as well
as the following:</p>
<ul>
<li><code>a</code> for null-terminated ASCII strings,</li>
<li><code>u</code> to read encoded, null-terminated UTF16 strings,</li>
<li><code>w</code> to read decoded, null-terminated UTF16 strings,</li>
<li><code>g</code> to read Microsoft GUID values,</li>
<li><code>E</code> to read 7-bit encoded integers.</li>
</ul>
<p>For example, the string <code>LLxxHaa</code> will read two unsigned 32bit integers, then skip two bytes,
then read one unsigned 16bit integer, then two null-terminated ASCII strings. The unit defaults
to using native byte order with no alignment. The <code>spec</code> parameter may additionally contain
format expressions of the following form:</p>
<p>{name[!alignment]:format}</p>
<p>The <code>alignment</code> parameter is optional. It must be an expression that evaluates to an integer
value. The current data pointer is aligned to a multiple of this value before reading the field.
The <code>format</code> can either be an integer expression specifying a number of bytes to read, or any
format string. If <code>name</code> is specified for an extracted field, its value is made available as a
meta variable under the given name. For example, the expression <code>LLxxH{foo:a}{bar:a}</code> would be
parsed in the same way as the previous example, but the two ASCII strings would also be stored
in meta variables under the names <code>foo</code> and <code>bar</code>, respectively. The <code>format</code> string of a named
field is itself parsed as a foramt string expression, where all the previously parsed fields
are already available. For example, <code>I{:{}}</code> reads a single 32-bit integer length prefix and
then reads as many bytes as that prefix specifies.</p>
<p>A second format string expression is used to specify the output format. For example, the format
string <code>LLxxH{foo:a}{bar:a}</code> together with the output format <code>{foo}/{bar}</code> would parse data as
before, but the output body would be the concatnation of the field <code>foo</code>, a forward slash, and
the field <code>bar</code>. Variables used in the output expression are not included as meta variables. As
format fields in the output expression, one can also use <code>{1}</code>, <code>{2}</code> or <code>{-1}</code> to access
extracted fields by index. The value <code>{0}</code> represents the entire chunk of structured data. By
default, the output format <code>{#}</code> is used, which represents either the last byte string field
that was extracted, or the entire chunk of structured data if none of the fields were extracted.</p>
<p>Reverse <code><a title="refinery.lib.argformats.multibin" href="lib/argformats.html#refinery.lib.argformats.multibin">multibin()</a></code> expressions can be used to post-process the fields
included in any output format. For example, <code>{F:b64:zl}</code> will be the base64-decoded and inflate-
decompressed contents of the data that was read as field <code>F</code>.</p>
<p>Finally, it is possible to specify a byte alignment by using the syntax <code>{field!T:a:b:c}</code> where
the letter <code>T</code> is either a single digit specifying the alignment, or a single letter variable
that holds the byte alignment value in the current metadata.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/pattern/struct_parser.py#L21-L234" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class struct(Unit):
    &#34;&#34;&#34;
    Read structured data from the beginning of a chunk and store the extracted fields in chunk meta
    variables. The structure format is specified in extended Python struct format, and all
    remaining arguments to this unit are the names of the variables that receive the values from
    this struct. The extended struct format supports all field types supported by Python, as well
    as the following:

    - `a` for null-terminated ASCII strings,
    - `u` to read encoded, null-terminated UTF16 strings,
    - `w` to read decoded, null-terminated UTF16 strings,
    - `g` to read Microsoft GUID values,
    - `E` to read 7-bit encoded integers.

    For example, the string `LLxxHaa` will read two unsigned 32bit integers, then skip two bytes,
    then read one unsigned 16bit integer, then two null-terminated ASCII strings. The unit defaults
    to using native byte order with no alignment. The `spec` parameter may additionally contain
    format expressions of the following form:

    {name[!alignment]:format}

    The `alignment` parameter is optional. It must be an expression that evaluates to an integer
    value. The current data pointer is aligned to a multiple of this value before reading the field.
    The `format` can either be an integer expression specifying a number of bytes to read, or any
    format string. If `name` is specified for an extracted field, its value is made available as a
    meta variable under the given name. For example, the expression `LLxxH{foo:a}{bar:a}` would be
    parsed in the same way as the previous example, but the two ASCII strings would also be stored
    in meta variables under the names `foo` and `bar`, respectively. The `format` string of a named
    field is itself parsed as a foramt string expression, where all the previously parsed fields
    are already available. For example, `I{:{}}` reads a single 32-bit integer length prefix and
    then reads as many bytes as that prefix specifies.

    A second format string expression is used to specify the output format. For example, the format
    string `LLxxH{foo:a}{bar:a}` together with the output format `{foo}/{bar}` would parse data as
    before, but the output body would be the concatnation of the field `foo`, a forward slash, and
    the field `bar`. Variables used in the output expression are not included as meta variables. As
    format fields in the output expression, one can also use `{1}`, `{2}` or `{-1}` to access
    extracted fields by index. The value `{0}` represents the entire chunk of structured data. By
    default, the output format `{#}` is used, which represents either the last byte string field
    that was extracted, or the entire chunk of structured data if none of the fields were extracted.

    Reverse `refinery.lib.argformats.multibin` expressions can be used to post-process the fields
    included in any output format. For example, `{F:b64:zl}` will be the base64-decoded and inflate-
    decompressed contents of the data that was read as field `F`.

    Finally, it is possible to specify a byte alignment by using the syntax `{field!T:a:b:c}` where
    the letter `T` is either a single digit specifying the alignment, or a single letter variable
    that holds the byte alignment value in the current metadata.
    &#34;&#34;&#34;

    def __init__(
        self,
        spec: Arg(type=str, help=&#39;Structure format as explained above.&#39;),
        *outputs: Arg(metavar=&#39;output&#39;, type=str, help=&#39;Output format as explained above.&#39;),
        multi: Arg.Switch(&#39;-m&#39;, help=(
            &#39;Read as many pieces of structured data as possible intead of just one.&#39;)) = False,
        count: Arg.Number(&#39;-n&#39;, help=(
            &#39;A limit on the number of chunks to read in multi mode; default is {default}.&#39;)) = INF,
        until: Arg(&#39;-u&#39;, metavar=&#39;E&#39;, type=str, help=(
            &#39;An expression evaluated on each chunk in multi mode. New chunks will be parsed &#39;
            &#39;only if the result is nonzero.&#39;)) = None,
        field: Arg.String(&#39;-f&#39;, help=(
            &#39;Optionally specify a format string expression to auto-name extracted fields without a &#39;
            &#39;given name based on their position.&#39;)) = None,
        more : Arg.Switch(&#39;-M&#39;, help=(
            &#39;After parsing the struct, emit one chunk that contains the data that was left &#39;
            &#39;over in the buffer. If no data was left over, this chunk will be empty.&#39;)) = False
    ):
        outputs = outputs or [F&#39;{{{_SHARP}}}&#39;]
        super().__init__(spec=spec, outputs=outputs, until=until, field=field, count=count, multi=multi, more=more)

    def process(self, data: Chunk):
        formatter = string.Formatter()
        until = self.args.until
        until = until and PythonExpression(until, all_variables_allowed=True)
        reader = StructReader(memoryview(data))
        checkpoint = 0
        mainspec = self.args.spec
        byteorder = mainspec[:1]
        if byteorder in &#39;&lt;@=!&gt;&#39;:
            mainspec = mainspec[1:]
        else:
            byteorder = &#39;=&#39;

        def fixorder(spec):
            if spec[0] not in &#39;&lt;@=!&gt;&#39;:
                spec = byteorder + spec
            return spec

        previously_existing_variables = set(metavars(data).variable_names())

        it = itertools.count() if self.args.multi else (0,)
        for index in it:

            checkpoint = reader.tell()

            if reader.eof:
                break
            if index &gt;= self.args.count:
                break

            meta = metavars(data)
            meta.ghost = True
            meta.update_index(index)

            args = []
            last = None
            self.log_debug(F&#39;starting new read at: 0x{checkpoint:08X}&#39;)

            try:
                for prefix, name, spec, conversion in formatter.parse(mainspec):
                    name: str
                    spec: str = spec and spec.strip()
                    if prefix:
                        fields = reader.read_struct(fixorder(prefix))
                        if fmt := self.args.field:
                            for k, field in enumerate(fields, len(args)):
                                meta[fmt.format(k)] = field
                        args.extend(fields)
                    if name is None:
                        continue
                    if name and not name.isdecimal():
                        check_variable_name(name)
                    if conversion:
                        _aa = reader.tell()
                        reader.byte_align(PythonExpression.Evaluate(conversion, meta))
                        _ab = reader.tell()
                        if _aa != _ab:
                            self.log_info(F&#39;aligned from 0x{_aa:X} to 0x{_ab:X}&#39;)
                    spec, _, pipeline = spec.partition(&#39;:&#39;)
                    if spec:
                        spec = meta.format_str(spec, self.codec, args)
                    if spec:
                        try:
                            _exp = PythonExpression.Evaluate(spec, meta)
                        except ParserError:
                            pass
                        else:
                            spec = _exp
                    if spec == &#39;&#39;:
                        last = value = reader.read()
                    elif isinstance(spec, int):
                        if spec &lt; 0:
                            spec += reader.remaining_bytes
                        if spec &lt; 0:
                            raise ValueError(F&#39;The specified negative read offset is {-spec} beyond the cursor.&#39;)
                        last = value = reader.read_bytes(spec)
                    else:
                        value = reader.read_struct(fixorder(spec))
                        if not value:
                            self.log_debug(F&#39;field {name} was empty, ignoring.&#39;)
                            continue
                        if len(value) &gt; 1:
                            self.log_info(F&#39;parsing field {name} produced {len(value)} items reading a tuple&#39;)
                        else:
                            value = value[0]

                    if pipeline:
                        value = numseq(pipeline, reverse=True, seed=value)
                    args.append(value)

                    if name == _SHARP:
                        raise ValueError(&#39;Extracting a field with name # is forbidden.&#39;)
                    elif name.isdecimal():
                        index = int(name)
                        limit = len(args) - 1
                        if index &gt; limit:
                            self.log_warn(F&#39;cannot assign index field {name}, the highest index is {limit}&#39;)
                        else:
                            args[index] = value
                        continue
                    elif name:
                        meta[name] = value

                if until and until(meta):
                    self.log_info(F&#39;the expression ({until}) evaluated to true; aborting.&#39;)
                    break

                with StreamDetour(reader, checkpoint) as detour:
                    full = reader.read(detour.cursor - checkpoint)
                if last is None:
                    last = full

                outputs = []
                symbols = dict(meta)
                symbols[_SHARP] = last

                for template in self.args.outputs:
                    used = set()
                    outputs.append(meta.format(template, self.codec, [full, *args], symbols, True, used=used))
                    for key in used:
                        if key in previously_existing_variables:
                            continue
                        meta.discard(key)

                for output in outputs:
                    chunk = Chunk(output)
                    chunk.meta.update(meta)
                    chunk.set_next_batch(index)
                    yield chunk

            except EOFError:
                break

        leftover = len(reader) - checkpoint

        if not leftover:
            return
        elif self.args.more:
            reader.seekset(checkpoint)
            yield reader.read()
        else:
            leftover = repr(SizeInt(leftover)).strip()
            self.log_info(F&#39;discarding {leftover} left in buffer&#39;)</code></pre>
</details>
</dd>
<dt id="refinery.shell.sub"><code class="flex name class">
<span>class <span class="ident">sub</span></span>
<span>(</span><span>argument, bigendian=False, blocksize=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Subtract the given argument from each block.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/blockwise/sub.py#L6-L13" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class sub(BinaryOperationWithAutoBlockAdjustment):
    &#34;&#34;&#34;
    Subtract the given argument from each block.
    &#34;&#34;&#34;
    @staticmethod
    def operate(a, b): return a - b
    @staticmethod
    def inplace(a, b): a -= b</code></pre>
</details>
</dd>
<dt id="refinery.shell.subfiles"><code class="flex name class">
<span>class <span class="ident">subfiles</span></span>
<span>(</span><span>memdump=False, recursive=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Deploys carvers for ZIP, 7-Zip, PE-File, Windows Shortcuts (LNK files), JSON and XML documents against
the input data and generates one output chunk for each successfully carved subfile.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/pattern/subfiles.py#L14-L59" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class subfiles(Unit):
    &#34;&#34;&#34;
    Deploys carvers for ZIP, 7-Zip, PE-File, Windows Shortcuts (LNK files), JSON and XML documents against
    the input data and generates one output chunk for each successfully carved subfile.
    &#34;&#34;&#34;

    _MINLENGTH = {
        &#39;json&#39;: 300,
        &#39;xml&#39; : 300,
        &#39;rtf&#39; : 100,
    }

    def __init__(
        self,
        memdump  : Unit.Arg.Switch(&#39;-m&#39;,
            help=&#39;Assume that the input is a memdump for PE file carving.&#39;) = False,
        recursive: Unit.Arg.Switch(&#39;-r&#39;,
            help=&#39;Extract files that are subfiles of other extracted files as separate chunks.&#39;) = False,
    ):
        super().__init__(memdump=memdump, recursive=recursive)

    def process(self, data: bytearray):
        carvers = {
            &#39;zip&#39;  : carve_zip(),
            &#39;7z&#39;   : carve_7z(),
            &#39;pe&#39;   : carve_pe(memdump=self.args.memdump, fileinfo=True, recursive=True, keep_root=True),
            &#39;lnk&#39;  : carve_lnk(),
            &#39;json&#39; : carve_json(dictonly=True),
            &#39;xml&#39;  : carve_xml(),
            &#39;rtf&#39;  : carve_rtf(),
        }

        covered = []

        for extension, unit in carvers.items():
            self.log_info(F&#39;carving {extension} files&#39;)
            for chunk in data | unit:
                if len(chunk) &lt; self._MINLENGTH.get(extension, 1):
                    continue
                start = chunk[&#39;offset&#39;]
                end = start + len(chunk)
                if any(start &gt; left and end &lt; right for left, right in covered):
                    continue
                if not self.args.recursive:
                    covered.append((start, end))
                yield chunk</code></pre>
</details>
</dd>
<dt id="refinery.shell.swap"><code class="flex name class">
<span>class <span class="ident">swap</span></span>
<span>(</span><span>src, dst=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Swap the contents of an existing variable with the contents of the chunk or with another meta variable.
When swapping with the chunk, the variable has to contain a binary string. When swapping with a variable
that does not exist, the original variable is cleared, essentially renaming the variable.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/meta/swap.py#L12-L59" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class swap(Unit):
    &#34;&#34;&#34;
    Swap the contents of an existing variable with the contents of the chunk or with another meta variable.
    When swapping with the chunk, the variable has to contain a binary string. When swapping with a variable
    that does not exist, the original variable is cleared, essentially renaming the variable.
    &#34;&#34;&#34;
    def __init__(
        self,
        src: Arg(type=str, help=&#39;The meta variable name.&#39;),
        dst: Arg(type=str, help=&#39;Optional name of the second meta variable.&#39;) = None
    ):
        super().__init__(
            src=check_variable_name(src),
            dst=check_variable_name(dst)
        )

    def filter(self, chunks: Iterable[Chunk]):
        src = self.args.src
        dst = self.args.dst
        for chunk in chunks:
            if not chunk.visible:
                pass
            elif dst is None:
                try:
                    value = chunk.meta[src]
                except KeyError:
                    value = bytearray()
                if isinstance(value, str):
                    value = value.encode(self.codec)
                elif not isbuffer(value):
                    raise ValueError(F&#39;Unable to swap data with variable {src} because it has type {type(value).__name__}.&#39;)
                if not chunk:
                    chunk.meta.discard(src)
                else:
                    chunk.meta[src] = bytes(chunk)
                chunk[:] = value
            else:
                try:
                    value = chunk.meta.pop(src)
                except KeyError:
                    raise KeyError(F&#39;The variable {src} does not exist.&#39;)
                try:
                    swap = chunk.meta.pop(dst)
                except KeyError:
                    chunk.meta[dst] = value
                else:
                    chunk.meta[src], chunk.meta[dst] = swap, value
            yield chunk</code></pre>
</details>
</dd>
<dt id="refinery.shell.szdd"><code class="flex name class">
<span>class <span class="ident">szdd</span></span>
</code></dt>
<dd>
<section class="desc"><p>Extract files from SZDD archives.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/compression/szdd.py#L7-L56" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class szdd(Unit):
    &#34;&#34;&#34;
    Extract files from SZDD archives.
    &#34;&#34;&#34;
    def process(self, data):
        with StructReader(data) as archive:
            if archive.read(8) != b&#39;SZDD\x88\xF0\x27\x33&#39;:
                if not self.args.lenient:
                    raise ValueError(&#39;signature missing&#39;)
                self.log_warn(&#39;the header signature is invalid, this is likely not an SZDD archive&#39;)
            if archive.read_byte() != 0x41:
                raise ValueError(&#39;Unsupported compression mode&#39;)
            # ignore the missing file extension letter:
            archive.seekrel(1)
            output_len = archive.u32()
            window_pos = 0x1000 - 0x10
            output_pos = 0
            output = bytearray(output_len)
            window = bytearray(0x1000)
            for k in range(len(window)):
                window[k] = 0x20
            while not archive.eof:
                control = archive.read_byte()
                for cb in (0x01, 0x02, 0x04, 0x08, 0x10, 0x20, 0x40, 0x80):
                    if archive.eof:
                        break
                    if control &amp; cb:
                        output[output_pos] = window[window_pos] = archive.read_byte()
                        output_pos += 1
                        window_pos += 1
                        window_pos &amp;= 0xFFF
                    else:
                        match_pos = archive.read_byte()
                        match_len = archive.read_byte()
                        match_pos |= (match_len &amp; 0xF0) &lt;&lt; 4
                        match_len = (match_len &amp; 0x0F) + 3
                        match_pos &amp;= 0xFFF
                        for _ in range(match_len):
                            window[window_pos] = window[match_pos]
                            output[output_pos] = window[window_pos]
                            output_pos += 1
                            window_pos += 1
                            match_pos += 1
                            window_pos &amp;= 0xFFF
                            match_pos &amp;= 0xFFF
            return output

    @classmethod
    def handles(self, data: bytearray):
        return data[:4] == B&#39;SZDD&#39;</code></pre>
</details>
</dd>
<dt id="refinery.shell.tea"><code class="flex name class">
<span>class <span class="ident">tea</span></span>
<span>(</span><span>key, iv=b'', padding=None, mode=None, raw=False, swap=False)</span>
</code></dt>
<dd>
<section class="desc"><p>TEA encryption and decryption.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/crypto/cipher/tea.py#L89-L92" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class tea(TEAUnit, cipher=BlockCipherFactory(TEA)):
    &#34;&#34;&#34;
    TEA encryption and decryption.
    &#34;&#34;&#34;</code></pre>
</details>
</dd>
<dt id="refinery.shell.termfit"><code class="flex name class">
<span>class <span class="ident">termfit</span></span>
<span>(</span><span>width=0, delta=0, tight=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Reformat incoming text data to fit a certain width.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/strings/termfit.py#L8-L24" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class termfit(Unit):
    &#34;&#34;&#34;
    Reformat incoming text data to fit a certain width.
    &#34;&#34;&#34;

    def __init__(
        self,
        width: Arg(&#39;width&#39;, help=&#39;Optionally specify the width, by default the current terminal width is used.&#39;) = 0,
        delta: Arg.Number(&#39;-d&#39;, help=&#39;Subtract this number from the calculated width (0 by default).&#39;) = 0,
        tight: Arg.Switch(&#39;-t&#39;, help=&#39;Separate paragraphs by a single line break instead of two.&#39;) = False,
    ):
        super().__init__(width=width, delta=delta, tight=tight)

    @unicoded
    def process(self, data: str) -&gt; str:
        parsep = &#39;\n&#39; if self.args.tight else &#39;\n\n&#39;
        return terminalfit(data, self.args.delta, self.args.width, parsep)</code></pre>
</details>
</dd>
<dt id="refinery.shell.terminate"><code class="flex name class">
<span>class <span class="ident">terminate</span></span>
<span>(</span><span>sentinel=b'\x00', blocksize=None, bigendian=False)</span>
</code></dt>
<dd>
<section class="desc"><p>The unit reads data from the incoming chunk in blocks of any given size until the
sentinel value is encountered. The output of the unit is all data that was read,
excluding the sentinel. The default block size is one and the default sentinel value
is zero, which corresponds to reading a null-terminated string from the input.
If the sentinel value is not found anywhere in the incoming data, the complete input
is returned as output.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/blockwise/terminate.py#L6-L57" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class terminate(BlockTransformationBase):
    &#34;&#34;&#34;
    The unit reads data from the incoming chunk in blocks of any given size until the
    sentinel value is encountered. The output of the unit is all data that was read,
    excluding the sentinel. The default block size is one and the default sentinel value
    is zero, which corresponds to reading a null-terminated string from the input.
    If the sentinel value is not found anywhere in the incoming data, the complete input
    is returned as output.
    &#34;&#34;&#34;
    def __init__(
        self,
        sentinel: Arg(help=&#39;sentinel value to look for; default is {default}&#39;) = B&#39;\0&#39;,
        blocksize=None, bigendian=False
    ):
        super().__init__(blocksize=blocksize, bigendian=bigendian, sentinel=sentinel)

    def process(self, data: bytearray):
        sentinel = self.args.sentinel
        position = 0
        blocksize = self.blocksize

        self.log_info(&#39;blocksize:&#39;, blocksize)
        self.log_debug(&#39;separator:&#39;, sentinel)

        while position &gt;= 0:
            position = data.find(sentinel, position)
            if position &lt; 0:
                self.log_info(F&#39;The sentinel value {sentinel} was not found.&#39;)
                break
            q, r = divmod(position, blocksize)
            if r:
                position = (q + 1) * blocksize
                continue
            else:
                data[position:] = []
                break

        return data

    def reverse(self, data: bytearray):
        sentinel = self.args.sentinel
        position = 0
        while True:
            position = data.find(sentinel, position)
            if position &lt; 0:
                data.extend(sentinel)
                break
            if position % self.blocksize == 0:
                self.log_warn(&#39;input string already contains the termination character; returning unmodified input&#39;)
                break
            position += 1
        return data</code></pre>
</details>
</dd>
<dt id="refinery.shell.transpose"><code class="flex name class">
<span>class <span class="ident">transpose</span></span>
<span>(</span><span>padding=b'')</span>
</code></dt>
<dd>
<section class="desc"><p>Interprets the chunks in the current frame as rows of a matrix and yields the columns
of that matrix. When chunks are not of even length, the matrix is considered to have
empty entries in some positions. Optionally, a padding sequence can be provided to pad
all rows to the same length.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/meta/transpose.py#L7-L66" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class transpose(Unit):
    &#34;&#34;&#34;
    Interprets the chunks in the current frame as rows of a matrix and yields the columns
    of that matrix. When chunks are not of even length, the matrix is considered to have
    empty entries in some positions. Optionally, a padding sequence can be provided to pad
    all rows to the same length.
    &#34;&#34;&#34;
    @Unit.Requires(&#39;numpy&#39;, &#39;speed&#39;, &#39;default&#39;, &#39;extended&#39;)
    def _numpy():
        import numpy
        return numpy

    def __init__(
        self,
        padding: Arg(help=&#39;Optional byte sequence to use as padding for incomplete rows.&#39;) = B&#39;&#39;,
    ):
        super().__init__(bigendian=False, padding=padding)

    def filter(self, chunks: Iterable[Chunk]):
        rows = []
        for chunk in chunks:
            if not chunk.visible:
                yield chunk
                continue
            rows.append(chunk)
        if not rows:
            return
        matrix = rows[0]
        matrix.temp = rows
        yield matrix

    def process(self, data: Chunk):
        chunks: List[Chunk] = data.temp
        if not chunks:
            return
        length = [len(chunk) for chunk in chunks]
        n = min(length)
        m = max(length)
        pad = self.args.padding
        if pad:
            for chunk in chunks:
                while len(chunk) &lt; m:
                    chunk.extend(pad)
                del chunk[m:]
        if n &gt; 0:
            try:
                np = self._numpy
            except ImportError:
                pass
            else:
                t = [chunk[n:] for chunk in chunks if len(chunk) &gt; n]
                for chunk in chunks:
                    del chunk[n:]
                a = np.array(chunks, dtype=np.uint8).transpose()
                for row in a:
                    yield row.tobytes(&#39;C&#39;)
                m = m - n
                chunks = t
        for i in range(m):
            yield bytes(chunk[i] for chunk in chunks if len(chunk) &gt; i)</code></pre>
</details>
</dd>
<dt id="refinery.shell.trim"><code class="flex name class">
<span>class <span class="ident">trim</span></span>
<span>(</span><span>*junk, unpad=False, left=True, right=True, nocase=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Removes byte sequences at beginning and end of input data.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/strings/trim.py#L8-L86" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class trim(Unit):
    &#34;&#34;&#34;
    Removes byte sequences at beginning and end of input data.
    &#34;&#34;&#34;

    def __init__(
        self, *junk: Arg(help=&#39;Binary strings to be removed, default are all whitespace characters.&#39;),
        unpad: Arg.Switch(&#39;-u&#39;, help=&#39;Also trim partial occurrences of the junk string.&#39;) = False,
        left: Arg.Switch(&#39;-r&#39;, &#39;--right-only&#39;, group=&#39;SIDE&#39;, help=&#39;Do not trim left.&#39;) = True,
        right: Arg.Switch(&#39;-l&#39;, &#39;--left-only&#39;, group=&#39;SIDE&#39;, help=&#39;Do not trim right.&#39;) = True,
        nocase: Arg.Switch(&#39;-i&#39;, help=&#39;Ignore capitalization for alphabetic characters.&#39;) = False,
    ):
        super().__init__(junk=junk, left=left, right=right, unpad=unpad, nocase=nocase)

    def _trimfast(self, view: memoryview, *junks: bytes, right=False) -&gt; Tuple[bool, memoryview]:
        done = False
        pos = 0
        while not done:
            done = True
            for junk in junks:
                temp = junk
                size = len(junk)
                if right and self.args.unpad:
                    for k in range(size):
                        n = size - k
                        if view[pos:pos + n] == junk[k:]:
                            pos += n
                            done = False
                            break
                if view[pos:pos + size] == temp:
                    m = len(temp)
                    while True:
                        mm = m &lt;&lt; 1
                        if view[pos + m:pos + mm] != temp:
                            break
                        temp += temp
                        m = mm
                    temp = memoryview(temp)
                    while m &gt;= size:
                        if view[pos:pos + m] == temp[:m]:
                            done = False
                            pos += m
                        m //= 2
                if right or not self.args.unpad:
                    continue
                while size &gt; 0:
                    if view[pos:pos + size] == temp[:size]:
                        done = False
                        pos += size
                        break
                    size -= 1
        return pos

    def process(self, data: bytearray):
        junk = list(self.args.junk)
        if not junk:
            import string
            space = string.whitespace.encode(&#39;ascii&#39;)
            junk = [space[k - 1:k] for k in range(1, len(space))]
        lpos = 0
        rpos = 0
        if self.args.nocase:
            work = data.lower()
            junk = [j.lower() for j in junk]
        else:
            work = data
        if self.args.left:
            lpos = self._trimfast(memoryview(work), *junk)
        if self.args.right:
            work.reverse()
            junk = [bytes(reversed(j)) for j in junk]
            rpos = self._trimfast(memoryview(work), *junk, right=True)
            work.reverse()
        view = memoryview(data)
        if lpos:
            view = view[+lpos:]
        if rpos:
            view = view[:-rpos]
        return view</code></pre>
</details>
</dd>
<dt id="refinery.shell.u16"><code class="flex name class">
<span>class <span class="ident">u16</span></span>
</code></dt>
<dd>
<section class="desc"><p>Encodes and decodes UTF-16 encoded string data.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/encoding/u16.py#L6-L25" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class u16(Unit):
    &#34;&#34;&#34;
    Encodes and decodes UTF-16 encoded string data.
    &#34;&#34;&#34;

    def reverse(self, data):
        return data.decode(self.codec).encode(&#39;utf-16LE&#39;)

    def process(self, data):
        return data.decode(&#39;utf-16&#39;).encode(self.codec)

    @classmethod
    def handles(self, data: bytearray):
        view = memoryview(data)
        if len(view) % 2 != 0:
            return False
        if not any(view[1:0x100:2]):
            return True
        if not any(view[0:0x100:2]):
            return any(view[:4])</code></pre>
</details>
</dd>
<dt id="refinery.shell.ucrypt"><code class="flex name class">
<span>class <span class="ident">ucrypt</span></span>
<span>(</span><span>size=13, salt=b'AA')</span>
</code></dt>
<dd>
<section class="desc"><p>Implements the classic Unix crypt algorithm.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/crypto/keyderive/unixcrypt.py#L336-L354" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class ucrypt(KeyDerivation):
    &#34;&#34;&#34;
    Implements the classic Unix crypt algorithm.
    &#34;&#34;&#34;
    def __init__(
        self,
        size: Arg(help=&#39;The number of bytes to generate, default is 13.&#39;) = 13,
        salt: Arg(help=&#39;Salt for the derivation, the default is &#34;AA&#34;.&#39;) = B&#39;AA&#39;
    ):
        super().__init__(size=size, salt=salt)

    def process(self, data):
        crypted = bytes(UnixCrypt(data, salt=self.args.salt))
        if len(crypted) &lt; self.args.size:
            raise RefineryPartialResult(
                F&#39;unix crypt only provided {len(crypted)} bytes, but {self.args.size} &#39;
                F&#39;were requested.&#39;, partial=crypted
            )
        return crypted[:self.args.size]</code></pre>
</details>
</dd>
<dt id="refinery.shell.url"><code class="flex name class">
<span>class <span class="ident">url</span></span>
<span>(</span><span>plus=False, hex=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Decodes and encodes URL-encoding, which preserves only alphanumeric characters and the
following symbols: <code>_</code>, <code>.</code>, <code>-</code>, <code>~</code>, <code>\</code>, <code>/</code>. Every other character is escaped by
hex-encoding it and prefixing it with a percent symbol.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/encoding/url.py#L9-L50" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class url(Unit):
    &#34;&#34;&#34;
    Decodes and encodes URL-encoding, which preserves only alphanumeric characters and the
    following symbols: `_`, `.`, `-`, `~`, `\\`, `/`. Every other character is escaped by
    hex-encoding it and prefixing it with a percent symbol.
    &#34;&#34;&#34;

    def __init__(
        self,
        plus: Arg.Switch(&#39;-p&#39;, help=&#39;also replace plus signs by spaces&#39;) = False,
        hex : Arg.Switch(&#39;-x&#39;, help=&#39;hex encode every character in reverse mode&#39;) = False
    ):
        super().__init__(plus=plus, hex=hex)

    def process(self, data):
        if self.args.plus:
            data = data.replace(B&#39;+&#39;, B&#39; &#39;)
        data = unquote_to_bytes(bytes(data))
        data = re.sub(
            B&#39;%[uU]([0-9a-fA-F]{4})&#39;,
            lambda m: int(m[1], 16).to_bytes(2, &#39;little&#39;),
            data)
        return data

    def reverse(self, data):
        if self.args.hex:
            result = bytearray(len(data) * 3)
            offset = 0
            for byte in data:
                result[offset + 0] = 0x25
                offset += 1
                result[offset:offset + 2] = B&#39;%02X&#39; % byte
                offset += 2
            return result
        elif self.args.plus:
            def replace(m):
                c = m[0][0]
                return b&#39;+&#39; if c == 0x20 else B&#39;%%%02X&#39; % c
        else:
            def replace(m):
                return B&#39;%%%02X&#39; % m[0][0]
        return re.sub(B&#39;[^a-zA-Z0-9_.-~\\/]&#39;, replace, data)</code></pre>
</details>
</dd>
<dt id="refinery.shell.urlfix"><code class="flex name class">
<span>class <span class="ident">urlfix</span></span>
<span>(</span><span>meta=False, keep=0)</span>
</code></dt>
<dd>
<section class="desc"><p>Removes fragments, query strings, and parameters from input URLs. It also correctly escapes all
characters in the URL path component and normalizes the network location part to lowercase. Note
that URLs without a scheme will not be recognized as valid URLs; chunks that do not look like a
URL will be swallowed and not return any output.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/misc/urlfix.py#L8-L51" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class urlfix(Unit):
    &#34;&#34;&#34;
    Removes fragments, query strings, and parameters from input URLs. It also correctly escapes all
    characters in the URL path component and normalizes the network location part to lowercase. Note
    that URLs without a scheme will not be recognized as valid URLs; chunks that do not look like a
    URL will be swallowed and not return any output.
    &#34;&#34;&#34;
    def __init__(
        self,
        meta: Arg.Switch(&#39;-m&#39;, help=&#39;Extract the query string parameters as metadata.&#39;) = False,
        keep: Arg.Counts(&#39;-k&#39;, help=(
            &#39;If specified once, keeps the it keeps the URL params and query string. If specified &#39;
            &#39;twice, it keeps the URL fragment as well. At this level, the unit still filters out &#39;
            &#39;anything that does not parse as a URL.&#39;
        )) = 0
    ):
        super().__init__(keep=keep, meta=meta)

    def process(self, data):
        def fix(string):
            return quote(unquote(string))
        keep = self.args.keep
        meta = self.args.meta
        parsed = urlparse(data.decode(self.codec))
        if not parsed.scheme or not parsed.netloc:
            return None
        query_dict = {key: unquote(value) for key, value in parse_qsl(parsed.query)}
        query_string = &#39;&amp;&#39;.join(F&#39;{key}={quote(value)}&#39; for key, value in query_dict.items())
        replacements = dict(
            netloc=parsed.netloc.lower(),
            params=fix(parsed.params),
            path=fix(parsed.path),
            query=query_string,
            fragment=fix(parsed.fragment),
        )
        if keep &lt; 2:
            replacements.update(fragment=&#39;&#39;)
            if keep &lt; 1:
                replacements.update(params=&#39;&#39;, query=&#39;&#39;)
        url = urlunparse(parsed._replace(**replacements))
        url = url.encode(self.codec)
        if meta:
            url = self.labelled(url, **query_dict)
        return url</code></pre>
</details>
</dd>
<dt id="refinery.shell.urlguards"><code class="flex name class">
<span>class <span class="ident">urlguards</span></span>
</code></dt>
<dd>
<section class="desc"><p>Restores the original URLs from their 'protected' versions as generated by
Outlook protection and ProofPoint.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/pattern/urlguards.py#L22-L118" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class urlguards(Unit):
    &#34;&#34;&#34;
    Restores the original URLs from their &#39;protected&#39; versions as generated by
    Outlook protection and ProofPoint.
    &#34;&#34;&#34;

    _PP3RLENC = {
        letter: rl for rl, letter in enumerate(
            &#39;ABCDEFGHIJKLMNOPQRSTUVWXYZ&#39;
            &#39;abcdefghijklmnopqrstuvwxyz&#39;
            &#39;0123456789-_&#39;, 2
        )
    }

    @unguard(r&#39;https?://urldefense(?:\.proofpoint)?\.com/v([12])/url\?([:;/_=!?#&amp;.,\w\%\-\+|]+)&#39;)
    def _proofpointV2(self, match):
        version = int(match[1])
        self.log_info(&#39;proofpoint match:&#39;, version)
        argmatch = re.match(
            R&#39;^u=(.+?)&amp;(?:amp;)?{}=&#39;.format(&#39;k&#39; if version == 1 else &#39;[dc]&#39;),
            match[2],
            flags=re.DOTALL
        )
        if not argmatch:
            self.log_warn(&#39;not able to translate unexpected proofpoint format:&#39;, match)
            return match[0]
        encoded = argmatch[1]
        if match[1] == &#39;2&#39;:
            encoded = encoded.translate(str.maketrans(&#39;-_&#39;, &#39;%/&#39;))
        return unescape(unquote(encoded))

    @unguard(r&#39;https?://urldefense(?:\.proofpoint)?\.com/v3/__(.+?)__;(.*?)![-\w!?$]+&#39;)
    def _proofpointV3(self, match):
        data = unquote(match[1])
        cmap = match[2] + &#39;=&#39; * (-len(match[2]) % 4)
        cmap = urlsafe_b64decode(cmap).decode(&#39;UTF-8&#39;)
        cursor = 0
        result = &#39;&#39;
        for k in range(len(cmap)):
            ast = data.find(&#39;*&#39;, cursor)
            if ast &lt; 0:
                break
            result += data[cursor:ast]
            if data[ast + 1] == &#39;*&#39;:
                end = self._PP3RLENC[data[ast + 2]]
                result += cmap[k:end]
                ast += 2
            else:
                result += cmap[k]
            cursor = ast + 1
        self.log_debug(result)
        self.log_debug(data[cursor:])
        return result + data[cursor:]

    @unguard(r&#39;https?://\w+.safelinks\.protection\.outlook\.com/([:;/_=!?#&amp;.,\w\%\-\+|]+)&#39;)
    def _outlook(self, match):
        result = match[0]
        self.log_info(&#39;outlook match:&#39;, result)
        parsed = urlparse(result)
        params = parse_qs(parsed.query)
        try:
            result = unquote(params[&#39;url&#39;][0])
        except Exception:
            pass
        return result

    @unguard(r&#39;https?://outlook.office.com/actions/ei\?u=([:;/_=!?#&amp;.,\w\%\-\+|]+)&#39;)
    def _outlook_image_proxy(self, match):
        return unquote(match[1])

    @unguard(r&#39;https?://(?:[\w-]+\.)?trendmicro.com(?::\d+)?/wis/clicktime/v[12]/(?:query|clickthrough)[:;/_=!?#&amp;.,\w\%\-\+|]+&#39;)
    def _trendmicro(self, match):
        result = match[0]
        self.log_info(&#39;trendmicro match:&#39;, result)
        parsed = urlparse(result)
        params = parse_qs(parsed.query)
        try:
            result = unquote(params[&#39;url&#39;][0])
        except Exception:
            pass
        return result

    @unicoded
    def process(self, data: str) -&gt; str:
        newsize, size = 0, len(data)
        while newsize != size:
            for handler in (
                self._proofpointV2,
                self._proofpointV3,
                self._outlook,
                self._outlook_image_proxy,
                self._trendmicro
            ):
                data = handler(data)
            size = newsize
            newsize = len(data)
        return data</code></pre>
</details>
</dd>
<dt id="refinery.shell.urn"><code class="flex name class">
<span>class <span class="ident">urn</span></span>
<span>(</span><span>size='N:N', keep=False, sort=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Treat the chunks in the current frame as items in an urn and produce every possible sequence
that could occur as a sequence of draws. For example, selecting both -k and -s is equivalent
to generating all possible permutations of these chunks.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/meta/urn.py#L10-L58" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class urn(Unit):
    &#34;&#34;&#34;
    Treat the chunks in the current frame as items in an urn and produce every possible sequence
    that could occur as a sequence of draws. For example, selecting both -k and -s is equivalent
    to generating all possible permutations of these chunks.
    &#34;&#34;&#34;

    def __init__(self,
        size: Arg.String(metavar=&#39;a:b&#39;, help=(
            &#39;Generate sequences of length x, where x is in [a:b]. The default value is {default}, &#39;
            &#39;where N is the number of chunks in the current frame.&#39;)) = &#39;N:N&#39;,
        keep: Arg.Switch(&#39;-k&#39;, help=(
            &#39;Chunks are not returned back to the urn after being drawn.&#39;)) = False,
        sort: Arg.Switch(&#39;-s&#39;, help=(
            &#39;The order of items does not matter; for the output, chunks are sorted according to &#39;
            &#39;their original position in the frame.&#39;)) = False
    ):
        super().__init__(size=size, keep=keep, sort=sort)

    def process(self, data: Chunk):
        yield from data.temp

    def filter(self, chunks: Iterable[Chunk]):
        it = iter(chunks)
        head = next(it)
        buffer = [bytes(head)]
        buffer.extend(bytes(c) for c in it)
        head = head.copy(meta=True, data=False)
        head.meta[&#39;N&#39;] = len(buffer)
        size = sliceobj(self.args.size, head)
        a = size.start or 1
        b = size.stop or len(buffer)
        b = max(b, a + 1)
        c = size.step or 1
        self.log_debug(F&#39;using size [{a}:{b}:{c}]&#39;)
        s = 1 if self.args.sort else 0
        k = 1 if self.args.keep else 0
        m = (s &lt;&lt; 1) | k
        method = {
            0b00: lambda i, r: product(i, repeat=r),
            0b01: combinations,
            0b10: combinations_with_replacement,
            0b11: permutations
        }[m]
        self.log_info(F&#39;choosing {method.__name__}&#39;)
        for n in range(a, b, c):
            self.log_debug(F&#39;generating sequences of length {n}&#39;)
            for head.temp in method(buffer, n):
                yield head</code></pre>
</details>
</dd>
<dt id="refinery.shell.uuenc"><code class="flex name class">
<span>class <span class="ident">uuenc</span></span>
</code></dt>
<dd>
<section class="desc"><p>Unit for uuencode.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/encoding/uuenc.py#L13-L77" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class uuenc(Unit):
    &#34;&#34;&#34;
    Unit for uuencode.
    &#34;&#34;&#34;
    def process(self, data):
        header = re.search(
            B&#39;^begin ([0-7]{3}) (.*?)$&#39;, data, flags=re.M)
        if header is None:
            raise ValueError(&#39;invalid uu header&#39;)
        output = bytearray()
        view = memoryview(data)
        breaks = [m.end() for m in iter(re.finditer(B&#39;^&#39;, data, flags=re.M))]
        eol = False
        for k, br in enumerate(itertools.islice(breaks, 1, None)):
            if eol and view[br:br + 3] == b&#39;end&#39;:
                path = header[2]
                if path != B&#39;-&#39;:
                    output = self.labelled(output, path=path)
                return output
            count = view[br] - 0x20
            if count not in range(0x41):
                raise ValueError(F&#39;Invalid length encoding 0x{view[br]:02X} in line {k}.&#39;)
            count %= 0x40
            cursor = len(output)
            q, r = divmod(count, 3)
            q += int(bool(r))
            end = br + 1 + q * 4
            for b in range(br + 1, end, 4):
                chunk = 0
                for j in range(4):
                    character = view[b + j]
                    if character not in range(0x21, 0x61):
                        raise ValueError(F&#39;Invalid character 0x{character:02X} in line {k}.&#39;)
                    chunk = ((character - 0x20) % 0x40) | (chunk &lt;&lt; 6)
                output.extend(chunk.to_bytes(3, &#39;big&#39;))
            del output[cursor + count:]
            eol = count == 0
            if len(output) &lt; cursor + count:
                break
        raise RefineryPartialResult(F&#39;Data truncated in line {k}&#39;, output)

    def reverse(self, data):
        meta = metavars(data)
        path = meta.get(&#39;path&#39;, None)
        name = path and pathlib.Path(path).name or &#39;-&#39;
        view = memoryview(data)
        with MemoryFile() as stream:
            stream.write(B&#39;begin 666 &#39;)
            stream.write(name.encode(self.codec))
            for k in range(0, len(view), 45):
                slice = view[k:k + 45]
                stream.write_byte(0x0A)
                stream.write_byte(0x20 + len(slice))
                for chunk in chunks.unpack(slice, 3, bigendian=True, pad=True):
                    for j in range(3, -1, -1):
                        stream.write_byte(0x20 + (((chunk &gt;&gt; j * 6) &amp; 0x3F) or 0x40))
            stream.write(B&#39;\n`\nend\n&#39;)
            return stream.getvalue()

    @classmethod
    def handles(self, data):
        if len(data) &lt; 16:
            return False
        if data[:6] == B&#39;begin &#39;:
            return set(data[6:9]) &lt;= set(B&#39;01234567&#39;)</code></pre>
</details>
</dd>
<dt id="refinery.shell.vaddr"><code class="flex name class">
<span>class <span class="ident">vaddr</span></span>
<span>(</span><span>*name, base=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Converts a metadata variable holding a file offset to a virtual address. This unit only works when the
chunk body contains a PE, ELF, or MachO executable. The variable will be substituted in place. If you
would like to retain the original value, it is recommended to use the <code><a title="refinery.put" href="index.html#refinery.put">put</a></code> unit first to create
a copy of an already existing variable, and then convert the copy.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/formats/exe/vaddr.py#L10-L46" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class vaddr(Unit):
    &#34;&#34;&#34;
    Converts a metadata variable holding a file offset to a virtual address. This unit only works when the
    chunk body contains a PE, ELF, or MachO executable. The variable will be substituted in place. If you
    would like to retain the original value, it is recommended to use the `refinery.put` unit first to create
    a copy of an already existing variable, and then convert the copy.
    &#34;&#34;&#34;

    def __init__(
        self, *name: Arg(type=str, help=&#39;The name of a metadata variable holding an integer.&#39;),
        base : Arg.Number(&#39;-b&#39;, metavar=&#39;ADDR&#39;, help=&#39;Optionally specify a custom base address B.&#39;) = None
    ):
        return super().__init__(names=name, base=base)

    def process(self, data):
        try:
            exe = Executable.Load(data, self.args.base)
        except Exception:
            self.log_warn(&#39;unable to parse input as executable; no variable conversion was performed&#39;)
            return data
        meta = metavars(data)
        for name in self.args.names:
            value = meta[name]
            meta[name] = exe.location_from_offset(value).virtual.position
        return data

    def reverse(self, data):
        try:
            exe = Executable.Load(data, self.args.base)
        except Exception:
            self.log_warn(&#39;unable to parse input as executable; no variable conversion was performed&#39;)
            return data
        meta = metavars(data)
        for name in self.args.names:
            value = meta[name]
            meta[name] = exe.location_from_address(value).physical.position
        return data</code></pre>
</details>
</dd>
<dt id="refinery.shell.vbapc"><code class="flex name class">
<span>class <span class="ident">vbapc</span></span>
<span>(</span><span>raw=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Extract VBA macro p-code from Office documents. By default, the unit also uses pcode2code to
decompile the disassembled p-code. This unit is specifically useful for macro documents that
use VBA code stomping, i.e. the embedded macro source code is stomped and does not represent
the p-code functionality that the document will actually execute.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/formats/office/vbapc.py#L11-L43" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class vbapc(Unit):
    &#34;&#34;&#34;
    Extract VBA macro p-code from Office documents. By default, the unit also uses pcode2code to
    decompile the disassembled p-code. This unit is specifically useful for macro documents that
    use VBA code stomping, i.e. the embedded macro source code is stomped and does not represent
    the p-code functionality that the document will actually execute.
    &#34;&#34;&#34;
    def __init__(self, raw: Unit.Arg.Switch(&#39;-r&#39;, help=&#39;Return disassembled p-code, do not try to decompile.&#39;) = False):
        super().__init__(raw=raw)

    @Unit.Requires(&#39;oletools&#39;, &#39;formats&#39;, &#39;office&#39;, &#39;extended&#39;)
    def _pcodedmp():
        with NoLogging():
            import pcodedmp.pcodedmp
            return pcodedmp.pcodedmp

    def process(self, data):
        class args:
            disasmOnly = True
            verbose = False
        with io.StringIO() as output:
            with VirtualFileSystem() as vfs:
                vf = vfs.new(data)
                self._pcodedmp.processFile(vf, args, output)
            code = output.getvalue()
            if not self.args.raw:
                from refinery.lib.thirdparty.pcode2code import Parser
                parser = Parser(code)
                parser.parseInput()
                parser.processInput(False)
                code = parser.getOutput()
                code = re.sub(R&#39;(?m)^((?:Sub|Function).*?)$(?!\n[^\s])&#39;, r&#39;\n\1&#39;, code)
            return code.encode(self.codec)</code></pre>
</details>
</dd>
<dt id="refinery.shell.vbastr"><code class="flex name class">
<span>class <span class="ident">vbastr</span></span>
<span>(</span><span>*paths, list=False, join_path=False, drop_path=False, fuzzy=0, exact=False, regex=False, path=b'path')</span>
</code></dt>
<dd>
<section class="desc"><p>Extract VBA macro variables from Office documents. The items are extracted in a directory
hierarchy that specifies their corresponding OLE stream. The stem of their file name is the
same as the variable's name. The variable can define a caption, a control tip text, and a
value; the unit extracts these with the synthesized file extension "cap", "tip", and "val",
respectively.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/formats/office/vbastr.py#L27-L72" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class vbastr(PathExtractorUnit):
    &#34;&#34;&#34;
    Extract VBA macro variables from Office documents. The items are extracted in a directory
    hierarchy that specifies their corresponding OLE stream. The stem of their file name is the
    same as the variable&#39;s name. The variable can define a caption, a control tip text, and a
    value; the unit extracts these with the synthesized file extension &#34;cap&#34;, &#34;tip&#34;, and &#34;val&#34;,
    respectively.
    &#34;&#34;&#34;
    @PathExtractorUnit.Requires(&#39;oletools&#39;, &#39;formats&#39;, &#39;office&#39;)
    def _olevba():
        from oletools import olevba
        return olevba

    def unpack(self, value):
        try:
            parser = self._olevba.VBA_Parser(&#39;.&#39;, data=bytes(value), relaxed=True)
        except self._olevba.FileOpenError:
            raise ValueError(&#39;Input data not recognized by VBA parser&#39;)
        try:
            for path, name, vars in parser.extract_form_strings_extended():
                if not vars:
                    continue
                name = _txt(vars[&#39;name&#39;])
                for ext, key in {
                    &#39;cap&#39;: &#39;caption&#39;,
                    &#39;tip&#39;: &#39;control_tip_text&#39;,
                    &#39;val&#39;: &#39;value&#39;,
                }.items():
                    value = _bin(vars.get(key))
                    if not value:
                        continue
                    yield UnpackResult(F&#39;{path!s}/{name!s}/{name}.{ext}&#39;, value)
        except self._olevba.oleform.OleFormParsingError as error:
            from collections import Counter
            self.log_debug(str(error))
            self.log_info(&#39;extended form extraction failed with error; falling back to simple method&#39;)
            form_strings = list(parser.extract_form_strings())
            name_counter = Counter(name for _, name, _ in form_strings)
            dedup = Counter()
            for path, name, string in form_strings:
                if string is None:
                    continue
                if name_counter[name] &gt; 1:
                    dedup[name] += 1
                    name = F&#39;{name!s}.v{dedup[name]}&#39;
                yield UnpackResult(F&#39;{path!s}/{name!s}.val&#39;, _bin(string))</code></pre>
</details>
</dd>
<dt id="refinery.shell.vigenere"><code class="flex name class">
<span>class <span class="ident">vigenere</span></span>
<span>(</span><span>key, alphabet=b'abcdefghijklmnopqrstuvwxyz', operator='add', case_sensitive=False, ignore_unknown=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Encryption and decryption using the Vigenre-Bellaso polyalphabetic cipher.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/crypto/cipher/vigenere.py#L21-L93" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class vigenere(Unit):
    &#34;&#34;&#34;
    Encryption and decryption using the Vigenre-Bellaso polyalphabetic cipher.
    &#34;&#34;&#34;

    def __init__(
        self,
        key: Arg(help=&#39;The encryption key&#39;),
        alphabet: Arg(
            help=&#39;The alphabet, by default the Latin one is used: &#34;{default}&#34;&#39;
        ) = b&#39;abcdefghijklmnopqrstuvwxyz&#39;,
        operator: Arg.Choice(&#39;-:&#39;, choices=[&#39;add&#39;, &#39;sub&#39;, &#39;xor&#39;], metavar=&#39;OP&#39;, help=(
            &#39;Choose the vigenere block operation. The default is {default}, and the available options are: {choices}&#39;)) = &#39;add&#39;,
        case_sensitive: Arg.Switch(&#39;-c&#39;, help=(
            &#39;Unless this option is set, the key will be case insensitive. Uppercase letters from the input are transformed &#39;
            &#39;using the same shift as would be the lowercase variant, but case is retained.&#39;)) = False,
        ignore_unknown: Arg.Switch(&#39;-i&#39;, help=(
            &#39;Unless this option is set, the key stream will be iterated even &#39;
            &#39;for letters that are not contained in the alphabet.&#39;
        )) = False
    ):
        if not callable(operator):
            operator = {
                &#39;add&#39;: __add__,
                &#39;sub&#39;: __sub__,
                &#39;xor&#39;: __xor__,
            }.get(operator.lower(), None)
            if operator is None:
                raise ValueError(F&#39;The value {operator!r} is not valid as an operator.&#39;)
        self.superinit(super(), **vars())

    def _tabula_recta(self, data, reverse=True):
        key: str = self.args.key.decode(self.codec)
        alphabet: str = self.args.alphabet.decode(self.codec)
        operator = self.args.operator
        case_sensitive: bool = self.args.case_sensitive
        ignore_unknown: bool = self.args.ignore_unknown
        if not case_sensitive:
            key = key.lower()
            alphabet = alphabet.lower()
            if len(set(alphabet)) != len(alphabet):
                raise ValueError(&#39;Duplicate entries detected in alphabet.&#39;)
        if not set(key) &lt;= set(alphabet):
            diff = set(key) - set(alphabet)
            diff = &#39;, &#39;.join(diff)
            raise ValueError(F&#39;key contains letters which are not from the given alphabet: {diff}&#39;)
        self.log_info(F&#39;using key {key} and alphabet {alphabet}&#39;)
        keystream = cycle(key)
        alph_size = len(alphabet)
        if reverse:
            operator = _opeator_inverse[operator]
        for letter in data:
            uppercase = not case_sensitive and letter.isupper()
            if uppercase:
                letter = letter.lower()
            try:
                position = alphabet.index(letter)
            except ValueError:
                yield letter
                if not ignore_unknown:
                    next(keystream)
                continue
            shift = alphabet.index(next(keystream))
            result = alphabet[operator(position, shift) % alph_size]
            yield result.upper() if uppercase else result

    @unicoded
    def process(self, data):
        return &#39;&#39;.join(self._tabula_recta(data, True))

    @unicoded
    def reverse(self, data):
        return &#39;&#39;.join(self._tabula_recta(data, False))</code></pre>
</details>
</dd>
<dt id="refinery.shell.vmemref"><code class="flex name class">
<span>class <span class="ident">vmemref</span></span>
<span>(</span><span>*address, take=None, base=None)</span>
</code></dt>
<dd>
<section class="desc"><p>The unit expects an executable as input (PE/ELF/MachO) and scans a function at a given virtual
address for memory references. For each memory reference, the unit looks up the corresponding
section and file offset for the reference. It then returns all data from that section starting
at the given offset.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/formats/exe/vmemref.py#L15-L154" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class vmemref(Unit):
    &#34;&#34;&#34;
    The unit expects an executable as input (PE/ELF/MachO) and scans a function at a given virtual
    address for memory references. For each memory reference, the unit looks up the corresponding
    section and file offset for the reference. It then returns all data from that section starting
    at the given offset.
    &#34;&#34;&#34;

    @Unit.Requires(&#39;smda&#39;, &#39;all&#39;)
    def _smda():
        import datetime
        datetime.UTC = datetime.timezone.utc
        import smda
        import smda.Disassembler
        import smda.DisassemblyResult
        return smda

    def _memory_references(
        self,
        exe: Executable,
        function: SmdaFunction,
        codes: Container[Range],
        max_dereference: int = 1
    ):
        def is_valid_data_address(address):
            if not isinstance(address, int):
                return False
            if address not in exe:
                return False
            if address in instructions:
                return False
            for code in codes:
                if address in code:
                    return False
            return True

        def dereference(address):
            return int.from_bytes(exe[address:address + pointer_size], exe.byte_order().value)

        pointer_size = exe.pointer_size // 8
        instructions = {op.offset for op in function.getInstructions()}

        references = set()

        for op in function.getInstructions():
            try:
                refs = list(op.getDataRefs())
            except Exception:
                continue
            for address in refs:
                try:
                    address = int(address)
                except Exception:
                    continue
                times_dereferenced = 0
                while is_valid_data_address(address) and address not in references:
                    references.add(address)
                    times_dereferenced += 1
                    if max_dereference and max_dereference &gt; 0 and times_dereferenced &gt; max_dereference:
                        break
                    try:
                        address = dereference(address)
                    except Exception:
                        break

        return references

    def __init__(
        self,
        *address: Arg.Number(metavar=&#39;ADDR&#39;, help=(
            &#39;Specify the address of a function to scan. If no argument is given, the unit will scan&#39;
            &#39; all functions for memory references.&#39;)),
        take: Arg.Number(&#39;-t&#39;, metavar=&#39;SIZE&#39;, help=(
            &#39;Optionally specify the number of bytes to read from each reference; by default, all &#39;
            &#39;data until the end of the section is returned.&#39;)) = None,
        base: Arg.Number(&#39;-b&#39;, metavar=&#39;ADDR&#39;,
            help=&#39;Optionally specify a custom base address B.&#39;) = None,
    ):
        super().__init__(address=address, take=take, base=base)

    def process(self, data):
        smda = self._smda
        take = self.args.take
        exe = Executable.Load(data, self.args.base)
        fmt = exe.pointer_size // 4
        addresses = self.args.address

        self.log_info(R&#39;disassembling and exploring call graph using smda&#39;)
        with NoLogging():
            cfg = smda.Disassembler.SmdaConfig()
            cfg.CALCULATE_SCC = False
            cfg.CALCULATE_NESTING = False
            cfg.TIMEOUT = 600
            dsm = smda.Disassembler.Disassembler(cfg)
            _input = data
            if not isinstance(_input, bytes):
                _input = bytes(data)
            graph = dsm.disassembleUnmappedBuffer(_input)

        self.log_info(&#39;collecting code addresses for memory reference exclusion list&#39;)
        visits = set()
        avoid = set()

        for symbol in exe.symbols():
            if not symbol.code:
                continue
            avoid.add(exe.location_from_address(symbol.address).virtual.box)

        if addresses:
            reset = visits.clear
        else:
            def reset(): pass
            self.log_info(&#39;scanning executable for functions&#39;)
            with NoLogging():
                addresses = [pfn.offset for pfn in graph.getFunctions()]
                addresses.sort()

        for a in addresses:
            reset()
            address, function = min(graph.xcfg.items(), key=lambda t: (t[0] &gt;= a, abs(t[0] - a)))
            self.log_debug(F&#39;scanning function: 0x{address:0{fmt}X}&#39;)
            refs = list(self._memory_references(exe, function, avoid))
            refs.sort(reverse=True)
            last_start = None
            for ref in refs:
                if ref in visits:
                    continue
                visits.add(ref)
                try:
                    box = exe.location_from_address(ref)
                    end = box.physical.box.upper
                    if take is not None:
                        end = min(ref + take, end)
                    if last_start is not None:
                        end = min(last_start, end)
                    last_start = box.physical.position
                except CompartmentNotFound:
                    self.log_info(F&#39;memory reference could not be resolved: 0x{ref:0{fmt}X}&#39;)
                else:
                    yield exe.data[last_start:end]</code></pre>
</details>
</dd>
<dt id="refinery.shell.vsect"><code class="flex name class">
<span>class <span class="ident">vsect</span></span>
<span>(</span><span>*paths, meta=False, synthetic=False, path=b'path', regex=False, exact=False, fuzzy=0, drop_path=False, join_path=False, list=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Extract sections/segments from PE, ELF, and MachO executables.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/formats/exe/vsect.py#L7-L44" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class vsect(PathExtractorUnit):
    &#34;&#34;&#34;
    Extract sections/segments from PE, ELF, and MachO executables.
    &#34;&#34;&#34;
    def __init__(
        self, *paths,
        meta: Arg.Switch(&#39;-m&#39;, help=(
            &#39;Populates the metadata variables vaddr and vsize containing the virtual address and size &#39;
            &#39;of each section, respectively.&#39;)) = False,
        synthetic: Arg.Switch(&#39;-s&#39;, help=(
            &#39;Include synthesized sections: These represent data regions that are outside the sections &#39;
            &#39;as listed by the executable metadata, such as headers and overlays.&#39;)) = False,
        **keywords
    ):
        super().__init__(*paths, meta=meta, synthetic=synthetic, **keywords)

    def unpack(self, data):
        exe = Executable.Load(data)
        mv = memoryview(data)
        for k, section in enumerate(exe.sections()):
            if section.synthetic and not self.args.synthetic:
                continue
            start = section.physical.lower
            end = section.physical.upper
            va = section.virtual.lower
            vs = len(section.virtual)
            kwargs = {&#39;offset&#39;: start}
            if self.args.meta:
                if va is not None:
                    kwargs[&#39;vaddr&#39;] = va
                if vs is not None:
                    kwargs[&#39;vsize&#39;] = vs
            name = section.name
            if not name:
                addr = F&#39;{section.virtual.lower:0{exe.pointer_size // 4}X}&#39;
                self.log_warn(F&#39;section {k} had no name, synthesizing name from virtual address 0x{addr}&#39;)
                name = F&#39;.{addr}&#39;
            yield UnpackResult(name, mv[start:end], **kwargs)</code></pre>
</details>
</dd>
<dt id="refinery.shell.vsnip"><code class="flex name class">
<span>class <span class="ident">vsnip</span></span>
<span>(</span><span>*addresses, ascii=False, utf16=False, until=b'', base=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Extract data from PE, ELF, and MachO files based on virtual offsets.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/formats/exe/vsnip.py#L24-L73" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class vsnip(Unit):
    &#34;&#34;&#34;
    Extract data from PE, ELF, and MachO files based on virtual offsets.
    &#34;&#34;&#34;

    def __init__(
        self, *addresses: Arg.Bounds(metavar=&#39;start:count:align&#39;, help=(
            &#39;Use Python slice syntax to describe an area of virtual memory to read. If a chunksize is &#39;
            &#39;specified, then the unit will always read a multiple of that number of bytes&#39;)),
        ascii: Arg.Switch(&#39;-a&#39;, group=&#39;END&#39;, help=&#39;Read ASCII strings; equivalent to -th:00&#39;) = False,
        utf16: Arg.Switch(&#39;-u&#39;, group=&#39;END&#39;, help=&#39;Read UTF16 strings; equivalent to -th:0000 (also sets chunksize to 2)&#39;) = False,
        until: Arg.Binary(&#39;-t&#39;, group=&#39;END&#39;, help=&#39;Read until sequence {varname} is read.&#39;) = B&#39;&#39;,
        base : Arg.Number(&#39;-b&#39;, metavar=&#39;ADDR&#39;, help=&#39;Optionally specify a custom base address B.&#39;) = None,
    ):
        if sum(1 for t in (until, utf16, ascii) if t) &gt; 1:
            raise ValueError(&#39;Only one of utf16, ascii, and until can be specified.&#39;)
        return super().__init__(addresses=addresses, utf16=utf16, ascii=ascii, until=until, base=base)

    def process(self, data: bytearray):
        until = self.args.until
        addrs = self.args.addresses
        if self.args.ascii:
            until = B&#39;\0&#39;
        if self.args.utf16:
            until = B&#39;\0\0&#39;
            addrs = (slice(a.start, a.stop, 2) for a in addrs)

        exe = Executable.Load(data, self.args.base)

        for addr in addrs:
            area = MemoryArea(addr)
            location = exe.location_from_address(area.start)
            offset = location.physical.position
            max_offset = location.physical.box.upper
            if not until:
                end = max_offset
            else:
                end = offset - 1
                align = area.align
                while True:
                    end = data.find(until, end + 1)
                    if end not in range(offset, max_offset):
                        raise EndOfStringNotFound
                    if (end - offset) % align == 0:
                        break

            if area.count:
                end = min(end, offset + area.count)

            yield self.labelled(data[offset:end], offset=offset)</code></pre>
</details>
</dd>
<dt id="refinery.shell.vstack"><code class="flex name class">
<span>class <span class="ident">vstack</span></span>
<span>(</span><span>*address, stop=None, base=None, arch=Arch.X32, engine=_engine.unicorn, se=False, ic=False, uc=False, meta_registers=False, timeout=None, patch_range=slice(5, None, None), write_range=slice(1, None, None), wait=20, wait_calls=False, skip_calls=0, stack_size=65536, stack_push=None, block_size=4096, max_visits=65536, log_writes_in_calls=False, log_stack_addresses=False, log_other_addresses=False, log_zero_overwrites=False, log_stack_cookies=False)</span>
</code></dt>
<dd>
<section class="desc"><p>The unit emulates instructions at a given address in the input executable (PE/ELF/MachO) and
extracts data patches that are written to the stack during emulation. Emulation is halted as
soon as a certain number of instructions has not performed any memory writes, or when an error
occurs. By default, most registers are set to the current location in the emulated stack.
However, if you want to initialize certain registers differently, you can set an environment
variable to the desired value.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/formats/exe/vstack.py#L324-L573" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class vstack(Unit):
    &#34;&#34;&#34;
    The unit emulates instructions at a given address in the input executable (PE/ELF/MachO) and
    extracts data patches that are written to the stack during emulation. Emulation is halted as
    soon as a certain number of instructions has not performed any memory writes, or when an error
    occurs. By default, most registers are set to the current location in the emulated stack.
    However, if you want to initialize certain registers differently, you can set an environment
    variable to the desired value.
    &#34;&#34;&#34;

    @Unit.Requires(&#39;intervaltree&#39;, &#39;default&#39;, &#39;extended&#39;)
    def _intervaltree():
        import intervaltree
        return intervaltree

    @Unit.Requires(&#39;capstone&#39;, &#39;default&#39;, &#39;extended&#39;)
    def _capstone():
        import capstone
        return capstone

    @Unit.Requires(&#39;unicorn==2.0.1.post1&#39;, &#39;default&#39;, &#39;extended&#39;)
    def _unicorn():
        import setuptools # noqa
        with NoLogging():
            import unicorn
            return unicorn

    @Unit.Requires(&#39;speakeasy-emulator-refined&#39;, &#39;extended&#39;)
    def _speakeasy():
        import speakeasy
        return speakeasy

    @Unit.Requires(&#39;icicle-emu&#39;, &#39;all&#39;)
    def _icicle():
        import icicle
        return icicle

    def __init__(
        self,
        *address: Arg.NumSeq(metavar=&#39;start&#39;, help=&#39;Specify the (virtual) addresses of a stack string instruction sequences.&#39;),
        stop: Arg.Number(&#39;-s&#39;, metavar=&#39;stop&#39;, help=&#39;Optional: Stop when reaching this address.&#39;) = None,
        base: Arg.Number(&#39;-b&#39;, metavar=&#39;Addr&#39;, help=&#39;Optionally specify a custom base address B.&#39;) = None,
        arch: Arg.Option(&#39;-a&#39;, help=&#39;Specify for blob inputs: {choices}&#39;, choices=Arch) = Arch.X32,
        engine: Arg.Option(&#39;-e&#39;, group=&#39;EMU&#39;, choices=_engine, metavar=&#39;E&#39;,
            help=&#39;The emulator engine. The default is {default}, options are: {choices}&#39;) = _engine.unicorn,
        se: Arg.Switch(group=&#39;EMU&#39;, help=&#39;Equivalent to --engine=speakeasy&#39;) = False,
        ic: Arg.Switch(group=&#39;EMU&#39;, help=&#39;Equivalent to --engine=icicle&#39;) = False,
        uc: Arg.Switch(group=&#39;EMU&#39;, help=&#39;Equivalent to --engine=unicorn&#39;) = False,
        meta_registers: Arg.Switch(&#39;-r&#39;, help=(
            &#39;Consume register initialization values from the chunk\&#39;s metadata. If the value is a byte string, &#39;
            &#39;the data will be mapped.&#39;)) = False,
        timeout: Arg.Number(&#39;-t&#39;, help=&#39;Optionally stop emulating after a given number of instructions.&#39;) = None,
        patch_range: Arg.Bounds(&#39;-p&#39;, metavar=&#39;MIN:MAX&#39;,
            help=&#39;Extract only patches that are in the given range, default is {default}.&#39;) = slice(5, None),
        write_range: Arg.Bounds(&#39;-n&#39;, metavar=&#39;MIN:MAX&#39;,
            help=&#39;Log only writes whose size is in the given range, default is {default}.&#39;) = slice(1, None),
        wait: Arg.Number(&#39;-w&#39;, help=(
            &#39;When this many instructions did not write to memory, emulation is halted. The default is {default}.&#39;)) = 20,
        wait_calls: Arg.Switch(&#39;-c&#39;, group=&#39;CALL&#39;,
            help=&#39;Wait indefinitely when inside a function call.&#39;) = False,
        skip_calls: Arg.Counts(&#39;-C&#39;, group=&#39;CALL&#39;,
            help=&#39;Skip function calls entirely. Use twice to treat each call as allocating memory.&#39;) = 0,
        stack_size: Arg.Number(&#39;-S&#39;, help=&#39;Optionally specify the stack size. The default is 0x{default:X}.&#39;) = 0x10000,
        stack_push: Arg(&#39;-u&#39;, action=&#39;append&#39;, type=str, metavar=&#39;REG&#39;,
            help=&#39;Push the value of a register to the stack before beginning emulation; implies -r.&#39;) = None,
        block_size: Arg.Number(&#39;-B&#39;, help=&#39;Standard memory block size for the emulator, 0x{default:X} by default.&#39;) = 0x1000,
        max_visits: Arg.Number(&#39;-V&#39;, help=&#39;Maximum number of times a code address is visited. Default is {default}.&#39;) = 0x10000,
        log_writes_in_calls: Arg.Switch(&#39;-W&#39;, help=&#39;Log writes of values that occur in functions calls.&#39;) = False,
        log_stack_addresses: Arg.Switch(&#39;-X&#39;, help=&#39;Log writes of values that are stack addresses.&#39;) = False,
        log_other_addresses: Arg.Switch(&#39;-Y&#39;, help=&#39;Log writes of values that are addresses to mapped segments.&#39;) = False,
        log_zero_overwrites: Arg.Switch(&#39;-Z&#39;, help=&#39;Log writes of zeros to memory that contained nonzero values.&#39;) = False,
        log_stack_cookies  : Arg.Switch(&#39;-E&#39;, help=&#39;Log writes that look like stack cookies.&#39;) = False,
    ):
        if sum((se, uc, ic)) &gt; 1:
            raise ValueError(&#39;Too many emulators selected.&#39;)
        elif se:
            engine = _engine.speakeasy
        elif ic:
            engine = _engine.icicle
        elif uc:
            engine = _engine.unicorn

        super().__init__(
            address=address,
            stop=stop,
            base=base,
            arch=Arg.AsOption(arch, Arch),
            engine=Arg.AsOption(engine, _engine),
            meta_registers=meta_registers,
            timeout=timeout,
            patch_range=patch_range,
            write_range=write_range,
            wait=wait,
            stack_size=stack_size,
            stack_push=stack_push,
            wait_calls=wait_calls,
            skip_calls=skip_calls,
            block_size=block_size,
            max_visits=max_visits,
            log_writes_in_calls=log_writes_in_calls,
            log_stack_addresses=log_stack_addresses,
            log_other_addresses=log_other_addresses,
            log_zero_overwrites=log_zero_overwrites,
            log_stack_cookies=log_stack_cookies
        )

    def process(self, data):
        meta = metavars(data)
        args = self.args

        engine: _engine = args.engine
        flags = Hook.Default
        self.log_debug(F&#39;attempting to use {engine.name}&#39;)
        getattr(self, F&#39;_{engine.name}&#39;)

        if engine is _engine.speakeasy:
            flags |= Hook.ApiCall

        class Emu(engine.value, VStackEmulatorMixin):
            pass

        emu = Emu(
            data,
            args.base,
            args.arch,
            flags,
            args.block_size,
            args.stack_size,
        )

        cfg = EmuConfig(
            args.wait_calls,
            args.skip_calls,
            args.write_range,
            args.wait,
            args.block_size,
            args.stack_size,
            args.max_visits,
            args.log_stack_cookies,
            args.log_writes_in_calls,
            args.log_stack_addresses,
            args.log_other_addresses,
            args.log_zero_overwrites,
        )

        register_values = {}
        emu.reset(None)

        if args.meta_registers or args.stack_push:
            for var, value in list(meta.items()):
                try:
                    register = emu.lookup_register(var)
                except LookupError:
                    continue
                meta.discard(var)
                register_values[register] = var, value

        def parse_address(a: Union[int, bytes]):
            if isinstance(a, int):
                return a
            a = a.decode(self.codec)
            if m := re.fullmatch(&#39;(?i)(?:sub_|fun_|0x)?([A-F0-9]+)H?&#39;, a):
                return int(m[1], 16)
            try:
                return PythonExpression.Evaluate(a, meta)
            except ParserVariableMissing:
                pass
            symbols = list(emu.exe.symbols())
            for filter in [
                lambda s: s.get_name().casefold() == a.casefold(),
                lambda s: s.name == a,
                lambda s: s.code,
                lambda s: s.exported
            ]:
                symbols = [s for s in symbols if filter(s)]
                if len(symbols) == 1:
                    return symbols[0].address
            if len(symbols) &gt; 1:
                raise RuntimeError(F&#39;there are {len(symbols)} exported function symbol named &#34;{a}&#34;, please specify the address&#39;)
            if not symbols:
                raise LookupError(F&#39;no symbol with name &#34;{a}&#34; was found&#39;)

        addresses = [parse_address(a) for a in args.address]

        if not addresses:
            for symbol in emu.exe.symbols():
                if symbol.name is None:
                    addresses.append(symbol.address)
                    break

        for address in addresses:
            tree = self._intervaltree.IntervalTree()
            state = EmuState(cfg, tree, address, emu.exe.pointer_size // 4, stop=args.stop)
            emu.reset(state)

            for reg in emu.general_purpose_registers():
                if reg not in register_values:
                    state.init_registers.append(reg)

            for reg, (var, value) in register_values.items():
                if isinstance(value, int):
                    self.log_info(F&#39;setting {var} to integer value 0x{value:X}&#39;)
                    emu.set_register(reg, value)
                    continue
                if isinstance(value, str):
                    value = value.encode()
                if isbuffer(value):
                    base = emu.malloc(len(value))
                    emu.mem_write(base, bytes(value))
                    emu.set_register(reg, base)
                    self.log_info(F&#39;setting {var} to mapped buffer of size 0x{len(value):X}&#39;)
                    continue
                _tn = value.__class__.__name__
                self.log_warn(F&#39;canot interpret value of type {_tn} for register {var}&#39;)

            if push := args.stack_push:
                for reg in push:
                    emu.push_register(reg)

            timeout = args.timeout
            if timeout is not None:
                self.log_info(F&#39;setting timeout of {timeout} steps&#39;)
                state.ticks = timeout

            try:
                emu.emulate(address, args.stop)
            except EmulationError:
                pass

            for patch, api in state.synthesized.items():
                chunk = self.labelled(patch, src=api)
                yield chunk

            tree.merge_overlaps()
            it: Iterator[Interval] = iter(tree)
            for interval in it:
                size = interval.end - interval.begin - 1
                if size not in bounds[args.patch_range]:
                    continue
                try:
                    patch = emu.mem_read(interval.begin, size)
                except Exception as error:
                    width = emu.exe.pointer_size // 4
                    self.log_info(F&#39;error reading 0x{interval.begin:0{width}X}:{size}: {error!s}&#39;)
                    continue
                if not any(patch):
                    continue
                self.log_info(F&#39;memory patch at {state.fmt(interval.begin)} of size {size}&#39;)
                chunk = self.labelled(patch, src=interval.begin)
                yield chunk</code></pre>
</details>
</dd>
<dt id="refinery.shell.winreg"><code class="flex name class">
<span>class <span class="ident">winreg</span></span>
<span>(</span><span>*paths, list=False, join_path=False, drop_path=False, fuzzy=0, exact=False, regex=False, path=b'path')</span>
</code></dt>
<dd>
<section class="desc"><p>Extract values from a Windows registry hive or from a registry export (.reg file).</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/formats/winreg.py#L31-L173" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class winreg(PathExtractorUnit):
    &#34;&#34;&#34;
    Extract values from a Windows registry hive or from a registry export (.reg file).
    &#34;&#34;&#34;
    @PathExtractorUnit.Requires(&#39;python-registry&#39;, &#39;formats&#39;)
    def _registry():
        import Registry
        import Registry.Registry
        import Registry.RegistryParse
        return Registry

    @staticmethod
    def _walk(patterns: List[PathPattern], key: RegistryKey, *path: str):
        here = &#39;/&#39;.join(path)
        if not any(p.reach(here) for p in patterns):
            winreg.log_debug(F&#39;pruning search at {here}&#39;)
            return
        for value in key.values():
            def raw(v: RegistryValue = value):
                return v.raw_data()
            vpath = here
            vname = value.name()
            if vname != &#39;(default)&#39;:
                vpath = F&#39;{vpath}/{vname}&#39;
            yield UnpackResult(vpath, raw)
        for subkey in key.subkeys():
            yield from winreg._walk(patterns, subkey, *path, subkey.name())

    def _unpack_hive(self, data: bytearray):
        try:
            with MemoryFile(data) as stream:
                root = self._registry.Registry.Registry(stream).root()
                yield from self._walk(self._patterns, root, root.name())
        except self._registry.RegistryParse.ParseException:
            raise ParseException

    def _decode_registry_export(self, data: str):
        def REG_BINARY(data: str) -&gt; bytes:
            return bytes.fromhex(re.sub(&#39;[^a-f0-9]+&#39;, &#39;&#39;, data))

        def REG_SZ(data: str) -&gt; bytes:
            return data.encode(self.codec) | esc(quoted=True) | bytes

        def REG_EXPAND_SZ(data: str):
            return REG_BINARY(data).decode(&#39;UTF-16LE&#39;).rstrip(&#39;\0&#39;).encode(self.codec)

        def REG_MULTI_SZ(data: str):
            data = REG_BINARY(data).decode(&#39;UTF-16LE&#39;).split(&#39;\0&#39;)
            for string in data:
                if string:
                    yield string.encode(self.codec)

        def REG_DWORD(data: str):
            value = int(data, 16)
            return F&#39;0x{value:X}&#39;.encode(self.codec)

        def REG_QWORD(data: str):
            value = int.from_bytes(REG_BINARY(data), &#39;little&#39;)
            return F&#39;0x{value:X}&#39;.encode(self.codec)

        class Missing:
            def __init__(self, name: str): self.name = name
            def __str__(self): return self.name

        REG_NONE = REG_EXPAND_SZ
        REG_DWORD_BIG_ENDIAN = Missing(&#39;REG_DWORD_BIG_ENDIAN&#39;)
        REG_LINK = Missing(&#39;REG_LINK&#39;)
        REG_RESOURCE_LIST = Missing(&#39;REG_RESOURCE_LIST&#39;)
        REG_FULL_RESOURCE_DESCRIPTOR = Missing(&#39;REG_FULL_RESOURCE_DESCRIPTOR&#39;)
        REG_RESOURCE_REQUIREMENTS_LIST = Missing(&#39;REG_RESOURCE_REQUIREMENTS_LIST&#39;)

        prefix, _, encoded = data.partition(&#39;:&#39;)

        try:
            decoder = {
                &#39;hex(0)&#39; : REG_NONE,
                &#39;hex(1)&#39; : REG_SZ,
                &#39;hex(2)&#39; : REG_EXPAND_SZ,
                &#39;hex(3)&#39; : REG_BINARY,
                &#39;hex&#39;    : REG_BINARY,
                &#39;hex(4)&#39; : REG_DWORD,
                &#39;dword&#39;  : REG_DWORD,
                &#39;hex(5)&#39; : REG_DWORD_BIG_ENDIAN,
                &#39;hex(6)&#39; : REG_LINK,
                &#39;hex(7)&#39; : REG_MULTI_SZ,
                &#39;hex(8)&#39; : REG_RESOURCE_LIST,
                &#39;hex(9)&#39; : REG_FULL_RESOURCE_DESCRIPTOR,
                &#39;hex(a)&#39; : REG_RESOURCE_REQUIREMENTS_LIST,
                &#39;hex(b)&#39; : REG_QWORD,
            }[prefix]
        except KeyError:
            decoder = REG_SZ
            encoded = data

        if isinstance(decoder, Missing):
            self.log_warn(F&#39;Found registry type {decoder!s}; no decoder implemented.&#39;)
            return
        self.log_debug(F&#39;decoding as {decoder.__name__}: {encoded}&#39;)
        it = decoder(encoded)
        if not inspect.isgenerator(it):
            it = (it,)
        yield from it

    def _unpack_file(self, data: bytearray):
        for codec in (&#39;utf16&#39;, &#39;utf-16le&#39;, &#39;utf8&#39;):
            try:
                reg = data.decode(codec).splitlines(keepends=True)
            except UnicodeError:
                continue
            if reg[0].startswith(&#39;Windows Registry Editor&#39;):
                break
        else:
            raise ParseException
        config = WinRegFileParser()
        config.read_string(&#39;&#39;.join(reg[1:]))
        for key in config.sections():
            self.log_debug(key)
            for value in config[key]:
                name = next(iter(shlex.split(value)))
                path = Path(key)
                if name != &#39;@&#39;:
                    path = path / Path(name)
                data = config[key][value]
                decoded = list(self._decode_registry_export(data))
                if len(decoded) == 1:
                    yield UnpackResult(str(path), decoded[0])
                    continue
                for k, d in enumerate(decoded):
                    yield UnpackResult(F&#39;{path!s}.{k}&#39;, d)

    def unpack(self, data):
        with contextlib.suppress(ParseException):
            yield from self._unpack_hive(data)
            return
        yield from self._unpack_file(data)

    @classmethod
    def handles(self, data):
        if data[:4] == B&#39;regf&#39;:
            return True
        if data[:31] == b&#39;Windows Registry Editor Version&#39;:
            return True
        return False</code></pre>
</details>
</dd>
<dt id="refinery.shell.wshenc"><code class="flex name class">
<span>class <span class="ident">wshenc</span></span>
<span>(</span><span>marker=True)</span>
</code></dt>
<dd>
<section class="desc"><p>Windows Scripting Host encoding and decoding of VBScript (VBS/VBE) and JScript (JS/JSE).</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/encoding/wshenc.py#L9-L163" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class wshenc(Unit):
    &#34;&#34;&#34;
    Windows Scripting Host encoding and decoding of VBScript (VBS/VBE) and JScript (JS/JSE).
    &#34;&#34;&#34;

    _MARKER_INIT = RB&#39;#@~^BINREF==&#39;
    _MARKER_STOP = RB&#39;BINREF==^#~@&#39;

    _CHUNKS = (
        0x57, 0x6E, 0x7B, 0x4A, 0x4C, 0x41, 0x0B, 0x0B, 0x0B, 0x0C, 0x0C, 0x0C, 0x4A, 0x4C, 0x41,
        0x0E, 0x0E, 0x0E, 0x0F, 0x0F, 0x0F, 0x10, 0x10, 0x10, 0x11, 0x11, 0x11, 0x12, 0x12, 0x12,
        0x13, 0x13, 0x13, 0x14, 0x14, 0x14, 0x15, 0x15, 0x15, 0x16, 0x16, 0x16, 0x17, 0x17, 0x17,
        0x18, 0x18, 0x18, 0x19, 0x19, 0x19, 0x1A, 0x1A, 0x1A, 0x1B, 0x1B, 0x1B, 0x1C, 0x1C, 0x1C,
        0x1D, 0x1D, 0x1D, 0x1E, 0x1E, 0x1E, 0x1F, 0x1F, 0x1F, 0x2E, 0x2D, 0x32, 0x47, 0x75, 0x30,
        0x7A, 0x52, 0x21, 0x56, 0x60, 0x29, 0x42, 0x71, 0x5B, 0x6A, 0x5E, 0x38, 0x2F, 0x49, 0x33,
        0x26, 0x5C, 0x3D, 0x49, 0x62, 0x58, 0x41, 0x7D, 0x3A, 0x34, 0x29, 0x35, 0x32, 0x36, 0x65,
        0x5B, 0x20, 0x39, 0x76, 0x7C, 0x5C, 0x72, 0x7A, 0x56, 0x43, 0x7F, 0x73, 0x38, 0x6B, 0x66,
        0x39, 0x63, 0x4E, 0x70, 0x33, 0x45, 0x45, 0x2B, 0x6B, 0x68, 0x68, 0x62, 0x71, 0x51, 0x59,
        0x4F, 0x66, 0x78, 0x09, 0x76, 0x5E, 0x62, 0x31, 0x7D, 0x44, 0x64, 0x4A, 0x23, 0x54, 0x6D,
        0x75, 0x43, 0x71, 0x4A, 0x4C, 0x41, 0x7E, 0x3A, 0x60, 0x4A, 0x4C, 0x41, 0x5E, 0x7E, 0x53,
        0x40, 0x4C, 0x40, 0x77, 0x45, 0x42, 0x4A, 0x2C, 0x27, 0x61, 0x2A, 0x48, 0x5D, 0x74, 0x72,
        0x22, 0x27, 0x75, 0x4B, 0x37, 0x31, 0x6F, 0x44, 0x37, 0x4E, 0x79, 0x4D, 0x3B, 0x59, 0x52,
        0x4C, 0x2F, 0x22, 0x50, 0x6F, 0x54, 0x67, 0x26, 0x6A, 0x2A, 0x72, 0x47, 0x7D, 0x6A, 0x64,
        0x74, 0x39, 0x2D, 0x54, 0x7B, 0x20, 0x2B, 0x3F, 0x7F, 0x2D, 0x38, 0x2E, 0x2C, 0x77, 0x4C,
        0x30, 0x67, 0x5D, 0x6E, 0x53, 0x7E, 0x6B, 0x47, 0x6C, 0x66, 0x34, 0x6F, 0x35, 0x78, 0x79,
        0x25, 0x5D, 0x74, 0x21, 0x30, 0x43, 0x64, 0x23, 0x26, 0x4D, 0x5A, 0x76, 0x52, 0x5B, 0x25,
        0x63, 0x6C, 0x24, 0x3F, 0x48, 0x2B, 0x7B, 0x55, 0x28, 0x78, 0x70, 0x23, 0x29, 0x69, 0x41,
        0x28, 0x2E, 0x34, 0x73, 0x4C, 0x09, 0x59, 0x21, 0x2A, 0x33, 0x24, 0x44, 0x7F, 0x4E, 0x3F,
        0x6D, 0x50, 0x77, 0x55, 0x09, 0x3B, 0x53, 0x56, 0x55, 0x7C, 0x73, 0x69, 0x3A, 0x35, 0x61,
        0x5F, 0x61, 0x63, 0x65, 0x4B, 0x50, 0x46, 0x58, 0x67, 0x58, 0x3B, 0x51, 0x31, 0x57, 0x49,
        0x69, 0x22, 0x4F, 0x6C, 0x6D, 0x46, 0x5A, 0x4D, 0x68, 0x48, 0x25, 0x7C, 0x27, 0x28, 0x36,
        0x5C, 0x46, 0x70, 0x3D, 0x4A, 0x6E, 0x24, 0x32, 0x7A, 0x79, 0x41, 0x2F, 0x37, 0x3D, 0x5F,
        0x60, 0x5F, 0x4B, 0x51, 0x4F, 0x5A, 0x20, 0x42, 0x2C, 0x36, 0x65, 0x57)
    _OFFSETS = (
        0, 1, 2, 0, 1, 2, 1, 2, 2, 1, 2, 1, 0, 2, 1, 2, 0, 2, 1, 2, 0, 0, 1, 2, 2, 1, 0, 2, 1, 2, 2, 1,
        0, 0, 2, 1, 2, 1, 2, 0, 2, 0, 0, 1, 2, 0, 2, 1, 0, 2, 1, 2, 0, 0, 1, 2, 2, 0, 0, 1, 2, 0, 2, 1)
    _ENCODER = {
        0x09 : [0x37, 0x69, 0x64], 0x0B : [0x0B, 0x0B, 0x0B], 0x0C : [0x0C, 0x0C, 0x0C],
        0x0E : [0x0E, 0x0E, 0x0E], 0x0F : [0x0F, 0x0F, 0x0F], 0x10 : [0x10, 0x10, 0x10],
        0x11 : [0x11, 0x11, 0x11], 0x12 : [0x12, 0x12, 0x12], 0x13 : [0x13, 0x13, 0x13],
        0x14 : [0x14, 0x14, 0x14], 0x15 : [0x15, 0x15, 0x15], 0x16 : [0x16, 0x16, 0x16],
        0x17 : [0x17, 0x17, 0x17], 0x18 : [0x18, 0x18, 0x18], 0x19 : [0x19, 0x19, 0x19],
        0x1A : [0x1A, 0x1A, 0x1A], 0x1B : [0x1B, 0x1B, 0x1B], 0x1C : [0x1C, 0x1C, 0x1C],
        0x1D : [0x1D, 0x1D, 0x1D], 0x1E : [0x1E, 0x1E, 0x1E], 0x1F : [0x1F, 0x1F, 0x1F],
        0x20 : [0x7E, 0x2C, 0x50], 0x21 : [0x5A, 0x65, 0x22], 0x22 : [0x45, 0x72, 0x4A],
        0x23 : [0x3A, 0x5B, 0x61], 0x24 : [0x79, 0x66, 0x5E], 0x25 : [0x59, 0x75, 0x5D],
        0x26 : [0x27, 0x4C, 0x5B], 0x27 : [0x76, 0x45, 0x42], 0x28 : [0x63, 0x76, 0x60],
        0x29 : [0x62, 0x2A, 0x23], 0x2A : [0x4D, 0x43, 0x65], 0x2B : [0x51, 0x33, 0x5F],
        0x2C : [0x53, 0x42, 0x7E], 0x2D : [0x52, 0x20, 0x4F], 0x2E : [0x20, 0x63, 0x52],
        0x2F : [0x26, 0x4A, 0x7A], 0x30 : [0x54, 0x5A, 0x21], 0x31 : [0x71, 0x38, 0x46],
        0x32 : [0x2B, 0x79, 0x20], 0x33 : [0x66, 0x32, 0x26], 0x34 : [0x2A, 0x57, 0x63],
        0x35 : [0x58, 0x6C, 0x2A], 0x36 : [0x7F, 0x2B, 0x76], 0x37 : [0x7B, 0x46, 0x47],
        0x38 : [0x30, 0x52, 0x25], 0x39 : [0x31, 0x4F, 0x2C], 0x3A : [0x6C, 0x3D, 0x29],
        0x3B : [0x49, 0x70, 0x69], 0x3D : [0x78, 0x7B, 0x27], 0x3F : [0x5F, 0x51, 0x67],
        0x40 : [0x40, 0x40, 0x40], 0x41 : [0x29, 0x7A, 0x62], 0x42 : [0x24, 0x7E, 0x41], # noqa
        0x43 : [0x2F, 0x3B, 0x5A], 0x44 : [0x39, 0x47, 0x66], 0x45 : [0x33, 0x41, 0x32],
        0x46 : [0x6F, 0x77, 0x73], 0x47 : [0x21, 0x56, 0x4D], 0x48 : [0x75, 0x5F, 0x43],
        0x49 : [0x28, 0x26, 0x71], 0x4A : [0x42, 0x78, 0x39], 0x4B : [0x46, 0x6E, 0x7C],
        0x4C : [0x4A, 0x64, 0x53], 0x4D : [0x5C, 0x74, 0x48], 0x4E : [0x48, 0x67, 0x31],
        0x4F : [0x36, 0x7D, 0x72], 0x50 : [0x4B, 0x68, 0x6E], 0x51 : [0x7D, 0x35, 0x70],
        0x52 : [0x5D, 0x22, 0x49], 0x53 : [0x6A, 0x55, 0x3F], 0x54 : [0x50, 0x3A, 0x4B],
        0x55 : [0x69, 0x60, 0x6A], 0x56 : [0x23, 0x6A, 0x2E], 0x57 : [0x09, 0x71, 0x7F],
        0x58 : [0x70, 0x6F, 0x28], 0x59 : [0x65, 0x49, 0x35], 0x5A : [0x74, 0x5C, 0x7D],
        0x5B : [0x2C, 0x5D, 0x24], 0x5C : [0x77, 0x27, 0x2D], 0x5D : [0x44, 0x59, 0x54],
        0x5E : [0x3F, 0x25, 0x37], 0x5F : [0x6D, 0x7C, 0x7B], 0x60 : [0x7C, 0x23, 0x3D],
        0x61 : [0x43, 0x6D, 0x6C], 0x62 : [0x38, 0x28, 0x34], 0x63 : [0x5E, 0x31, 0x6D],
        0x64 : [0x5B, 0x39, 0x4E], 0x65 : [0x6E, 0x7F, 0x2B], 0x66 : [0x57, 0x36, 0x30],
        0x67 : [0x4C, 0x54, 0x6F], 0x68 : [0x34, 0x34, 0x74], 0x69 : [0x72, 0x62, 0x6B],
        0x6A : [0x25, 0x4E, 0x4C], 0x6B : [0x56, 0x30, 0x33], 0x6C : [0x73, 0x5E, 0x56],
        0x6D : [0x68, 0x73, 0x3A], 0x6E : [0x55, 0x09, 0x78], 0x6F : [0x47, 0x4B, 0x57],
        0x70 : [0x32, 0x61, 0x77], 0x71 : [0x35, 0x24, 0x3B], 0x72 : [0x2E, 0x4D, 0x44],
        0x73 : [0x64, 0x6B, 0x2F], 0x74 : [0x4F, 0x44, 0x59], 0x75 : [0x3B, 0x21, 0x45],
        0x76 : [0x2D, 0x37, 0x5C], 0x77 : [0x41, 0x53, 0x68], 0x78 : [0x61, 0x58, 0x36],
        0x79 : [0x7A, 0x48, 0x58], 0x7A : [0x22, 0x2E, 0x79], 0x7B : [0x60, 0x50, 0x09],
        0x7C : [0x6B, 0x2D, 0x75], 0x7D : [0x4E, 0x29, 0x38], 0x7E : [0x3D, 0x3F, 0x55],
        0x7F : [0x67, 0x2F, 0x51]
    }

    _ESCAPE = {
        0x40: B&#39;@$&#39;,
        0x3C: B&#39;@!&#39;,
        0x3E: B&#39;@*&#39;,
        0x0D: B&#39;@#&#39;,
        0x0A: B&#39;@&amp;&#39;,
    }

    _UNESCAPE = {
        B&#39;@$&#39;: B&#39;@&#39;,
        B&#39;@!&#39;: B&#39;&lt;&#39;,
        B&#39;@*&#39;: B&#39;&gt;&#39;,
        B&#39;@#&#39;: B&#39;\r&#39;,
        B&#39;@&amp;&#39;: B&#39;\n&#39;,
    }

    def __init__(
        self,
        marker: Arg.Switch(&#39;-m&#39;, &#39;--no-marker&#39;, off=True, help=(
            &#39;Do not require magic marker when encoding and do not search for &#39;
            &#39;marker when decoding.&#39;)
        ) = True
    ):
        super().__init__(marker=marker)

    @classmethod
    def _chunk(cls, byte, index):
        k = byte - 9
        c = cls._CHUNKS[k * 3 : k * 3 + 3]
        return c[cls._OFFSETS[index % 64]]

    def _escape(self, iterable):
        if self.args.marker:
            yield from self._MARKER_INIT
        for byte in iterable:
            try:
                yield from self._ESCAPE[byte]
            except KeyError:
                yield byte
        if self.args.marker:
            yield from self._MARKER_STOP

    def _unescape(self, data):
        def unescaper(m): return self._UNESCAPE[m[0]]
        return re.sub(RB&#39;@[$!*#&amp;]&#39;, unescaper, data)

    @classmethod
    def _decoded(cls, data):
        index = -1
        for byte in data:
            if byte &lt; 128:
                index += 1
            if (byte == 9 or 31 &lt; byte &lt; 128) and byte != 60 and byte != 62 and byte != 64:
                byte = cls._chunk(byte, index)
            yield byte

    @classmethod
    def _encoded(cls, data):
        for i, byte in enumerate(data):
            try:
                sequence = cls._ENCODER[byte]
            except KeyError:
                yield byte
            else:
                offset = cls._OFFSETS[i % 0x40]
                yield sequence[offset]

    def reverse(self, data):
        return bytearray(self._escape(self._encoded(data)))

    def process(self, data):
        if self.args.marker:
            match = formats.wshenc.search(data)
            if not match:
                raise ValueError(&#39;Encoded script marker was not found.&#39;)
            data = match[0][12:-12]
        return bytearray(self._decoded(self._unescape(data)))</code></pre>
</details>
</dd>
<dt id="refinery.shell.xchacha"><code class="flex name class">
<span>class <span class="ident">xchacha</span></span>
<span>(</span><span>key, stateful=False, discard=0, nonce=b'REFINERY', magic=b'', offset=0, rounds=20)</span>
</code></dt>
<dd>
<section class="desc"><p>XChaCha encryption and decryption. The nonce must be 24 bytes long.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/crypto/cipher/chacha.py#L82-L98" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class xchacha(LatinCipherUnit):
    &#34;&#34;&#34;
    XChaCha encryption and decryption. The nonce must be 24 bytes long.
    &#34;&#34;&#34;
    def keystream(self) -&gt; Iterable[int]:
        kdp, kdn, nonce = struct.unpack(&#39;&lt;Q8s8s&#39;, self.args.nonce)
        yield from LatinX(
            ChaChaCipher,
            (0, 1, 2, 3, 12, 13, 14, 15),
            self.args.key,
            kdn,
            kdp,
            nonce,
            self.args.magic,
            self.args.rounds,
            self.args.offset,
        )</code></pre>
</details>
</dd>
<dt id="refinery.shell.xfcc"><code class="flex name class">
<span>class <span class="ident">xfcc</span></span>
<span>(</span><span>variable='count', relative=False)</span>
</code></dt>
<dd>
<section class="desc"><p>The cross frame chunk count unit! It computes the number of times a chunk occurs across several frames
of input. It consumes all frames at its current level of the frame tree and counts the number of times
each item occurs in each of them. It converts a frame tree of depth 2 into a new frame tree of depth 2
where the parent of every leaf has this leaf as its only child. The leaves of this tree have been
enriched with a meta variable containing the number of times the corresponding chunk has occurred in
the input frame tree. This unit can be used to compute set intersections across frames as follows:</p>
<pre><code>(1) [| (2) [| dedup | xfcc -r t | iff t==1 ]| (3) ]
</code></pre>
<p>A sequence of chunks is emitted at (1), each of which has chunks extracted at (2). It is then important
to use dedup before calling xfcc, since xfcc performs an absolute count. The frame at (3) contains the
intersection of all datasets that were extracted at (2).</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/meta/xfcc.py#L9-L73" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class xfcc(Unit):
    &#34;&#34;&#34;
    The cross frame chunk count unit! It computes the number of times a chunk occurs across several frames
    of input. It consumes all frames at its current level of the frame tree and counts the number of times
    each item occurs in each of them. It converts a frame tree of depth 2 into a new frame tree of depth 2
    where the parent of every leaf has this leaf as its only child. The leaves of this tree have been
    enriched with a meta variable containing the number of times the corresponding chunk has occurred in
    the input frame tree. This unit can be used to compute set intersections across frames as follows:

        (1) [| (2) [| dedup | xfcc -r t | iff t==1 ]| (3) ]

    A sequence of chunks is emitted at (1), each of which has chunks extracted at (2). It is then important
    to use dedup before calling xfcc, since xfcc performs an absolute count. The frame at (3) contains the
    intersection of all datasets that were extracted at (2).
    &#34;&#34;&#34;
    def __init__(
        self,
        variable: Arg(help=&#39;The variable which is used as the accumulator&#39;) = &#39;count&#39;,
        relative: Arg.Switch(&#39;-r&#39;, help=&#39;Normalize the accumulator to a number between 0 and 1.&#39;) = False
    ):
        super().__init__(variable=variable, relative=relative)
        self._trunk = None
        self._store: Dict[Chunk, int] = defaultdict(int)

    def finish(self):
        vn = self.args.variable
        rc = self.args.relative
        if rc and self._store:
            maximum = max(self._store.values())
        for index, (chunk, count) in enumerate(self._store.items()):
            if rc:
                count /= maximum
            chunk.path[-2] = 0
            chunk.path[-1] = index
            chunk.meta[vn] = count
            yield chunk
        self._store.clear()

    def _getcount(self, chunk):
        try:
            count = int(chunk.meta[self.args.variable])
        except (AttributeError, KeyError, TypeError):
            return 1
        else:
            return count

    def filter(self, chunks: Iterable[Chunk]):
        it = iter(chunks)
        try:
            head = next(it)
        except StopIteration:
            return
        if len(head.path) &lt; 2:
            self.log_warn(F&#39;the current frame is nested {len(head.path)} layers deep, at least two layers are required.&#39;)
            yield head
            yield from it
            return
        trunk = head.path[:-2]
        store = self._store
        if trunk != self._trunk:
            yield from self.finish()
            self._trunk = trunk
        store[head] += self._getcount(head)
        for chunk in it:
            store[chunk] += self._getcount(chunk)</code></pre>
</details>
</dd>
<dt id="refinery.shell.xj0"><code class="flex name class">
<span>class <span class="ident">xj0</span></span>
<span>(</span><span>fmt='', all=False, one=False, raw=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Extracts a single field from a JSON document at depth 0. By default, the unit applies a heuristic to
extract remaining fields as metadata: String values are extracted only if they do not exceed 80
characters in length and do not contain any line breaks. Floating-point, integer, boolean values, and
lists of the latter are also extracted.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/formats/json.py#L52-L124" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class xj0(Unit):
    &#34;&#34;&#34;
    Extracts a single field from a JSON document at depth 0. By default, the unit applies a heuristic to
    extract remaining fields as metadata: String values are extracted only if they do not exceed 80
    characters in length and do not contain any line breaks. Floating-point, integer, boolean values, and
    lists of the latter are also extracted.
    &#34;&#34;&#34;
    def __init__(
        self,
        fmt: Unit.Arg.String(help=(
            &#39;Format expression for the output chunk; may use previously extracted JSON items. The default &#39;
            &#39;is {default}, which represents the input data.&#39;)) = &#39;&#39;,
        all: Unit.Arg.Switch(&#39;-a&#39;, group=&#39;META&#39;, help=&#39;Extract all other fields as metadata regardless of length and type.&#39;) = False,
        one: Unit.Arg.Switch(&#39;-x&#39;, group=&#39;META&#39;, help=&#39;Do not extract any other fields as metadata.&#39;) = False,
        raw: Unit.Arg.Switch(&#39;-r&#39;, help=&#39;Disable conversion of JSON strings to binary strings in metadata&#39;) = False,
    ):
        super().__init__(fmt=fmt, one=one, raw=raw, all=all)

    def process(self, data: Chunk):

        def convert(value, iskey=False):
            if self.args.raw:
                return value
            if isinstance(value, (float, int, bool)):
                return value
            if isinstance(value, str):
                return value.encode(self.codec)
            if iskey:
                raise TypeError
            if isinstance(value, dict):
                return {convert(k): convert(v) for k, v in value.items()}
            if isinstance(value, list):
                return [convert(k) for k in value]

        def acceptable(key, value, nested=False, convert=False):
            if not is_valid_variable_name(key):
                self.log_info(F&#39;rejecting item with invalid name {key}&#39;)
                return None
            if isinstance(value, (float, int, bool)):
                return value
            if isinstance(value, dict):
                if not self.args.all:
                    self.log_info(F&#39;rejecting item {key} with dictionary value&#39;)
                    return False
                return True
            if isinstance(value, list):
                if nested:
                    self.log_info(F&#39;rejecting item {key} containing a doubly nested list&#39;)
                    return False
                return all(acceptable(key, t, True) for t in value)
            if isinstance(value, str):
                if not self.args.all:
                    if len(value) not in range(1, 80):
                        self.log_info(F&#39;rejecting string item {key} because {len(value)} exceeds the length limit&#39;)
                        return False
                    if &#39;\n&#39; in value:
                        self.log_info(F&#39;rejecting string item {key} because it contains line breaks&#39;)
                        return False
                return True
            return False

        jdoc: dict = json.loads(data)
        if not isinstance(jdoc, dict):
            raise ValueError(&#39;The input must be a JSON dictionary.&#39;)
        meta = metavars(data)
        args = {k: convert(v) for k, v in jdoc.items() if acceptable(k, v)}
        used = set()
        data[:] = meta.format_bin(self.args.fmt, self.codec, [data], args, used)
        for u in used:
            args.pop(u, None)
        if not self.args.one:
            data.meta.update(args)
        return data</code></pre>
</details>
</dd>
<dt id="refinery.shell.xjl"><code class="flex name class">
<span>class <span class="ident">xjl</span></span>
</code></dt>
<dd>
<section class="desc"><p>Returns all JSON elements from a JSON iterable as individual outputs. When reversed, the unit
collects all chunks in the frame and wraps them as a JSON list.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/formats/json.py#L127-L160" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class xjl(Unit):
    &#34;&#34;&#34;
    Returns all JSON elements from a JSON iterable as individual outputs. When reversed, the unit
    collects all chunks in the frame and wraps them as a JSON list.
    &#34;&#34;&#34;

    def process(self, data):
        try:
            doc: Union[list, dict] = json.loads(data)
        except Exception:
            from refinery.units.pattern.carve_json import carve_json
            doc = data | carve_json | json.loads
        try:
            it = doc.values()
        except AttributeError:
            it = doc
        for item in it:
            yield json.dumps(item, indent=4).encode(self.codec)

    def reverse(self, data):
        return json.dumps(data.temp).encode(self.codec)

    def filter(self, chunks: Iterable[Chunk]):
        if not self.args.reverse:
            yield from chunks

        from refinery.lib.tools import begin

        if it := begin(chunks):
            head, rest = it
            collected = [head.decode(self.codec)]
            collected.extend(chunk.decode(self.codec) for chunk in rest)
            head.temp = collected
            yield head</code></pre>
</details>
</dd>
<dt id="refinery.shell.xkey"><code class="flex name class">
<span>class <span class="ident">xkey</span></span>
<span>(</span><span>range=slice(1, 32, None), no_alph=False, no_crib=False)</span>
</code></dt>
<dd>
<section class="desc"><p>The unit expects encrypted input which was encrypted byte-wise with a polyalphabetic key. It
attempts do determine this key by three methods:</p>
<ol>
<li>Known plaintext cribs: The unit contains a library of file signatures that are expected to
occur at specific offsets. It uses these to attempt a known-plaintext attack against the
input. If a key is found that is at most half the size of such a crib, it is returned.</li>
<li>Known alphabets: For each given key length, the input is split into slices that would have
been encrypted with a single byte for keys of that length. Each such slice undergoes a
character frequency analysis. If the histogram indicates that an alphabet of a small size
was used (i.e. base64), then the unit attempts to determine the key based on this.</li>
<li>Known high frequency glyph: Works if the plaintext contains one letter that occurs with
very high frequency, i.e. zero padding in PE or ELF files, and the space character in text.
Based on this assumption, the unit computes the most likely key. This method will work best
on uncompressed files that were encrypted with a short key.</li>
</ol></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/misc/xkey.py#L38-L246" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class xkey(Unit):
    &#34;&#34;&#34;
    The unit expects encrypted input which was encrypted byte-wise with a polyalphabetic key. It
    attempts do determine this key by three methods:

    1. Known plaintext cribs: The unit contains a library of file signatures that are expected to
       occur at specific offsets. It uses these to attempt a known-plaintext attack against the
       input. If a key is found that is at most half the size of such a crib, it is returned.
    2. Known alphabets: For each given key length, the input is split into slices that would have
       been encrypted with a single byte for keys of that length. Each such slice undergoes a
       character frequency analysis. If the histogram indicates that an alphabet of a small size
       was used (i.e. base64), then the unit attempts to determine the key based on this.
    3. Known high frequency glyph: Works if the plaintext contains one letter that occurs with
       very high frequency, i.e. zero padding in PE or ELF files, and the space character in text.
       Based on this assumption, the unit computes the most likely key. This method will work best
       on uncompressed files that were encrypted with a short key.
    &#34;&#34;&#34;

    _CRIBS: dict[range, dict[str, bytes | tuple[bytes | tuple[bytes, ...], ...]]] = {
        range(0, 64, 4): {
            &#39;ZIP&#39;           : (B&#39;PK\x03\x04&#39;, (B&#39;\x14\x00&#39;, B&#39;\x0A\x00&#39;), (B&#39;\x08\x00&#39;, B&#39;\x00\x00&#39;)),
            &#39;RAR&#39;           : (B&#39;Rar!\x1A\x07&#39;, (B&#39;\x01\x00&#39;, B&#39;\x00&#39;)),
            &#39;ZPAQ&#39;          : (B&#39;\x37\x6B\x53\x74\xA0\x31\x83\xD3\x8C\xB2\x28\xB0\xD3\x7A\x50\x51&#39;),
            &#39;ZSTD&#39;          : (B&#39;\x28\xB5\x2F\xFD&#39;),
            &#39;ZZip&#39;          : (B&#39;7z\xBC\xAF\x27\x1C&#39;, (B&#39;\x00\x02&#39;, B&#39;\x00\x03&#39;, B&#39;\x00\x04&#39;)),
            &#39;APLib&#39;         : (B&#39;AP32\x18\0\0\0&#39;),
            &#39;BZip&#39;          : (B&#39;BZh&#39;),
            &#39;DDS&#39;           : (B&#39;\x00\x00\x00\x01Bud1&#39;),
            &#39;ELF&#39;           : (B&#39;\x7FELF&#39;),
            &#39;JavaClass&#39;     : (B&#39;\xCA\xFE\xBA\xBE&#39;),
            &#39;LZIP&#39;          : (B&#39;LZIP&#39;),
            &#39;SZDD&#39;          : (B&#39;SZDD\x88\xF0\x27\x33&#39;),
            &#39;LZMA&#39;          : (B&#39;\x5D\x00\x00\x00&#39;),
            &#39;LZMA/XZ&#39;       : (B&#39;\xFD7zXZ&#39;),
            &#39;LZO&#39;           : (B&#39;\x89\x4c\x5a\x4f\x00\x0d\x0a\x1a\x0a&#39;),
            &#39;MachO/BE&#39;      : (B&#39;\xCA\xFE\xBA\xBE&#39;),
            &#39;MachO/LE&#39;      : (B&#39;\xBE\xBA\xFE\xCA&#39;),
            &#39;MSCF&#39;          : (B&#39;\x0A\x51\xE5\xC0&#39;),
            &#39;OleDocument&#39;   : (B&#39;\xD0\xCF\x11\xE0&#39;),
            &#39;PdfDocument&#39;   : (B&#39;%PDF-&#39;, _S(B&#39;12&#39;), (B&#39;.&#39;), _S(B&#39;0123456789&#39;), _S(B&#39;\r\n&#39;)),
            &#39;SQLite&#39;        : (B&#39;SQLite format 3\0&#39;),
            &#39;GIF&#39;           : (B&#39;GIF87a&#39;, B&#39;GIF89a&#39;),
            &#39;PNG&#39;           : (B&#39;\x89PNG\r\n\x1A\n&#39;),
            &#39;DEX&#39;           : (B&#39;dex\n035\0&#39;),
            &#39;JPG&#39;           : (B&#39;\xFF\xD8\xFF&#39;, _S(B&#39;\xE0\xE1\xEE&#39;), (B&#39;\x00\x10\x4A\x46\x49\x46\x00\x01&#39;, B&#39;&#39;)),
            &#39;OneNote&#39;       : (B&#39;\xE4\x52\x5C\x7B\x8C\xD8\xA7\x4D\xAE\xB1\x53\x78\xD0\x29\x96\xD3&#39;),
            &#39;A3xScript&#39;     : (B&#39;\xA3\x48\x4B\xBE\x98\x6C\x4A\xA9\x99\x4C\x53\x0A\x86\xD6\x48\x7D&#39;),
            &#39;RTFDocument&#39;   : (B&#39;{\\rtf1&#39;, (B&#39;\\adeflang&#39;, B&#39;\\ansi&#39;, B&#39;&#39;)),
            &#39;CallToPop&#39;     : (B&#39;\xE8\0\0\0\0&#39;, (
                               B&#39;\x41\x58&#39;, B&#39;\x41\x59&#39;, B&#39;\x41\x5A&#39;, B&#39;\x41\x5B&#39;,
                                   B&#39;\x58&#39;,     B&#39;\x59&#39;,     B&#39;\x5A&#39;,     B&#39;\x5B&#39;,   # noqa
                                   B&#39;\x5C&#39;,     B&#39;\x5D&#39;,     B&#39;\x5E&#39;,     B&#39;\x5F&#39;,   # noqa
                               )),
            &#39;Cert&#39;          : (B&#39;-----BEGIN CERTIFICATE-----&#39;),
            &#39;PrivateKey&#39;    : (B&#39;-----BEGIN PRIVATE KEY-----&#39;),
            &#39;PrivateKeyDSA&#39; : (B&#39;-----BEGIN DSA PRIVATE KEY-----&#39;),
            &#39;PrivateKeyRSA&#39; : (B&#39;-----BEGIN RSA PRIVATE KEY-----&#39;),
            &#39;PrivateKeySSH&#39; : (B&#39;-----BEGIN OPENSSH PRIVATE KEY-----&#39;),
            &#39;PEM&#39;           : (B&#39;-----BEGIN &#39;),
            &#39;PuTTY-Key&#39;     : (B&#39;PuTTY-User-Key-File-&#39;, (B&#39;2:&#39;, B&#39;3:&#39;)),
            &#39;MsAccess&#39;      : (B&#39;\0\01\0\0Standard &#39;, (B&#39;ACE&#39;, B&#39;Jet&#39;), B&#39; DB&#39;),
        },
        range(0x10, 0x11): {
            &#39;ASAR&#39;          : (B&#39;{&#34;files&#34;:{&#34;&#39;),
        },
        range(0x40): {
            &#39;DocTypeLower&#39;  : (B&#39;&lt;!doctype&#39;),
            &#39;DocTypeUpper&#39;  : (B&#39;&lt;!DOCTYPE&#39;),
            &#39;HTMLLower&#39;     : (B&#39;&lt;html&gt;&#39;),
            &#39;HTMLUpper&#39;     : (B&#39;&lt;HTML&gt;&#39;),
            &#39;XML&#39;           : (B&#39;&lt;?xml version=&#34;&#39;),
            &#39;Ace&#39;           : (B&#39;**ACE**&#39;),
        },
        range(0x40, 0x60): {
            &#39;PEDelphiStub&#39;  : (B&#39;This program cannot be run in DOS mode.&#39;),
            &#39;PEStub&#39;        : (B&#39;This program must be run under Win&#39;, (B&#39;32&#39;, B&#39;64&#39;)),
        },
        range(0xD0, 0xD1): {
            &#39;Tar&#39;           : (B&#39;\x00&#39; * 0x30 + B&#39;ustar&#39;, (B&#39;\x20\x20\x00&#39;, B&#39;\x00\x30\x30&#39;)),
        },
        range(0x200): {
            &#39;EmailReceived&#39; : (B&#39;\nReceived:\x20from&#39;),
            &#39;EmailSubject&#39;  : (B&#39;\nSubject:\x20&#39;),
            &#39;EmailFrom&#39;     : (B&#39;\nFrom:\x20&#39;),
            &#39;EmailTo&#39;       : (B&#39;\nTo:\x20&#39;),
            &#39;PESectionData&#39; : (B&#39;.data\0\0\0&#39;),
            &#39;PESectionText&#39; : (B&#39;.text\0\0\0&#39;),
        },
    }

    _ENC_ALPHABETS = [
        B&#39;0123456789,&#39;,
        B&#39;0123456789;&#39;,
        B&#39;0123456789ABCDEF&#39;,
        B&#39;0123456789abcdef&#39;,
        B&#39;ABCDEFGHIJKLMNOPQRSTUVWXYZ234567&#39;,
        B&#39;abcdefghijklmnopqrstuvwxyz234567&#39;,
        B&#39;0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ+/&#39;,
        B&#39;0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ-_&#39;,
    ]

    _WSH_ALPHABET = bytes(set(range(0x20, 0x80)) - {0x3C, 0x3E} | {0x09})

    def __init__(
        self,
        range: Arg.Bounds(help=&#39;range of length values to try in Python slice syntax, the default is {default}.&#39;) = slice(1, 32),
        no_alph: Arg.Switch(&#39;-na&#39;, &#39;--no-alph&#39;, help=&#39;disables search for keys via known encoder alphabets&#39;) = False,
        no_crib: Arg.Switch(&#39;-nc&#39;, &#39;--no-crib&#39;, help=&#39;disables search for keys via known plaintext cribs&#39;) = False,
    ):
        super().__init__(range=range, no_alph=no_alph, no_crib=no_crib)

    def process(self, data: bytearray):
        score = 0
        guess = None
        bounds: slice = self.args.range
        view = memoryview(data)

        n = len(view)

        if n &lt;= 1:
            return view
        if n &gt;= 0x100:
            view = view[:-4]

        start = bounds.start or 1
        stop = min(bounds.stop or n, n)

        if bounds.step is not None:
            step = bounds.step
            if bounds.start is None:
                start *= step
        else:
            step = 1

        self.log_debug(F&#39;received input range [{bounds.start}:{bounds.stop}:{bounds.step}], using [{start}:{stop}:{step}]&#39;)

        if not self.args.no_crib:
            for offsets, cribs_by_type in self._CRIBS.items():
                for name, cribs in cribs_by_type.items():
                    cribs = _generate_cribs(cribs)
                    for offset, crib in product(offsets, cribs):
                        if len(crib) &lt; start:
                            continue
                        if len(crib) &gt; stop:
                            continue
                        if (len(crib) - start) % step != 0:
                            continue
                        test = view[offset:offset + len(crib)]
                        if len(test) != len(crib):
                            continue
                        key = _cyclic_base(strxor(test, crib))
                        if key is not None:
                            self.log_info(F&#39;found key via crib for {name}:&#39;, crib)
                            shift = -offset % len(key)
                            return key[shift:] + key[:shift]

        if not self.args.no_alph:
            alphabets: dict[int, list[bytes]] = {}
            for alphabet in self._ENC_ALPHABETS:
                for suffix in (B&#39;&#39;, B&#39;\x20&#39;, B&#39;\x0A&#39;, B&#39;\x20\x0A&#39;):
                    a = alphabet + suffix
                    alphabets.setdefault(len(a), []).append(a)

            alphabets[len(self._WSH_ALPHABET)] = self._WSH_ALPHABET

        for keylen in range(start, stop + 1, step):
            patches = [view[j::keylen] for j in range(keylen)]
            histograms = [Counter(p) for p in patches]

            if not self.args.no_alph:
                hlc = Counter(len(h) for h in histograms)
                base, coverage = hlc.most_common(1)[0]

                if coverage * 2 &gt; keylen and base in alphabets:
                    self.log_info(F&#39;detected potential plaintext alphabet of size 0x{base:02X} at {keylen}&#39;)
                    keys: dict[bytes, bytes] = {}
                    for alphabet in alphabets[base]:
                        key = bytearray(keylen)
                        for k, patch in enumerate(patches):
                            keybyte = set(range(0x100))
                            for x in patch:
                                keybyte &amp;= {y ^ x for y in alphabet}
                                if len(keybyte) == 1:
                                    key[k] = next(iter(keybyte))
                                    break
                            else:
                                key = None
                                break
                        if key is not None:
                            keys[alphabet] = key
                    if len(keys) == 1:
                        alphabet, key = keys.popitem()
                        return key

            _guess = [h.most_common(1)[0] for h in histograms]
            _score = sum(letter_count for _, letter_count in _guess) / n
            # This scaling accounts for the smaller probability of larger keys. No proper statistical analysis has been
            # conducted to derive it; there might be plenty of room for improvement here.
            _score = _score * ((n - keylen) / (n - 1)) ** keylen

            logmsg = F&#39;got score {_score * 100:5.2f}% for length {keylen}&#39;
            if _score &gt; score:
                self.log_info(logmsg)
                score = _score
                guess = bytearray(value for value, _ in _guess)
            else:
                self.log_debug(logmsg)

        return guess</code></pre>
</details>
</dd>
<dt id="refinery.shell.xlmdeobf"><code class="flex name class">
<span>class <span class="ident">xlmdeobf</span></span>
<span>(</span><span>extract_only=False, sort_formulas=False, with_ms_excel=False, day=-1, output_formula_format='CELL:[[CELL-ADDR]], [[STATUS]], [[INT-FORMULA]]', extract_formula_format='CELL:[[CELL-ADDR]], [[CELL-FORMULA]], [[CELL-VALUE]]', no_indent=False, start_point='', password='', output_level=0, timeout=0)</span>
</code></dt>
<dd>
<section class="desc"><p>Wrapper around XLMMacroDeobfuscator to decode obfuscated Excel v4.0 (XLM) macros.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/formats/office/xlmdeobf.py#L10-L106" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class xlmdeobf(Unit):
    &#34;&#34;&#34;
    Wrapper around XLMMacroDeobfuscator to decode obfuscated Excel v4.0 (XLM) macros.
    &#34;&#34;&#34;

    def __init__(
        self,
        extract_only: Unit.Arg.Switch(
            &#39;-x&#39;, help=&#39;Only extract cells without any emulation.&#39;
        ) = False,
        sort_formulas: Unit.Arg.Switch(
            &#39;-s&#39;, &#39;--sort-formulas&#39;,
            help=&#39;Sort extracted formulas based on their cell address (implies -x).&#39;,
        ) = False,
        with_ms_excel: Unit.Arg.Switch(
            &#39;-X&#39;, &#39;--with-ms-excel&#39;, help=&#39;Use MS Excel to process XLS files.&#39;
        ) = False,
        day: Unit.Arg.Number(
            &#39;-d&#39;,
            &#39;--day&#39;,
            help=&#39;Specify the day of month&#39;,
        ) = -1,
        output_formula_format: Unit.Arg(
            &#39;-O&#39;, &#39;--output-format&#39;,
            type=str,
            metavar=&#39;FMT&#39;,
            help=&#39;Specify the format for output formulas (using [[CELL-ADDR]], [[INT-FORMULA]], and [[STATUS]])&#39;,
        ) = &#39;CELL:[[CELL-ADDR]], [[STATUS]], [[INT-FORMULA]]&#39;,
        extract_formula_format: Unit.Arg(
            &#39;-E&#39;, &#39;--extract-format&#39;,
            metavar=&#39;FMT&#39;,
            type=str,
            help=&#39;Specify the format for extracted formulas (using [[CELL-ADDR]], [[CELL-FORMULA]], and [[CELL-VALUE]])&#39;,
        ) = &#39;CELL:[[CELL-ADDR]], [[CELL-FORMULA]], [[CELL-VALUE]]&#39;,
        no_indent: Unit.Arg.Switch(
            &#39;-I&#39;, &#39;--no-indent&#39;,
            help=&#39;Do not show indent before formulas&#39;,
        ) = False,
        start_point: Unit.Arg(
            &#39;-c&#39;, &#39;--start-point&#39;,
            type=str,
            help=&#39;Start interpretation from a specific cell address&#39;,
            metavar=&#39;CELL&#39;,
        ) = &#39;&#39;,
        password: Unit.Arg(
            &#39;-p&#39;,
            &#39;--password&#39;,
            type=str,
            help=&#39;Password to decrypt the protected document&#39;,
        ) = &#39;&#39;,
        output_level: Unit.Arg.Number(
            &#39;-o&#39;,
            &#39;--output-level&#39;,
            help=(
                &#39;Set the level of details to be shown (0:all commands, 1: commands no jump 2:important &#39;
                &#39;commands 3:strings in important commands).&#39;
            ),
        ) = 0,
        timeout: Unit.Arg.Number(
            &#39;-t&#39;,
            &#39;--timeout&#39;,
            help=&#39;Stop emulation after N seconds (0: not interruption N&gt;0: stop emulation after N seconds)&#39;,
        ) = 0,
    ):
        extract_only = sort_formulas or extract_only
        self.superinit(super(), **vars())

    @Unit.Requires(&#39;XLMMacroDeobfuscator&#39;, &#39;formats&#39;, &#39;office&#39;)
    def _process_file():
        with NoLogging(NoLogging.Mode.ALL):
            from XLMMacroDeobfuscator.configs import settings
            settings.SILENT = True
            from XLMMacroDeobfuscator.deobfuscator import process_file
            return process_file

    def process(self, data: bytearray):
        with VirtualFileSystem() as vfs, NoLogging():
            result = self._process_file(
                file=vfs.new(data),
                noninteractive=True,
                return_deobfuscated=True,
                extract_only=self.args.extract_only,
                silent=True,
                sort_formulas=self.args.sort_formulas,
                defined_names=False,
                with_ms_excel=self.args.with_ms_excel,
                start_with_shell=False,
                day=self.args.day,
                output_formula_format=self.args.output_formula_format,
                extract_formula_format=self.args.extract_formula_format,
                no_indent=self.args.no_indent,
                start_point=self.args.start_point,
                password=self.args.password,
                output_level=self.args.output_level,
                timeout=self.args.timeout,
            )
        return &#39;\n&#39;.join(result).encode(self.codec)</code></pre>
</details>
</dd>
<dt id="refinery.shell.xlxtr"><code class="flex name class">
<span>class <span class="ident">xlxtr</span></span>
<span>(</span><span>*references)</span>
</code></dt>
<dd>
<section class="desc"><p>Extract data from Microsoft Excel documents, both Legacy and new XML type documents. A sheet
reference is of the form <code>B1</code> or <code>1.2</code>, both specifying the first cell of the second column.
A cell range can be specified as <code>B1:C12</code>, or <code>1.2:C12</code>, or <code>1.2:12.3</code>. Finally, the unit will
always refer to the first sheet in the document and to change this, specify the sheet name or
index separated by a hashtag, i.e. <code>sheet#B1:C12</code> or <code>1#B1:C12</code>. Note that indices are
1-based. To get all elements of one sheet, use <code>sheet#</code>. The unit If parsing a sheet reference
fails, the script will assume that the given reference specifies a sheet.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/formats/office/xlxtr.py#L260-L297" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class xlxtr(_ExcelUnit):
    &#34;&#34;&#34;
    Extract data from Microsoft Excel documents, both Legacy and new XML type documents. A sheet
    reference is of the form `B1` or `1.2`, both specifying the first cell of the second column.
    A cell range can be specified as `B1:C12`, or `1.2:C12`, or `1.2:12.3`. Finally, the unit will
    always refer to the first sheet in the document and to change this, specify the sheet name or
    index separated by a hashtag, i.e. `sheet#B1:C12` or `1#B1:C12`. Note that indices are
    1-based. To get all elements of one sheet, use `sheet#`. The unit If parsing a sheet reference
    fails, the script will assume that the given reference specifies a sheet.
    &#34;&#34;&#34;
    def __init__(self, *references: Arg(metavar=&#39;reference&#39;, type=SheetReference, help=(
        &#39;A sheet reference to be extracted. &#39;
        &#39;If no sheet references are given, the unit lists all sheet names.&#39;
    ))):
        if not references:
            references = [SheetReference(&#39;*&#39;)]
        super().__init__(references=references)

    def process(self, data):
        wb = Workbook(data, self)
        for ref in self.args.references:
            ref: SheetReference
            for k, name in enumerate(wb.sheets()):
                if not ref.match(k, name):
                    continue
                for r, row in enumerate(wb.get_sheet_data(name), 1):
                    for c, value in enumerate(row, 1):
                        if (r, c) not in ref:
                            continue
                        if value is None:
                            continue
                        yield self.labelled(
                            str(value).encode(self.codec),
                            row=r,
                            col=c,
                            ref=_rc2ref(r, c),
                            sheet=name
                        )</code></pre>
</details>
</dd>
<dt id="refinery.shell.xor"><code class="flex name class">
<span>class <span class="ident">xor</span></span>
<span>(</span><span>argument, bigendian=False, blocksize=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Form the exclusive or of the input data with the given argument.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/blockwise/xor.py#L6-L30" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class xor(BinaryOperationWithAutoBlockAdjustment):
    &#34;&#34;&#34;
    Form the exclusive or of the input data with the given argument.
    &#34;&#34;&#34;
    @staticmethod
    def operate(a, b): return a ^ b
    @staticmethod
    def inplace(a, b): a ^= b

    def _fastblock(self, data):
        try:
            return super()._fastblock(data)
        except FastBlockError as E:
            try:
                from Cryptodome.Util.strxor import strxor
            except ModuleNotFoundError:
                raise E
            else:
                from itertools import islice
                size = len(data)
                arg0 = self._normalize_argument(*self._argument_parse_hook(self.args.argument[0]))
                take = len(data) // self.blocksize + 1
                argb = self.unchunk(islice(arg0, take))
                del argb[size:]
                return strxor(data, argb)</code></pre>
</details>
</dd>
<dt id="refinery.shell.xsalsa"><code class="flex name class">
<span>class <span class="ident">xsalsa</span></span>
<span>(</span><span>key, stateful=False, discard=0, nonce=b'REFINERY', magic=b'', offset=0, rounds=20)</span>
</code></dt>
<dd>
<section class="desc"><p>XSalsa encryption and decryption. The nonce must be 24 bytes long.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/crypto/cipher/salsa.py#L170-L186" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class xsalsa(LatinCipherUnit):
    &#34;&#34;&#34;
    XSalsa encryption and decryption. The nonce must be 24 bytes long.
    &#34;&#34;&#34;
    def keystream(self) -&gt; Iterable[int]:
        kdn, kdp, nonce = struct.unpack(&#39;&lt;8sQ8s&#39;, self.args.nonce)
        yield from LatinX(
            SalsaCipher,
            (0, 5, 10, 15, 6, 7, 8, 9),
            self.args.key,
            kdn,
            kdp,
            nonce,
            self.args.magic,
            self.args.rounds,
            self.args.offset,
        )</code></pre>
</details>
</dd>
<dt id="refinery.shell.xt"><code class="flex name class">
<span>class <span class="ident">xt</span></span>
<span>(</span><span>*paths, list=False, join_path=False, drop_path=False, fuzzy=0, exact=False, regex=False, path=b'path', date=b'date', pwd=b'')</span>
</code></dt>
<dd>
<section class="desc"><p>Extract files from archives. The unit tries to identify the archive format and use the
correct extractor.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/formats/archive/xt.py#L10-L166" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class xt(ArchiveUnit):
    &#34;&#34;&#34;
    Extract files from archives. The unit tries to identify the archive format and use the
    correct extractor.
    &#34;&#34;&#34;
    @classmethod
    def handles(cls, data: bytearray) -&gt; Optional[bool]:
        out = False
        for engine in cls.handlers():
            engine_verdict = engine.handles(data)
            if engine_verdict is True:
                return True
            if engine_verdict is None:
                out = None
        return out

    @staticmethod
    def handlers():
        &#34;&#34;&#34;
        Returns all archive handlers supported by the unit.
        &#34;&#34;&#34;
        from refinery.units.formats.archive.xtinno import xtinno
        yield xtinno
        from refinery.units.formats.winreg import winreg
        yield winreg
        from refinery.units.formats.office.xtone import xtone
        yield xtone
        from refinery.units.formats.archive.xtgz import xtgz
        yield xtgz
        from refinery.units.formats.email import xtmail
        yield xtmail
        from refinery.units.formats.pdf import xtpdf
        yield xtpdf
        from refinery.units.formats.archive.xtasar import xtasar
        yield xtasar
        from refinery.units.formats.office.xtrtf import xtrtf
        yield xtrtf
        from refinery.units.formats.archive.xtzpaq import xtzpaq
        yield xtzpaq
        from refinery.units.formats.pe.dotnet.dnsfx import dnsfx
        yield dnsfx
        from refinery.units.formats.archive.xtnsis import xtnsis
        yield xtnsis
        from refinery.units.formats.archive.xtnode import xtnode
        yield xtnode
        from refinery.units.formats.archive.xtace import xtace
        yield xtace
        from refinery.units.formats.archive.xtcab import xtcab
        yield xtcab
        from refinery.units.formats.archive.xtcpio import xtcpio
        yield xtcpio
        from refinery.units.formats.archive.xtiso import xtiso
        yield xtiso
        from refinery.units.formats.archive.xtpyi import xtpyi
        yield xtpyi
        from refinery.units.formats.archive.xttar import xttar
        yield xttar
        from refinery.units.formats.archive.xtiss import xtiss
        yield xtiss
        from refinery.units.formats.archive.xtzip import xtzip
        yield xtzip
        from refinery.units.formats.archive.xt7z import xt7z
        yield xt7z
        from refinery.units.formats.msi import xtmsi
        yield xtmsi
        from refinery.units.formats.archive.xtmacho import xtmacho
        yield xtmacho
        from refinery.units.formats.archive.xtnuitka import xtnuitka
        yield xtnuitka
        from refinery.units.formats.office.xtdoc import xtdoc
        yield xtdoc
        from refinery.units.formats.json import xtjson
        yield xtjson
        from refinery.units.formats.exe.vsect import vsect
        yield vsect

    def unpack(self, data):
        fallback: List[Type[ArchiveUnit]] = []
        errors = {}
        pos_args = self.args.paths
        key_args = dict(
            list=self.args.list,
            path=self.args.path,
            date=self.args.date,
            join_path=self.args.join,
            drop_path=self.args.drop,
        )
        if self.args.pwd:
            key_args.update(pwd=self.args.pwd)
        if self.args.regex:
            key_args.update(regex=self.args.regex)

        class unpacker:
            unit = self

            def __init__(self, handler: Type[ArchiveUnit], fallback: bool):
                self.success = False
                self.handler = handler
                self.fallback = fallback

            def __iter__(self):
                handler = self.handler
                if self.fallback:
                    verdict = True
                else:
                    verdict = handler.handles(data)
                if verdict is False:
                    self.unit.log_info(F&#39;rejected: {handler.name}&#39;)
                elif verdict is True:
                    if not self.fallback:
                        self.unit.log_info(F&#39;accepted: {handler.name}&#39;)
                    try:
                        unit = handler(*pos_args, **key_args)
                        unit.args.lenient = self.unit.args.lenient
                        unit.args.quiet = self.unit.args.quiet
                    except TypeError as error:
                        self.unit.log_debug(&#39;handler construction failed:&#39;, error)
                        return
                    try:
                        test_unpack = not self.unit.args.list
                        for item in unit.unpack(data):
                            if test_unpack:
                                item.get_data()
                                test_unpack = False
                            yield item
                    except Exception as error:
                        if not self.fallback:
                            errors[handler.name] = error
                        if isinstance(error, MultipleArchives):
                            self.unit.log_warn(error)
                        else:
                            self.unit.log_debug(&#39;handler unpacking failed:&#39;, error)
                    else:
                        self.success = True
                elif verdict is None:
                    fallback.append(handler)

        for handler in self.handlers():
            self.CustomPathSeparator = handler.CustomPathSeparator
            it = unpacker(handler, fallback=False)
            yield from it
            if it.success:
                return

        self.log_debug(&#39;fallback order:&#39;, lambda: &#39;, &#39;.join(h.name for h in fallback))

        for handler in fallback:
            it = unpacker(handler, fallback=True)
            yield from it
            if it.success:
                return

        if not errors:
            raise ValueError(&#39;input data did not match any known archive format&#39;)
        for name, error in errors.items():
            self.log_info(F&#39;error when trying to unpack with {name}:&#39;, error)
        raise RefineryException(&#39;none of the available unpackers could handle this data&#39;)</code></pre>
</details>
</dd>
<dt id="refinery.shell.xt7z"><code class="flex name class">
<span>class <span class="ident">xt7z</span></span>
<span>(</span><span>*paths, list=False, join_path=False, drop_path=False, fuzzy=0, exact=False, regex=False, path=b'path', date=b'date', pwd=b'')</span>
</code></dt>
<dd>
<section class="desc"><p>Extract files from a 7zip archive.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/formats/archive/xt7z.py#L15-L95" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class xt7z(ArchiveUnit):
    &#34;&#34;&#34;
    Extract files from a 7zip archive.
    &#34;&#34;&#34;
    @ArchiveUnit.Requires(&#39;py7zr&#39;, &#39;arc&#39;, &#39;default&#39;, &#39;extended&#39;)
    def _py7zr():
        import py7zr
        import py7zr.exceptions
        return py7zr

    def unpack(self, data: bytearray):
        for match in re.finditer(re.escape(B&#39;7z\xBC\xAF\x27\x1C&#39;), data):
            start = match.start()
            if start != 0:
                self.log_info(F&#39;found a header at offset 0x{start:X}, trying to extract from there.&#39;)
            try:
                yield from self._unpack_from(data, start)
            except self._py7zr.Bad7zFile:
                continue
            else:
                break

    def _unpack_from(self, data: bytearray, zp: int = 0):
        def mk7z(**keywords):
            return self._py7zr.SevenZipFile(MemoryFile(mv[zp:]), **keywords)

        pwd = self.args.pwd
        mv = memoryview(data)

        def test(archive: SevenZipFile):
            if self.args.list:
                archive.list()
                return False
            return archive.testzip()

        if pwd:
            try:
                archive = mk7z(password=pwd.decode(self.codec))
            except self._py7zr.Bad7zFile:
                raise ValueError(&#39;corrupt archive; the password is likely invalid.&#39;)
        else:
            def passwords():
                yield None
                yield from self._COMMON_PASSWORDS
            for pwd in passwords():
                if pwd is None:
                    self.log_debug(U&#39;trying empty password&#39;)
                else:
                    self.log_debug(F&#39;trying password: {pwd}&#39;)
                try:
                    archive = mk7z(password=pwd)
                    problem = test(archive)
                except self._py7zr.PasswordRequired:
                    problem = True
                except self._py7zr.UnsupportedCompressionMethodError as E:
                    raise ValueError(E.message)
                except self._py7zr.exceptions.InternalError:
                    # ignore internal errors during testzip
                    break
                except SystemError:
                    problem = True
                except Exception:
                    if pwd is None:
                        raise
                    problem = True
                if not problem:
                    break
            else:
                raise ValueError(&#39;a password is required and none of the default passwords worked.&#39;)

        for info in archive.list():
            def extract(archive: SevenZipFile = archive, info: FileInfo = info):
                archive.reset()
                return archive.read([info.filename]).get(info.filename).read()
            if info.is_directory:
                continue
            yield self._pack(info.filename, info.creationtime, extract, crc32=info.crc32, uncompressed=info.uncompressed)

    @classmethod
    def handles(cls, data: bytearray) -&gt; bool:
        return B&#39;7z\xBC\xAF\x27\x1C&#39; in data</code></pre>
</details>
</dd>
<dt id="refinery.shell.xtace"><code class="flex name class">
<span>class <span class="ident">xtace</span></span>
<span>(</span><span>*paths, list=False, join_path=False, drop_path=False, fuzzy=0, exact=False, regex=False, path=b'path', date=b'date', pwd=b'')</span>
</code></dt>
<dd>
<section class="desc"><p>Extract files from an ACE archive.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/formats/archive/xtace.py#L8-L26" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class xtace(ArchiveUnit):
    &#34;&#34;&#34;
    Extract files from an ACE archive.
    &#34;&#34;&#34;
    def unpack(self, data):
        ace = acefile.open(MemoryFile(data, read_as_bytes=True))
        for member in ace.getmembers():
            member: acefile.AceMember
            comment = {} if not member.comment else {&#39;comment&#39;: member.comment}
            yield self._pack(
                member.filename,
                member.datetime,
                lambda a=ace, m=member: a.read(m, pwd=self.args.pwd),
                **comment
            )

    @classmethod
    def handles(cls, data: bytearray) -&gt; bool:
        return b&#39;**ACE**&#39; in data[:0x100]</code></pre>
</details>
</dd>
<dt id="refinery.shell.xtasar"><code class="flex name class">
<span>class <span class="ident">xtasar</span></span>
<span>(</span><span>*paths, list=False, join_path=False, drop_path=False, fuzzy=0, exact=False, regex=False, path=b'path', date=b'date', pwd=b'')</span>
</code></dt>
<dd>
<section class="desc"><p>Extract files from a ASAR archive.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/formats/archive/xtasar.py#L32-L66" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class xtasar(ArchiveUnit):
    &#34;&#34;&#34;
    Extract files from a ASAR archive.
    &#34;&#34;&#34;
    def unpack(self, data: bytearray):
        def _unpack(dir: JSONDict, *path):
            for name, listing in dir.get(&#39;files&#39;, {}).items():
                yield from _unpack(listing, *path, name)
            try:
                offset = dir[&#39;offset&#39;]
                size = dir[&#39;size&#39;]
            except KeyError:
                return
            try:
                offset = int(offset) + header.base
                end = int(size) + offset
            except TypeError:
                self.log_warn(F&#39;unable to convert offset &#34;{offset}&#34; and size &#34;{size}&#34; to integers&#39;)
                return
            if not path:
                self.log_warn(F&#39;not processing item at root with offset {offset} and size {size}&#39;)
                return
            yield UnpackResult(
                &#39;/&#39;.join(path),
                lambda a=offset, b=end: data[a:b],
                offset=offset
            )

        header = AsarHeader(data)
        self.log_debug(F&#39;header read successfully, base offset is {header.base}.&#39;)
        yield from _unpack(header.directory)

    @classmethod
    def handles(cls, data: bytearray) -&gt; Optional[bool]:
        return data.startswith(b&#39;\04\0\0\0&#39;) and data[0x10:0x18] == B&#39;{&#34;files&#34;&#39;</code></pre>
</details>
</dd>
<dt id="refinery.shell.xtcab"><code class="flex name class">
<span>class <span class="ident">xtcab</span></span>
<span>(</span><span>*paths, list=False, join_path=False, drop_path=False, fuzzy=0, exact=False, regex=False, path=b'path', date=b'date', pwd=b'')</span>
</code></dt>
<dd>
<section class="desc"><p>Extract files from CAB (cabinet) archives.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/formats/archive/xtcab.py#L8-L24" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class xtcab(ArchiveUnit):
    &#34;&#34;&#34;
    Extract files from CAB (cabinet) archives.
    &#34;&#34;&#34;
    @ArchiveUnit.Requires(&#39;cabarchive&#39;, &#39;arc&#39;, &#39;default&#39;, &#39;extended&#39;)
    def _cabarchive():
        import cabarchive
        return cabarchive

    def unpack(self, data: bytearray):
        arc = self._cabarchive.CabArchive(data)
        for item in arc.find_files(&#39;*&#39;):
            yield self._pack(item.filename, datetime.combine(item.date, item.time), item.buf)

    @classmethod
    def handles(cls, data: bytearray):
        return data.startswith(B&#39;MSCF&#39;)</code></pre>
</details>
</dd>
<dt id="refinery.shell.xtcpio"><code class="flex name class">
<span>class <span class="ident">xtcpio</span></span>
<span>(</span><span>*paths, list=False, join_path=False, drop_path=False, fuzzy=0, exact=False, regex=False, path=b'path', date=b'date', pwd=b'')</span>
</code></dt>
<dd>
<section class="desc"><p>Extract files from a CPIO archive.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/formats/archive/xtcpio.py#L35-L56" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class xtcpio(ArchiveUnit):
    &#34;&#34;&#34;
    Extract files from a CPIO archive.
    &#34;&#34;&#34;
    def unpack(self, data):
        def cpio():
            with suppress(EOF): return CPIOEntry(reader)
        reader = StructReader(memoryview(data))
        for entry in iter(cpio, None):
            if entry.name == &#39;TRAILER!!!&#39;:
                break
            yield self._pack(entry.name, entry.mtime, entry.data)

    @classmethod
    def handles(cls, data: bytearray) -&gt; bool:
        for signature in (B&#39;\x71\xC7&#39;, B&#39;\xC7\x71&#39;, B&#39;0707&#39;):
            if data.startswith(signature):
                if B&#39;TRAILER!!&#39; in data:
                    return True
                else:
                    return None
        return False</code></pre>
</details>
</dd>
<dt id="refinery.shell.xtdoc"><code class="flex name class">
<span>class <span class="ident">xtdoc</span></span>
<span>(</span><span>*paths, list=False, join_path=False, drop_path=False, fuzzy=0, exact=False, regex=False, path=b'path')</span>
</code></dt>
<dd>
<section class="desc"><p>Extract files from an OLE document such as a Microsoft Word DOCX file.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/formats/office/xtdoc.py#L23-L63" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class xtdoc(PathExtractorUnit):
    &#34;&#34;&#34;
    Extract files from an OLE document such as a Microsoft Word DOCX file.
    &#34;&#34;&#34;

    @PathExtractorUnit.Requires(&#39;olefile&#39;, &#39;formats&#39;, &#39;office&#39;, &#39;extended&#39;)
    def _olefile():
        import olefile
        return olefile

    def unpack(self, data):
        with MemoryFile(data) as stream:
            try:
                oledoc = self._olefile.OleFileIO(stream)
            except OSError as error:
                self.log_info(F&#39;error, {error}, treating input as zip file&#39;)
                yield from xtzip().unpack(data)
                return
            for item in oledoc.listdir():
                if not item or not item[-1]:
                    continue
                path = &#39;/&#39;.join(item)
                olestream = oledoc.openstream(path)
                c0 = ord(item[-1][:1])
                if c0 &lt; 20:
                    item[-1] = F&#39;[{c0:d}]{item[-1][1:]}&#39;
                    path = &#39;/&#39;.join(item)
                path = convert_msi_name(path)
                self.log_debug(&#39;exploring:&#39;, path)
                yield UnpackResult(path, olestream.read())

    @classmethod
    def handles(self, data: bytearray) -&gt; Optional[bool]:
        if data.startswith(B&#39;\xD0\xCF\x11\xE0&#39;):
            return True
        if xtzip.handles(data):
            return sum(1 for marker in [
                B&#39;[Content_Types].xml&#39;,
                B&#39;word/document.xml&#39;,
                B&#39;docProps/core.xml&#39;,
            ] if marker in data) &gt;= 2</code></pre>
</details>
</dd>
<dt id="refinery.shell.xtea"><code class="flex name class">
<span>class <span class="ident">xtea</span></span>
<span>(</span><span>key, iv=b'', padding=None, mode=None, raw=False, swap=False)</span>
</code></dt>
<dd>
<section class="desc"><p>XTEA encryption and decryption.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/crypto/cipher/xtea.py#L39-L43" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class xtea(TEAUnit, cipher=BlockCipherFactory(XTEA)):
    &#34;&#34;&#34;
    XTEA encryption and decryption.
    &#34;&#34;&#34;
    pass</code></pre>
</details>
</dd>
<dt id="refinery.shell.xtgz"><code class="flex name class">
<span>class <span class="ident">xtgz</span></span>
<span>(</span><span>*paths, list=False, join_path=False, drop_path=False, fuzzy=0, exact=False, regex=False, path=b'path', date=b'date', pwd=b'')</span>
</code></dt>
<dd>
<section class="desc"><p>Extract a file from a GZip archive.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/formats/archive/xtgz.py#L29-L56" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class xtgz(ArchiveUnit):
    &#34;&#34;&#34;
    Extract a file from a GZip archive.
    &#34;&#34;&#34;
    def unpack(self, data: bytearray):
        archive = GzipHeader(data)
        path = archive.name
        date = archive.mtime
        date = date and datetime.fromtimestamp(date) or None
        if path is None:
            try:
                meta = metavars(data)
                path = Path(meta[&#39;path&#39;])
            except KeyError:
                path = &#39;ungz&#39;
            else:
                self.log_warn(path)
                suffix = path.suffix
                if suffix.lower() == &#39;.gz&#39;:
                    path = path.with_suffix(&#39;&#39;)
                else:
                    path = path.with_suffix(F&#39;{suffix}.ungz&#39;)
                path = path.as_posix()
        yield self._pack(path, date, archive.data)

    @classmethod
    def handles(cls, data: bytearray) -&gt; bool:
        return data.startswith(B&#39;\x1F\x8B&#39;)</code></pre>
</details>
</dd>
<dt id="refinery.shell.xthtml"><code class="flex name class">
<span>class <span class="ident">xthtml</span></span>
<span>(</span><span>*paths, outer=False, attributes=False, list=False, join_path=False, drop_path=False, fuzzy=0, exact=False, regex=False, path=b'path')</span>
</code></dt>
<dd>
<section class="desc"><p>The unit processes an HTML document and extracts the contents of all elemnts in the DOM of the
given tag. The main purpose is to extract scripts from HTML documents.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/formats/html.py#L117-L198" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class xthtml(XMLToPathExtractorUnit):
    &#34;&#34;&#34;
    The unit processes an HTML document and extracts the contents of all elemnts in the DOM of the
    given tag. The main purpose is to extract scripts from HTML documents.
    &#34;&#34;&#34;
    def __init__(
        self, *paths,
        outer: Arg.Switch(&#39;-o&#39;, help=&#39;Include the HTML tags for an extracted element.&#39;) = False,
        attributes: Arg.Switch(&#39;-a&#39;, help=&#39;Populate chunk metadata with HTML tag attributes.&#39;) = False,
        list=False, join_path=False, drop_path=False, fuzzy=0, exact=False, regex=False,
        path=b&#39;path&#39;
    ):
        super().__init__(
            *paths,
            outer=outer,
            attributes=attributes,
            format=&#39;{tag}&#39;,
            path=path,
            list=list,
            join_path=join_path,
            drop_path=drop_path,
            fuzzy=fuzzy,
            exact=exact,
            regex=regex,
        )

    def unpack(self, data):
        html = HTMLTreeParser()
        html.feed(data.decode(self.codec))
        root = html.tos
        root.reindex()

        meta = metavars(data)
        path = self._make_path_builder(meta, root)

        while root.parent:
            self.log_info(F&#39;tag was not closed: {root.tag}&#39;)
            root = root.parent

        while len(root.children) == 1:
            child, = root.children
            if child.tag != root.tag:
                break
            root = child

        def tree(root: HTMLNode, *parts: str):

            def outer(root: HTMLNode = root):
                return root.recover(inner=False).encode(self.codec)

            def inner(root: HTMLNode = root):
                return root.recover().encode(self.codec)

            tagpath = &#39;/&#39;.join(parts)
            meta = {}

            if self.args.attributes:
                meta.update(root.attributes)

            if root.root:
                yield UnpackResult(tagpath, inner, **meta)
            elif self.args.outer:
                yield UnpackResult(tagpath, outer, **meta)
            else:
                yield UnpackResult(tagpath, inner, **meta)

            for child in root.children:
                if child.textual:
                    continue
                yield from tree(child, *parts, path(child))

        yield from tree(root, path(root))

    @classmethod
    def handles(self, data: bytearray):
        from refinery.lib import mime
        info = mime.get_cached_file_magic_info(data)
        if info.extension == &#39;html&#39;:
            return True
        if info.mime.endswith(&#39;html&#39;):
            return True
        return False</code></pre>
</details>
</dd>
<dt id="refinery.shell.xtinno"><code class="flex name class">
<span>class <span class="ident">xtinno</span></span>
<span>(</span><span>*paths, list=False, join_path=False, drop_path=False, fuzzy=0, exact=False, regex=False, path=b'path', date=b'date', pwd=b'')</span>
</code></dt>
<dd>
<section class="desc"><p>Extract files from InnoSetup archives.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/formats/archive/xtinno.py#L2046-L2662" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class xtinno(ArchiveUnit):
    &#34;&#34;&#34;
    Extract files from InnoSetup archives.
    &#34;&#34;&#34;
    _STREAM_NAMES = &#39;meta/TSetup&#39;, &#39;meta/TData&#39;, &#39;embedded/uninstaller.exe&#39;
    _ISCRIPT_NAME = &#39;embedded/script&#39;
    _LICENSE_NAME = &#39;embedded/license.rtf&#39;
    _OFFSETS_PATH = &#39;RCDATA/11111/0&#39;
    _CHUNK_PREFIX = b&#39;zlb\x1a&#39;
    _MAX_ATTEMPTS = 200_000

    def unpack(self, data: bytearray):
        try:
            file_metadata = one(data | perc(self._OFFSETS_PATH))
        except Exception as E:
            raise ValueError(F&#39;Could not find TSetupOffsets PE resource at {self._OFFSETS_PATH}&#39;) from E

        meta = TSetupOffsets(file_metadata)
        view = memoryview(data)
        base = meta.base
        inno = StructReader(view[base:base + meta.total_size])

        self._decompressed = {}
        password = self.args.pwd or None

        blobsize = meta.setup0 - meta.setup1
        inno.seek(meta.setup1)
        blobs = StructReader(inno.read(blobsize))

        header = bytes(inno.read(16))

        try:
            version = InnoVersion.ParseLegacy(header)
        except ValueError:
            header += bytes(inno.read(64 - 16))
            try:
                version = InnoVersion.Parse(header)
            except ValueError:
                name, _, _rest = header.partition(b&#39;\0&#39;)
                method = &#39;broken&#39;
                if any(_rest):
                    header = name.hex()
                else:
                    header = name.decode(&#39;latin1&#39;)
                if self.leniency &lt; 1:
                    raise ValueError(F&#39;unable to parse header identifier &#34;{header}&#34;&#39;)
                version = _DEFAULT_INNO_VERSION
            else:
                header, _, _ = header.partition(B&#39;\0&#39;)
                header = header.decode(&#39;latin1&#39;)
                method = &#39;modern&#39;
        else:
            header, _, _ = header.partition(B&#39;\x1A&#39;)
            header = header.decode(&#39;latin1&#39;)
            method = &#39;legacy&#39;

        self.log_info(F&#39;inno {version!s} via {method} header: {header}&#39;)

        class _notok(object):
            def __init__(self, e: Exception):
                self.failures = [str(e)]

            def ok(self):
                return False

        def _parse(v: InnoVersion):
            try:
                inno.seekset(inno_start)
                if v.legacy:
                    inno.seekrel(-48)
                r = self._try_parse_as(inno, blobs, v, password)
            except Exception as e:
                self.log_info(F&#39;exception while parsing as {v!s}: {exception_to_string(e)}&#39;)
                return _notok(e)
            else:
                results[v] = r
                return r

        inno_start = inno.tell()
        best_parse = None
        best_score = 0
        success = False
        results: dict[InnoVersion, InnoParseResult] = {}

        if not version.is_ambiguous():
            index = _VERSIONS.index(version)
        else:
            try:
                index = max(k for k, v in enumerate(_VERSIONS) if v &lt;= version)
            except Exception:
                index = 0

        upper = index + 1
        lower = index

        while lower &gt; 0 and _IS_AMBIGUOUS[_VERSIONS[lower - 1]]:
            lower -= 1

        versions = [version] + _VERSIONS[lower:index] + _VERSIONS[upper:] + _VERSIONS[:lower]

        for v in versions:
            result = _parse(v)
            if success := result.ok():
                break
            if not result.failures and (best_parse is None or result.warnings &lt; best_score):
                best_score = best_score
                best_parse = result

        if not success:
            if best_parse is not None:
                result = best_parse
                self.log_warn(F&#39;using parse result for {result.version!s} with {result.warnings} warnings&#39;)
            else:
                result = min(results.values(), key=lambda result: len(result.failures))
                self.log_warn(F&#39;using parse result for {result.version!s} with {len(result.failures)} failures&#39;)
                for k, failure in enumerate(result.failures, 1):
                    self.log_info(F&#39;failure {k}: {failure}&#39;)

        version = result.version
        codec = result.stream0.Codec
        stream0 = result.stream0
        stream1 = result.stream1
        files = result.files
        streams = result.streams
        encrypted_file = result.encrypted_file

        if version.unicode:
            codec = &#39;latin1&#39;
        self.log_info(
            F&#39;inno {version!s} &#39;
            F&#39;compression:{stream0.Header.CompressionMethod.name} &#39;
            F&#39;codepage:{codec} &#39;
            F&#39;password:{stream0.Header.PasswordType.name} &#39;
        )

        if (script := stream0.Header.CompiledCode) and encrypted_file:
            assert password is None, (
                F&#39;An encrypted test file was chosen even though a password was provided: {password!r}&#39;)
            self.log_info(F&#39;guessing password using encrypted file: {encrypted_file.path}&#39;)
            try:
                def _pwd(s: str):
                    if not s:
                        return False
                    if re.search(r&#39;\{\w+\}&#39;, s):
                        return False
                    if re.search(R&#39;^\w{2,12}://&#39;, s):
                        return False
                    return True
                from refinery.units.formats.ifpsstr import ifpsstr
                from itertools import combinations
                strings = script | ifpsstr(codec) | {str}
                strings = [s for s in strings if _pwd(s)]
                total = 0
                for k in range(1, 10):
                    self.log_info(F&#39;checking combinations of {k} strings as potential password&#39;)
                    if total &gt; self._MAX_ATTEMPTS:
                        break
                    for parts in combinations(strings, k):
                        if total &gt; self._MAX_ATTEMPTS:
                            break
                        string = &#39;&#39;.join(parts)
                        tests = [string]
                        try:
                            tests.append(bytes.fromhex(string))
                        except Exception:
                            pass
                        try:
                            tests.append(base64.b64decode(string).decode(codec))
                        except Exception:
                            pass
                        for pwd in tests:
                            try:
                                plaintext = self._read_file(encrypted_file, password=pwd)
                                if not encrypted_file.check(plaintext):
                                    continue
                                password = pwd
                                btw = F&#39; (combination of {k} parts)&#39; if k &gt; 1 else &#39;&#39;
                                self.log_info(F&#39;found password{btw}: {pwd}&#39;)
                                break
                            except Exception:
                                continue
                            finally:
                                total += 1
                        if password is not None:
                            break
            except Exception as e:
                self.log_info(F&#39;failed to extract strings from IFPS: {exception_to_string(e)}&#39;)

        if encrypted_file and password is None:
            self.log_warn(&#39;some files are password-protected and automatic password search failed&#39;)

        yield self._pack(streams[0].name, None, streams[0].data)
        with BytesAsStringEncoder as encoder:
            yield self._pack(F&#39;{streams[0].name}.json&#39;, None,
                encoder.dumps(stream0.json()).encode(self.codec))

        yield self._pack(streams[1].name, None, streams[1].data)
        with BytesAsStringEncoder as encoder:
            yield self._pack(F&#39;{streams[1].name}.json&#39;, None,
                encoder.dumps(stream1.json()).encode(self.codec))

        yield self._pack(
            streams[2].name, None, lambda s=streams[2]: self._read_stream(s))

        if license := stream0.Header.get_license():
            yield self._pack(self._LICENSE_NAME, None, license.encode(self.codec))

        if script:
            from refinery.units.formats.ifps import ifps
            yield self._pack(F&#39;{self._ISCRIPT_NAME}.ps&#39;, None, script | ifps(codec) | bytes)
            yield self._pack(F&#39;{self._ISCRIPT_NAME}.bin&#39;, None, script)

        if dll := stream0.DecompressDLL:
            yield self._pack(F&#39;embedded/decompress.{magic(dll).extension}&#39;, None, dll)

        if dll := stream0.DecryptionDLL:
            yield self._pack(F&#39;embedded/decryption.{magic(dll).extension}&#39;, None, dll)

        for size, images in (
            (&#39;small&#39;, stream0.WizardImagesSmall),
            (&#39;large&#39;, stream0.WizardImagesLarge),
        ):
            _formatting = len(str(len(images) + 1))
            for k, img in enumerate(images, 1):
                yield self._pack(F&#39;embedded/images/{size}{k:0{_formatting}d}.{magic(img).extension}&#39;, None, img)

        for file in files:
            if file.dupe:
                continue
            yield self._pack(
                file.path,
                file.date,
                lambda f=file: self._read_file_and_check(f, password=password),
                tags=[t.name for t in SetupFileFlags if t &amp; file.tags],
            )

    def _try_parse_as(
        self,
        inno: StructReader,
        blobs: StructReader,
        version: InnoVersion,
        password: Optional[str] = None,
        max_failures: int = 5
    ):
        streams: List[InnoStream] = []
        files: List[InnoFile] = []
        encrypted_file = None
        warnings = 0

        for name in self._STREAM_NAMES:
            stream = InnoStream(StreamHeader(inno, name, version))
            streams.append(stream)
            to_read = stream.header.StoredSize
            while to_read &gt; 4:
                block = CrcCompressedBlock(inno, min(to_read - 4, 0x1000))
                stream.blocks.append(block)
                to_read -= len(block)

        self.log_debug(F&#39;{version!s} parsing stream 1 (TData)&#39;)
        stream1 = TData(memoryview(self._read_stream(streams[1])), version)

        for meta in stream1.DataEntries:
            file = InnoFile(blobs, version, meta)
            files.append(file)
            if password or not file.encrypted or not file.size:
                continue
            if encrypted_file is None or file.size &lt; encrypted_file.size:
                encrypted_file = file

        self.log_debug(F&#39;{version!s} parsing stream 0 (TSetup)&#39;)
        stream0 = TSetup(memoryview(self._read_stream(streams[0])), version)
        path_dedup: dict[str, list[SetupFile]] = {}

        for file in files:
            file.compression_method = stream0.Header.CompressionMethod
            file.password_hash = stream0.Header.PasswordHash
            file.password_type = stream0.Header.PasswordType
            file.password_salt = stream0.Header.PasswordSalt

        for sf in stream0.Files:
            sf: SetupFile
            location = sf.Location
            if location == 0xFFFFFFFF or sf.Type != SetupFileType.UserFile or sf.Source:
                msg = F&#39;skipping file: offset=0x{location:08X} type={sf.Type.name}&#39;
                if sf.Source:
                    msg = F&#39;{msg} src={sf.Source}&#39;
                self.log_debug(msg)
                continue
            if location &gt;= len(files):
                self.log_warn(F&#39;parsed {len(file)} entries, ignoring invalid setup reference to entry {location + 1}&#39;)
                continue
            path = sf.Destination.replace(&#39;\\&#39;, &#39;/&#39;)
            if condition := sf.Condition.Check:
                condition = condition.replace(&#39; &#39;, &#39;-&#39;)
                path = F&#39;{condition}/{path}&#39;
            path = F&#39;data/{path}&#39;
            path_dedup.setdefault(path, []).append(sf)
            files[location].setup = sf
            files[location].path = path

        for path, infos in path_dedup.items():
            if len(infos) == 1:
                files[infos[0].Location].path = path
                continue
            bycheck = {}
            for info in infos:
                file = files[info.Location]
                if not file.checksum_type.strong():
                    bycheck.clear()
                    break
                dkey = (file.checksum, file.size)
                if dkey in bycheck:
                    self.log_debug(F&#39;skipping exact duplicate: {path}&#39;)
                    file.dupe = True
                    continue
                bycheck[dkey] = info
            if bycheck:
                if len(bycheck) == 1:
                    file.path = path
                    continue
                infos = list(bycheck.values())
            for k, info in enumerate(infos):
                files[info.Location].path = F&#39;{path}[{k}]&#39;

        _width = len(str(len(files)))

        for k, file in enumerate(files):
            if file.dupe:
                continue
            if not file.path:
                self.log_debug(F&#39;file {k} does not have a path&#39;)
                file.path = F&#39;raw/FileData{k:0{_width}d}&#39;

        warnings = sum(1 for file in files if file.setup is None)
        failures = []
        nonempty = [f for f in files if f.size &gt; 0]

        self._decompressed.clear()

        for file in nonempty:
            if len(failures) &gt;= max_failures:
                break
            if file.setup is None:
                failures.append(F&#39;file {file.path} had no associated metadata&#39;)
                continue
            if file.chunk_length &lt; 0x10000:
                try:
                    data = self._read_file(file)
                except InvalidPassword:
                    continue
                except Exception as e:
                    failures.append(F&#39;extraction error for {file.path}: {e!s}&#39;)
                    continue
                if file.check(data) != file.checksum:
                    failures.append(F&#39;invalid checksum for {file.path}&#39;)

        return InnoParseResult(
            version,
            streams,
            files,
            encrypted_file,
            warnings,
            failures,
            stream0,
            stream1,
        )

    def _read_stream(self, stream: InnoStream):
        if stream.data is not None:
            return stream.data
        result = bytearray()
        it = iter(stream.blocks)
        if stream.compression == StreamCompressionMethod.Store:
            class _dummy(self):
                def decompress(self, b):
                    return b
            dec = _dummy()
        elif stream.compression == StreamCompressionMethod.LZMA1:
            import lzma
            first = next(it).BlockData
            prop, first = first[:5], first[5:]
            filter = parse_lzma_properties(prop, 1)
            dec = lzma.LZMADecompressor(lzma.FORMAT_RAW, filters=[filter])
            result.extend(dec.decompress(first))
        elif stream.compression == StreamCompressionMethod.Flate:
            import zlib
            dec = zlib.decompressobj()
        for block in it:
            result.extend(dec.decompress(block.BlockData))
        stream.data = result
        return result

    def _read_chunk(self, file: InnoFile, password: Optional[str] = None):
        reader = file.reader
        offset = file.chunk_offset
        length = file.chunk_length
        method = file.compression

        if offset + length &gt; len(reader):
            span = F&#39;0x{offset:X}-0x{offset + length:X}&#39;
            raise LookupError(
                F&#39;File data spans 0x{len(file.reader):X} bytes, but the file {file.path} is located at {span}.&#39;)

        reader.seek(offset)
        prefix = reader.read(4)

        if prefix != self._CHUNK_PREFIX:
            raise ValueError(F&#39;Error reading chunk at offset 0x{offset:X}; invalid magic {prefix.hex()}.&#39;)

        if file.encrypted:
            if file.password_type == PasswordType.Nothing:
                raise RuntimeError(F&#39;File {file.path} is encrypted, but no password type was set.&#39;)
            if password is None:
                raise InvalidPassword
            if file.password_type == PasswordType.XChaCha20:
                salt, iterations, nonce = struct.unpack(&#39;=16sI24s&#39;, file.password_salt)
                key = password.encode(&#39;utf8&#39;) | pbkdf2(32, salt, iterations, &#39;SHA256&#39;) | bytes
                test_nonce = list(struct.unpack(&#39;6I&#39;, nonce))
                test_nonce[2] = ~test_nonce[2]
                test_nonce = struct.pack(&#39;6I&#39;, test_nonce)
                if B&#39;\0\0\0\0&#39; | xchacha(key, nonce=test_nonce) | bytes != file.password_hash:
                    raise InvalidPassword(password)
                decryptor = xchacha(key, nonce=nonce)
            else:
                password_bytes = password.encode(
                    &#39;utf-16le&#39; if file.unicode else &#39;utf8&#39;)
                algorithm = {
                    PasswordType.SHA1: sha1,
                    PasswordType.MD5 : md5,
                }[file.password_type]
                hash = algorithm(b&#39;PasswordCheckHash&#39; + file.password_salt)
                hash.update(password_bytes)
                if hash.digest() != file.password_hash:
                    raise InvalidPassword(password)
                hash = algorithm(reader.read(8))
                hash.update(password_bytes)
                decryptor = rc4(hash.digest(), discard=1000)

        data = reader.read_exactly(length)

        if file.encrypted:
            data = data | decryptor | bytearray

        if method is None:
            return chunk

        try:
            if method == CompressionMethod.Store:
                chunk = data
            elif method == CompressionMethod.LZMA1:
                props = parse_lzma_properties(data[0:5], 1)
                dec = lzma.LZMADecompressor(lzma.FORMAT_RAW, filters=[props])
                chunk = dec.decompress(data[5:])
            elif method == CompressionMethod.LZMA2:
                props = parse_lzma_properties(data[0:1], 2)
                dec = lzma.LZMADecompressor(lzma.FORMAT_RAW, filters=[props])
                chunk = dec.decompress(data[1:])
            elif method == CompressionMethod.BZip2:
                chunk = bz2.decompress(data)
            elif method == CompressionMethod.Flate:
                chunk = zlib.decompress(data)
        except Exception as E:
            if not file.encrypted:
                raise
            raise InvalidPassword(password) from E

        return chunk

    def _read_file(
        self,
        file: InnoFile,
        password: Optional[str] = None,
    ):
        offset = file.chunk_offset
        length = file.chunk_length

        try:
            chunk = self._decompressed[offset, length]
        except KeyError:
            chunk = self._decompressed[offset, length] = self._read_chunk(file, password)

        view = memoryview(chunk)
        data = view[file.offset:file.offset + file.size]

        if file.filtered:
            if file.version &gt;= (5, 2, 0):
                flip = (file.version &gt;= (5, 3, 9))
                data = self._filter_new(data, flip_high_byte=flip)
            else:
                data = self._filter_old(data)

        return data

    def _read_file_and_check(
        self,
        file: InnoFile,
        password: Optional[str] = None,
    ):
        data = self._read_file(file, password)

        if not self.leniency and (cs := file.check(data)) is not None and cs != file.checksum:
            if isinstance(cs, int):
                computed = F&#39;{cs:08X}&#39;
                expected = F&#39;{file.checksum:08X}&#39;
            else:
                computed = cs.hex().upper()
                expected = file.checksum.hex().upper()
            raise ValueError(F&#39;checksum error; computed:{computed} expected:{expected} [ignore this check with -L]&#39;)

        return data

    @ArchiveUnit.Requires(&#39;numpy&#39;, &#39;speed&#39;, &#39;default&#39;, &#39;extended&#39;)
    def _numpy():
        import numpy
        return numpy

    def _filter_new(self, data: ByteStr, flip_high_byte=False):
        try:
            np = self._numpy
        except ImportError:
            return self._filter_new_fallback(data, flip_high_byte)
        u08 = np.uint8
        u32 = np.uint32
        ab0 = bytearray()
        ab1 = bytearray()
        ab2 = bytearray()
        ab3 = bytearray()
        positions = []
        if isinstance(data, bytearray):
            out = data
        else:
            out = bytearray(data)
        mem = memoryview(out)
        for k in range(0, len(mem), 0x10000):
            for match in re.finditer(B&#39;(?s)[\xE8\xE9]....&#39;, mem[k:k + 0x10000], flags=re.DOTALL):
                a = match.start() + k
                top = mem[a + 4]
                if top != 0x00 and top != 0xFF:
                    continue
                ab0.append(mem[a + 1])
                ab1.append(mem[a + 2])
                ab2.append(mem[a + 3])
                ab3.append(top)
                positions.append(a + 5)
        ab0 = np.frombuffer(ab0, dtype=u08)
        ab1 = np.frombuffer(ab1, dtype=u08)
        low = np.frombuffer(ab2, dtype=u08).astype(u32)
        msb = np.frombuffer(ab3, dtype=u08)
        sub = np.fromiter(positions, dtype=u32)
        low &lt;&lt;= 8
        low += ab1
        low &lt;&lt;= 8
        low += ab0
        low -= sub
        low &amp;= 0xFFFFFF
        if flip_high_byte:
            flips = low &gt;&gt; 23
            keeps = 1 - flips
            keeps *= msb
            msb ^= 0xFF
            msb *= flips
            msb += keeps
        low += (msb.astype(u32) &lt;&lt; 24)
        addresses = low.tobytes()
        for k, offset in enumerate(positions):
            out[offset - 4:offset] = addresses[k * 4:(k + 1) * 4]
        return out

    def _filter_new_fallback(self, data: ByteStr, flip_high_byte=False):
        block_size = 0x10000
        out = bytearray(data)
        i = 0
        while len(data) - i &gt;= 5:
            c = out[i]
            block_size_left = block_size - (i % block_size)
            i += 1
            if (c == 0xE8 or c == 0xE9) and block_size_left &gt;= 5:
                address = out[i:i + 4]
                i += 4
                if address[3] == 0 or address[3] == 0xFF:
                    rel = address[0] | address[1] &lt;&lt; 8 | address[2] &lt;&lt; 16
                    rel -= i &amp; 0xFFFFFF
                    out[i - 4] = rel &amp; 0xFF
                    out[i - 3] = (rel &gt;&gt; 8) &amp; 0xFF
                    out[i - 2] = (rel &gt;&gt; 16) &amp; 0xFF
                    if flip_high_byte and (rel &amp; 0x800000) != 0:
                        out[i - 1] = (~out[i - 1]) &amp; 0xFF
        return out

    @staticmethod
    def _filter_old(data: ByteStr):
        if not isinstance(data, bytearray):
            data = bytearray(data)
        addr_bytes_left = 0
        addr_offset = 5
        addr = 0
        for i, c in enumerate(data):
            if addr_bytes_left == 0:
                if c == 0xE8 or c == 0xE9:
                    addr = (~addr_offset + 1) &amp; 0xFFFFFFFF
                    addr_bytes_left = 4
            else:
                addr = (addr + c) &amp; 0xFFFFFFFF
                c = addr &amp; 0xFF
                addr = addr &gt;&gt; 8
                addr_bytes_left -= 1
            data[i] = c
        return data

    @classmethod
    def handles(self, data):
        if data[:2] != B&#39;MZ&#39;:
            return False
        if re.search(re.escape(self._CHUNK_PREFIX), data) is None:
            return False
        return bool(
            re.search(BR&#39;Inno Setup Setup Data \(\d+\.\d+\.&#39;, data))</code></pre>
</details>
</dd>
<dt id="refinery.shell.xtiso"><code class="flex name class">
<span>class <span class="ident">xtiso</span></span>
<span>(</span><span>*paths, list=False, join_path=False, drop_path=False, fuzzy=0, exact=False, regex=False, path=b'path', date=b'date', fs='auto')</span>
</code></dt>
<dd>
<section class="desc"><p>Extract files from a ISO archive.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/formats/archive/xtiso.py#L11-L122" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class xtiso(ArchiveUnit):
    &#34;&#34;&#34;
    Extract files from a ISO archive.
    &#34;&#34;&#34;
    def __init__(
        self,
        *paths, list=False, join_path=False, drop_path=False, fuzzy=0, exact=False, regex=False,
        path=b&#39;path&#39;, date=b&#39;date&#39;,
        fs: Arg.Choice(&#39;-s&#39;, metavar=&#39;TYPE&#39;, choices=_ISO_FILE_SYSTEMS, help=(
            &#39;Specify a file system ({choices}) extension to use. The default setting {default} will automatically &#39;
            &#39;detect the first of the other available options and use it.&#39;)) = &#39;auto&#39;
    ):
        if fs not in _ISO_FILE_SYSTEMS:
            raise ValueError(F&#39;invalid file system {fs}: must be udf, joliet, rr, iso, or auto.&#39;)
        super().__init__(
            *paths,
            list=list,
            join_path=join_path,
            drop_path=drop_path,
            fuzzy=fuzzy,
            exact=exact,
            regex=regex,
            path=path,
            date=date,
            fs=fs
        )

    @ArchiveUnit.Requires(&#39;pycdlib&#39;, &#39;arc&#39;, &#39;default&#39;, &#39;extended&#39;)
    def _pycdlib():
        import pycdlib
        import pycdlib.dates

        def fixed_parse(self, datestr):
            datestr = datestr[:-3] + b&#39;00\0&#39;
            return original_parse(self, datestr)

        original_parse = pycdlib.dates.VolumeDescriptorDate.parse
        pycdlib.dates.VolumeDescriptorDate.parse = fixed_parse
        return pycdlib

    @staticmethod
    def _strip_revision(name: str):
        base, split, revision = name.partition(&#39;;&#39;)
        return base if split and revision.isdigit() else name

    def unpack(self, data):
        if not self.handles(data):
            self.log_warn(&#39;The data does not look like an ISO file.&#39;)
        with MemoryFile(data, read_as_bytes=True) as stream:
            iso = self._pycdlib.PyCdlib()
            iso.open_fp(stream)
            fs = self.args.fs
            if fs != &#39;auto&#39;:
                mkfacade = {
                    &#39;iso&#39;    : iso.get_iso9660_facade,
                    &#39;udf&#39;    : iso.get_udf_facade,
                    &#39;joliet&#39; : iso.get_joliet_facade,
                    &#39;rr&#39;     : iso.get_rock_ridge_facade,
                }
                facade = mkfacade[fs]()
            elif iso.has_udf():
                self.log_info(&#39;using format: udf&#39;)
                facade = iso.get_udf_facade()
            elif iso.has_joliet():
                self.log_info(&#39;using format: joliet&#39;)
                facade = iso.get_joliet_facade()
            elif iso.has_rock_ridge():
                self.log_info(&#39;using format: rr&#39;)
                facade = iso.get_rock_ridge_facade()
            else:
                self.log_info(&#39;using format: iso&#39;)
                facade = iso.get_iso9660_facade()

            for root, _, files in facade.walk(&#39;/&#39;):
                root = root.rstrip(&#39;/&#39;)
                for name in files:
                    name = name.lstrip(&#39;/&#39;)
                    path = F&#39;{root}/{name}&#39;
                    try:
                        info = facade.get_record(path)
                        date = info.date
                    except Exception:
                        info = None
                        date = None
                    else:
                        date = datetime.datetime(
                            date.years_since_1900 + 1900,
                            date.month,
                            date.day_of_month,
                            date.hour,
                            date.minute,
                            date.second,
                            tzinfo=datetime.timezone(datetime.timedelta(minutes=15 * date.gmtoffset))
                        )

                    def extract(info=info, path=path):
                        if info:
                            buffer = MemoryFile(bytearray(info.data_length))
                        else:
                            buffer = MemoryFile(bytearray())
                        facade.get_file_from_iso_fp(buffer, path)
                        return buffer.getvalue()

                    yield self._pack(self._strip_revision(path), date, extract)

    @classmethod
    def handles(cls, data: bytearray) -&gt; bool:
        return any(data[k] == B&#39;CD001&#39; for k in (
            slice(0x8001, 0x8006),
            slice(0x8801, 0x8806),
            slice(0x9001, 0x9006),
        ))</code></pre>
</details>
</dd>
<dt id="refinery.shell.xtiss"><code class="flex name class">
<span>class <span class="ident">xtiss</span></span>
<span>(</span><span>*paths, list=False, join_path=False, drop_path=False, fuzzy=0, exact=False, regex=False, path=b'path', date=b'date', pwd=b'')</span>
</code></dt>
<dd>
<section class="desc"><p>Extracts files from Install Shield Setup files.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/formats/archive/xtiss.py#L71-L92" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class xtiss(ArchiveUnit):
    &#34;&#34;&#34;
    Extracts files from Install Shield Setup files.
    &#34;&#34;&#34;
    def unpack(self, data: bytearray):
        offset = max(data.rfind(magic) for magic in ISSReader.MAGIC)
        if offset &lt; 0:
            raise ValueError(&#39;ISS magic not found.&#39;)
        data[:offset] = []

        reader = ISSReader(data)
        count = reader.iss_archive_header()

        self.log_info(F&#39;archive contains {count} files according to header&#39;)

        for _ in range(count):
            name, data = reader.iss_file()
            yield self._pack(name, None, data)

    @classmethod
    def handles(cls, data: bytearray) -&gt; Optional[bool]:
        return data.startswith(B&#39;MZ&#39;) and any(data.find(m) &gt; 0 for m in ISSReader.MAGIC)</code></pre>
</details>
</dd>
<dt id="refinery.shell.xtjson"><code class="flex name class">
<span>class <span class="ident">xtjson</span></span>
<span>(</span><span>*paths, list=False, join_path=False, drop_path=False, fuzzy=0, exact=False, regex=False, path=b'path')</span>
</code></dt>
<dd>
<section class="desc"><p>Extract values from a JSON document.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/formats/json.py#L14-L49" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class xtjson(PathExtractorUnit):
    &#34;&#34;&#34;
    Extract values from a JSON document.
    &#34;&#34;&#34;
    CustomPathSeparator = &#39;.&#39;

    def unpack(self, data):

        sep = self.CustomPathSeparator

        def crawl(path, cursor):
            if isinstance(cursor, dict):
                for key, value in cursor.items():
                    yield from crawl(F&#39;{path}{sep}{key}&#39;, value)
            elif isinstance(cursor, list):
                for key, value in enumerate(cursor):
                    yield from crawl(F&#39;{path}{sep}{key:d}&#39;, value)
            if path:
                yield path, cursor, cursor.__class__.__name__

        for path, item, typename in crawl(&#39;&#39;, json.loads(data)):
            def extract(item=item):
                if isinstance(item, (list, dict)):
                    dumped = json.dumps(item, indent=4)
                else:
                    dumped = str(item)
                try:
                    return dumped.encode(&#39;latin1&#39;)
                except UnicodeEncodeError:
                    return dumped.encode(&#39;utf8&#39;)

            yield UnpackResult(path, extract, type=typename)

    @classmethod
    def handles(self, data: bytearray) -&gt; Optional[bool]:
        return bool(checks.json.fullmatch(data))</code></pre>
</details>
</dd>
<dt id="refinery.shell.xtmacho"><code class="flex name class">
<span>class <span class="ident">xtmacho</span></span>
<span>(</span><span>*paths, list=False, join_path=False, drop_path=False, fuzzy=0, exact=False, regex=False, path=b'path', date=b'date', pwd=b'')</span>
</code></dt>
<dd>
<section class="desc"><p>Extract the individual executables from a MachO universal binary (sometimes called a MachO fat file)."</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/formats/archive/xtmacho.py#L40-L72" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class xtmacho(ArchiveUnit):
    &#34;&#34;&#34;
    Extract the individual executables from a MachO universal binary (sometimes called a MachO fat file).&#34;
    &#34;&#34;&#34;
    _SIGNATURE_BE = B&#39;\xCA\xFE\xBA\xBE&#39;
    _SIGNATURE_LE = B&#39;\xBE\xBA\xFE\xCA&#39;

    def unpack(self, data: bytearray):
        view = memoryview(data)
        signature = bytes(view[:4])
        try:
            reader = StructReader(view, bigendian={
                self._SIGNATURE_BE: True,
                self._SIGNATURE_LE: False,
            }[signature])
        except KeyError as KE:
            raise ValueError(&#39;Not a MachO universal binary; invalid magic header bytes.&#39;) from KE
        else:
            reader.seekset(4)
        count = reader.u32()
        self.log_info(F&#39;reading {count} embedded executables&#39;)
        while count &gt; 0:
            fa = FatArch(reader)
            self.log_info(F&#39;reading item of size 0x{len(fa.data):08X}, arch {fa.cputype.name}&#39;)
            yield self._pack(fa.cputype.name, None, fa.data)
            count -= 1

    @classmethod
    def handles(cls, data: bytearray):
        return data[:4] in (
            cls._SIGNATURE_BE,
            cls._SIGNATURE_LE,
        )</code></pre>
</details>
</dd>
<dt id="refinery.shell.xtmagtape"><code class="flex name class">
<span>class <span class="ident">xtmagtape</span></span>
</code></dt>
<dd>
<section class="desc"><p>Extract files from SIMH magtape files.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/formats/archive/xtmagtape.py#L9-L43" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class xtmagtape(Unit):
    &#34;&#34;&#34;
    Extract files from SIMH magtape files.
    &#34;&#34;&#34;
    def process(self, data: bytearray):
        reader = StructReader(data)

        for r in itertools.count():
            buffer = MemoryFile()

            for k in itertools.count():
                try:
                    head = reader.peek(4)
                    size = reader.read_integer(24)
                    mark = reader.read_byte()
                except EOFError:
                    self.log_info(&#39;end of file while reading chunk header, terminating&#39;)
                    return
                if not any(head):
                    if k == 0:
                        return
                    break
                if mark != 0:
                    self.log_warn(F&#39;error code 0x{mark:02X} in record {r}.{k}&#39;)
                buffer.write(reader.read(size))
                if reader.peek(4) != head:
                    if reader.tell() % 2 and reader.peek(5)[1:] == head:
                        padding = reader.read_byte()
                        if padding != 0:
                            self.log_info(F&#39;nonzero padding byte in record {r}.{k}&#39;)
                    else:
                        raise ValueError(&#39;Invalid footer, data is corrupted.&#39;)
                reader.seekrel(4)

            yield buffer.getbuffer()</code></pre>
</details>
</dd>
<dt id="refinery.shell.xtmail"><code class="flex name class">
<span>class <span class="ident">xtmail</span></span>
<span>(</span><span>*paths, list=False, join_path=False, drop_path=False, fuzzy=0, exact=False, regex=False, path=b'path')</span>
</code></dt>
<dd>
<section class="desc"><p>Extract files and body from EMail messages. The unit supports both the Outlook message format
and regular MIME documents.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/formats/email.py#L16-L171" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class xtmail(PathExtractorUnit):
    &#34;&#34;&#34;
    Extract files and body from EMail messages. The unit supports both the Outlook message format
    and regular MIME documents.
    &#34;&#34;&#34;
    def _get_headparts(self, head):
        mw = mimewords()
        mw = partial(mw.process.__wrapped__.__wrapped__, mw)
        jh = defaultdict(list)
        for key, value in head:
            jh[key].append(mw(&#39;&#39;.join(t.lstrip() for t in value.splitlines(False))))
        jh = {k: v[0] if len(v) == 1 else [t for t in v if t] for k, v in jh.items()}
        yield UnpackResult(&#39;headers.txt&#39;,
            lambda h=head: &#39;\n&#39;.join(F&#39;{k}: {v}&#39; for k, v in h).encode(self.codec))
        yield UnpackResult(&#39;headers.json&#39;,
            lambda jsn=jh: json.dumps(jsn, indent=4).encode(self.codec))

    @PathExtractorUnit.Requires(&#39;extract-msg&lt;=0.41.0&#39;, &#39;formats&#39;, &#39;office&#39;, &#39;default&#39;, &#39;extended&#39;)
    def _extract_msg():
        import extract_msg.message
        import extract_msg.enums
        return extract_msg

    def _get_parts_outlook(self, data):
        def ensure_bytes(data):
            return data if isinstance(data, bytes) else data.encode(self.codec)

        def make_message(name, msg):
            with NoLogging():
                try:
                    htm = msg.htmlBody
                except Exception:
                    htm = None
                try:
                    txt = msg.body
                except Exception:
                    txt = None
            if txt:
                yield UnpackResult(F&#39;{name}.txt&#39;, ensure_bytes(txt))
            if htm:
                yield UnpackResult(F&#39;{name}.htm&#39;, ensure_bytes(htm))

        msgcount = 0

        with NoLogging():
            class ForgivingMessage(self._extract_msg.message.Message):
                &#34;&#34;&#34;
                If parsing the input bytes fails early, the &#34;__open&#34; private attribute may not
                yet exist. This hack prevents an exception to occur in the destructor.
                &#34;&#34;&#34;
                def __getattr__(self, key: str):
                    if key.endswith(&#39;_open&#39;):
                        return False
                    raise AttributeError(key)
            msg = ForgivingMessage(bytes(data))

        yield from self._get_headparts(msg.header.items())
        yield from make_message(&#39;body&#39;, msg)

        def attachments(msg):
            for attachment in getattr(msg, &#39;attachments&#39;, ()):
                yield attachment
                if attachment.type == &#39;data&#39;:
                    continue
                yield from attachments(attachment.data)

        for attachment in attachments(msg):
            at = attachment.type
            if at is self._extract_msg.enums.AttachmentType.MSG:
                msgcount += 1
                yield from make_message(F&#39;attachments/msg_{msgcount:d}&#39;, attachment.data)
                continue
            if not isbuffer(attachment.data):
                self.log_warn(F&#39;unknown attachment of type {at}, please report this!&#39;)
                continue
            path = attachment.longFilename or attachment.shortFilename
            yield UnpackResult(F&#39;attachments/{path}&#39;, attachment.data)

    @PathExtractorUnit.Requires(&#39;chardet&#39;, &#39;default&#39;, &#39;extended&#39;)
    def _chardet():
        import chardet
        return chardet

    def _get_parts_regular(self, data: bytes):
        try:
            info = self._chardet.detect(data)
            msg = data.decode(info[&#39;encoding&#39;])
        except UnicodeDecodeError:
            raise ValueError(&#39;This is not a plaintext email message.&#39;)
        else:
            msg = Parser().parsestr(msg)

        yield from self._get_headparts(msg.items())

        for k, part in enumerate(msg.walk()):
            path = part.get_filename()
            elog = None
            if path is None:
                extension = file_extension(part.get_content_type(), &#39;txt&#39;)
                path = F&#39;body.{extension}&#39;
            else:
                path = path | mimewords | str
                path = F&#39;attachments/{path}&#39;
            try:
                data = part.get_payload(decode=True)
            except Exception as E:
                try:
                    data = part.get_payload(decode=False)
                except Exception as E:
                    elog = str(E)
                    data = None
                else:
                    from refinery import carve
                    self.log_warn(F&#39;manually decoding part {k}, data might be corrupted: {path}&#39;)
                    if isinstance(data, str):
                        data = data.encode(&#39;latin1&#39;)
                    if isbuffer(data):
                        data = next(data | carve(&#39;b64&#39;, stripspace=True, single=True, decode=True))
                    else:
                        elog = str(E)
                        data = None
            if not data:
                if elog is not None:
                    self.log_warn(F&#39;could not get content of message part {k}: {elog!s}&#39;)
                continue
            yield UnpackResult(path, data)

    def unpack(self, data):
        try:
            yield from self._get_parts_outlook(data)
        except Exception:
            self.log_debug(&#39;failed parsing input as Outlook message&#39;)
            yield from self._get_parts_regular(data)

    @classmethod
    def handles(cls, data: bytearray) -&gt; bool:
        markers = [
            b&#39;\nReceived:\x20from&#39;
            b&#39;\nSubject:\x20&#39;,
            b&#39;\nTo:\x20&#39;,
            b&#39;\nFrom:\x20&#39;,
            B&#39;\nMessage-ID:\x20&#39;,
            b&#39;\nBcc:\x20&#39;,
            b&#39;\nContent-Transfer-Encoding:\x20&#39;,
            b&#39;\nContent-Type:\x20&#39;,
            b&#39;\nReturn-Path:\x20&#39;,
        ]
        if data.startswith(B&#39;\xD0\xCF\x11\xE0\xA1\xB1\x1A\xE1&#39;):
            markers = [marker.decode(&#39;latin1&#39;).encode(&#39;utf-16le&#39;) for marker in markers]
        counter = 0
        for marker in markers:
            if re.search(re.escape(marker), data, flags=re.IGNORECASE):
                counter += 1
            if counter &gt;= 3:
                return True
        return False</code></pre>
</details>
</dd>
<dt id="refinery.shell.xtmsi"><code class="flex name class">
<span>class <span class="ident">xtmsi</span></span>
<span>(</span><span>*paths, list=False, path=b'path', join_path=False, drop_path=False, fuzzy=0, exact=False, regex=False, nocab=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Extract files and metadata from Microsoft Installer (MSI) archives. The synthetic file MsiTables.json contains
parsed MSI table information, similar to the output of the Orca tool. Binary streams are placed in a
virtual folder called "Binary", and extracted scripts from custom actions are separately extracted in
a virtual folder named "Action".</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/formats/msi.py#L135-L426" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class xtmsi(xtdoc):
    &#34;&#34;&#34;
    Extract files and metadata from Microsoft Installer (MSI) archives. The synthetic file {FN} contains
    parsed MSI table information, similar to the output of the Orca tool. Binary streams are placed in a
    virtual folder called &#34;Binary&#34;, and extracted scripts from custom actions are separately extracted in
    a virtual folder named &#34;Action&#34;.
    &#34;&#34;&#34;

    _SYNTHETIC_STREAMS_FILENAME = &#39;MsiTables.json&#39;
    _SYNTHETIC_STREAMS_TOPLEVEL = &#39;MsiTables&#39;

    # https://learn.microsoft.com/en-us/windows/win32/msi/summary-list-of-all-custom-action-types
    _CUSTOM_ACTION_TYPES = {
        0x01: &#39;DLL file stored in a Binary table stream.&#39;,
        0x02: &#39;EXE file stored in a Binary table stream.&#39;,
        0x05: &#39;JScript file stored in a Binary table stream.&#39;,
        0x06: &#39;VBScript file stored in a Binary table stream.&#39;,
        0x11: &#39;DLL file that is installed with a product.&#39;,
        0x12: &#39;EXE file that is installed with a product.&#39;,
        0x13: &#39;Displays a specified error message and returns failure, terminating the installation.&#39;,
        0x15: &#39;JScript file that is installed with a product.&#39;,
        0x16: &#39;VBScript file that is installed with a product.&#39;,
        0x22: &#39;EXE file having a path referencing a directory.&#39;,
        0x23: &#39;Directory set with formatted text.&#39;,
        0x25: &#39;JScript text stored in this sequence table.&#39;,
        0x26: &#39;VBScript text stored in this sequence table.&#39;,
        0x32: &#39;EXE file having a path specified by a property value.&#39;,
        0x33: &#39;Property set with formatted text.&#39;,
        0x35: &#39;JScript text specified by a property value.&#39;,
        0x36: &#39;VBScript text specified by a property value.&#39;,
    }

    def __init__(
            self, *paths,
            list=False, path=b&#39;path&#39;, join_path=False, drop_path=False, fuzzy=0, exact=False, regex=False,
            nocab: Arg.Switch(&#39;-N&#39;, help=&#39;Do not list and extract embedded CAB archives.&#39;) = False, **kw,
    ):
        super().__init__(
            *paths,
            list=list,
            path=path,
            join_path=join_path,
            drop_path=drop_path,
            nocab=nocab,
            fuzzy=fuzzy,
            exact=exact,
            regex=regex,
            **kw,
        )

    def unpack(self, data):
        streams = {result.path: result for result in super().unpack(data)}

        def stream(name: str):
            return streams.pop(name).get_data()

        def column_formats(table: Dict[str, MSITableColumnInfo]) -&gt; str:
            return &#39;&#39;.join(v.struct_format for v in table.values())

        def stream_to_rows(data: ByteStr, row_format: str):
            row_size = struct.calcsize(F&#39;&lt;{row_format}&#39;)
            row_count = int(len(data) / row_size)
            reader = StructReader(data)
            columns = [reader.read_struct(F&#39;&lt;{sc * row_count}&#39;) for sc in row_format]
            for i in range(row_count):
                yield [c[i] for c in columns]

        tables: Dict[str, Dict[str, MSITableColumnInfo]] = collections.defaultdict(collections.OrderedDict)
        strings = MSIStringData(stream(&#39;!_StringData&#39;), stream(&#39;!_StringPool&#39;))

        for tbl_name_id, col_number, col_name_id, col_attributes in stream_to_rows(stream(&#39;!_Columns&#39;), &#39;HHHH&#39;):
            tbl_name = strings.ref(tbl_name_id)
            col_name = strings.ref(col_name_id)
            tables[tbl_name][col_name] = MSITableColumnInfo(col_number, col_attributes)

        table_names_given = {strings.ref(k) for k in chunks.unpack(stream(&#39;!_Tables&#39;), 2, False)}
        table_names_known = set(tables)

        for name in table_names_known - table_names_given:
            self.log_warn(F&#39;table name known but not given: {name}&#39;)
        for name in table_names_given - table_names_known:
            self.log_warn(F&#39;table name given but not known: {name}&#39;)

        class ScriptItem(NamedTuple):
            row_index: int
            extension: Optional[str]

        processed_table_data: Dict[str, List[Dict[str, str]]] = {}
        tbl_properties: Dict[str, str] = {}
        tbl_files: Dict[str, str] = {}
        tbl_components: Dict[str, str] = {}
        postprocessing: List[ScriptItem] = []

        def format_string(string: str):
            # https://learn.microsoft.com/en-us/windows/win32/msi/formatted
            def _replace(match: re.Match[str]):
                _replace.done = False
                prefix, name = match.groups()
                if not prefix:
                    tbl = tbl_properties
                elif prefix in &#39;%&#39;:
                    name = name.rstrip(&#39;%&#39;).upper()
                    return F&#39;%{name}%&#39;
                elif prefix in &#39;!#&#39;:
                    tbl = tbl_files
                elif prefix in &#39;$&#39;:
                    tbl = tbl_components
                else:
                    raise ValueError
                return tbl.get(name, &#39;&#39;)
            while True:
                _replace.done = True
                string = re.sub(R&#39;&#39;&#39;(?x)
                    \[             # open square bracket
                      (?![~\\])    # not followed by escapes
                      ([%$!#]?)    # any of the valid prefix characters
                      ([^[\]{}]+)  # no brackets or braces
                    \]&#39;&#39;&#39;, _replace, string)
                if _replace.done:
                    break
            string = re.sub(r&#39;\[\\(.)\]&#39;, r&#39;\1&#39;, string)
            string = string.replace(&#39;[~]&#39;, &#39;\0&#39;)
            return string

        for table_name, table in tables.items():
            stream_name = F&#39;!{table_name}&#39;
            if stream_name not in streams:
                continue
            processed = []
            info = list(table.values())
            for r, row in enumerate(stream_to_rows(stream(stream_name), column_formats(table))):
                values = []
                for index, value in enumerate(row):
                    vt = info[index].type
                    if vt is MsiType.Long:
                        if value != 0:
                            value -= 0x80000000
                    elif vt is MsiType.Short:
                        if value != 0:
                            value -= 0x8000
                    elif value in strings:
                        value = strings.ref(value)
                    elif not info[index].is_integer:
                        value = &#39;&#39;
                    values.append(value)
                if table_name == &#39;Property&#39;:
                    tbl_properties[values[0]] = values[1]
                if table_name == &#39;File&#39;:
                    tbl_properties[values[0]] = values[2]
                if table_name == &#39;Component&#39;:
                    tbl_properties[values[0]] = F&#39;%{values[2]}%&#39;
                entry = dict(zip(table, values))
                einfo = {t: i for t, i in zip(table, info)}
                if table_name == &#39;MsiFileHash&#39;:
                    entry[&#39;Hash&#39;] = struct.pack(
                        &#39;&lt;IIII&#39;,
                        row[2] ^ 0x80000000,
                        row[3] ^ 0x80000000,
                        row[4] ^ 0x80000000,
                        row[5] ^ 0x80000000,
                    ).hex()
                if table_name == &#39;CustomAction&#39;:
                    code = row[1] &amp; 0x3F
                    try:
                        entry[&#39;Comment&#39;] = self._CUSTOM_ACTION_TYPES[code]
                    except LookupError:
                        pass
                    t = einfo.get(&#39;Target&#39;)
                    c = {0x25: &#39;js&#39;, 0x26: &#39;vbs&#39;, 0x33: None}
                    if code in c and t and not t.is_integer:
                        postprocessing.append(ScriptItem(r, c[code]))
                processed.append(entry)
            if processed:
                processed_table_data[table_name] = processed

        ca = processed_table_data.get(&#39;CustomAction&#39;, None)
        for item in postprocessing:
            entry = ca[item.row_index]
            try:
                path: str = entry[&#39;Action&#39;]
                data: str = entry[&#39;Target&#39;]
            except KeyError:
                continue
            root = F&#39;Action/{path}&#39;
            if item.extension:
                path = F&#39;{root}.{item.extension}&#39;
                streams[path] = UnpackResult(path, data.encode(self.codec))
                continue
            data = format_string(data)
            parts = [part.partition(&#39;\x02&#39;) for part in data.split(&#39;\x01&#39;)]
            if not all(part[1] == &#39;\x02&#39; for part in parts):
                continue
            for name, _, script in parts:
                if not name.lower().startswith(&#39;script&#39;):
                    continue
                if not script:
                    continue
                path = F&#39;{root}.{name}&#39;
                streams[path] = UnpackResult(path, script.encode(self.codec))

        for ignored_stream in [
            &#39;[5]SummaryInformation&#39;,
            &#39;[5]DocumentSummaryInformation&#39;,
            &#39;[5]DigitalSignature&#39;,
            &#39;[5]MsiDigitalSignatureEx&#39;
        ]:
            streams.pop(ignored_stream, None)

        inconsistencies = 0
        w1 = len(str(len(strings)))
        w2 = len(str(max(max(strings.computed_ref_count), max(strings.provided_ref_count))))
        for k in range(len(strings)):
            c = strings.computed_ref_count[k]
            p = strings.provided_ref_count[k]
            if c != p and not self.log_debug(F&#39;string {k:0{w1}d} reference count computed={c:0{w2}d} provided={p:0{w2}d}&#39;):
                inconsistencies += 1
        if inconsistencies:
            self.log_info(F&#39;found {inconsistencies} incorrect string reference counts&#39;)

        def fix_msi_path(path: str):
            prefix, dot, name = path.partition(&#39;.&#39;)
            if dot == &#39;.&#39; and prefix in processed_table_data:
                path = F&#39;{prefix}/{name}&#39;
            return path

        if self.args.nocab:
            cabs = {}
        else:
            def _iscab(path):
                return media_info and any(item.get(&#39;Cabinet&#39;, &#39;&#39;) == F&#39;#{path}&#39; for item in media_info)
            media_info: List[JSONDict] = processed_table_data.get(&#39;Media&#39;, [])
            cabs: Dict[str, UnpackResult] = {
                path: item for path, item in streams.items() if _iscab(path)}
            for cab in cabs:
                self.log_info(F&#39;found cab file: {cab}&#39;)
        if cabs:
            from refinery.units.formats.archive.xtcab import xtcab
            file_names: Dict[str, JSONDict] = {}

            for file_info in processed_table_data.get(&#39;File&#39;, []):
                try:
                    src_name = file_info[&#39;File&#39;]
                    dst_name = file_info[&#39;FileName&#39;]
                except KeyError:
                    continue
                _, _, long = dst_name.partition(&#39;|&#39;)
                dst_name = long or dst_name
                file_names[src_name] = dst_name

            for path, cab in cabs.items():
                try:
                    unpacked: List[UnpackResult] = list(xtcab().unpack(cab.get_data()))
                except Exception as e:
                    self.log_info(F&#39;unable to extract embedded cab file: {e!s}&#39;)
                    continue
                base, dot, ext = path.rpartition(&#39;.&#39;)
                if dot == &#39;.&#39; and ext.lower() == &#39;cab&#39;:
                    path = base
                else:
                    del streams[path]
                    cab.path = F&#39;{path}.cab&#39;
                    streams[cab.path] = cab
                for result in unpacked:
                    sub_path = file_names.get(result.path, result.path)
                    sub_path = self._get_path_separator().join((path, sub_path))
                    streams[sub_path] = result

        streams = {fix_msi_path(path): item for path, item in streams.items()}
        ds = UnpackResult(self._SYNTHETIC_STREAMS_FILENAME,
                json.dumps(processed_table_data, indent=4).encode(self.codec))
        streams[ds.path] = ds

        converter = csv()
        for key, data in processed_table_data.items():
            sk = key.strip(&#39;_&#39;)
            if sk not in processed_table_data:
                key = sk
            try:
                tbl = UnpackResult(F&#39;{self._SYNTHETIC_STREAMS_TOPLEVEL}/{key}.csv&#39;, converter.json_to_csv(data))
            except Exception:
                continue
            streams[tbl.path] = tbl

        for path in sorted(streams):
            streams[path].path = path
            yield streams[path]

    @classmethod
    def handles(self, data: bytearray):
        if not data.startswith(B&#39;\xD0\xCF\x11\xE0&#39;):
            return False
        return FileMagicInfo(data).extension == &#39;msi&#39;</code></pre>
</details>
</dd>
<dt id="refinery.shell.xtnode"><code class="flex name class">
<span>class <span class="ident">xtnode</span></span>
<span>(</span><span>*paths, entry=False, list=False, join_path=False, drop_path=False, fuzzy=0, exact=False, regex=False, path=b'path', date=b'date')</span>
</code></dt>
<dd>
<section class="desc"><p>Extracts and decompiles files from compiled Node.Js applications. Supports both nexe and pkg, two
utilities that are commonly used to generate stand-alone executables.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/formats/archive/xtnode.py#L56-L228" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class xtnode(ArchiveUnit):
    &#34;&#34;&#34;
    Extracts and decompiles files from compiled Node.Js applications. Supports both nexe and pkg, two
    utilities that are commonly used to generate stand-alone executables.
    &#34;&#34;&#34;

    _NEXE_SENTINEL = B&#39;&lt;nexe~~sentinel&gt;&#39;
    _PKG_PAYLOAD_P = B&#39;PAYLOAD_POSITION&#39;
    _PKG_PAYLOAD_S = B&#39;PAYLOAD_SIZE&#39;
    _PKG_PRELUDE_P = B&#39;PRELUDE_POSITION&#39;
    _PKG_PRELUDE_S = B&#39;PRELUDE_SIZE&#39;
    _PKG_COMMON_JS = B&#39;sourceMappingURL=common.js.map&#39;

    def __init__(
        self, *paths, entry: Arg.Switch(&#39;-u&#39;, help=&#39;Only extract the entry point.&#39;) = False,
        list=False, join_path=False, drop_path=False, fuzzy=0, exact=False, regex=False,
        path=b&#39;path&#39;, date=b&#39;date&#39;,
    ):
        super().__init__(*paths, entry=entry,
            list=list, join_path=join_path, drop_path=drop_path, fuzzy=fuzzy, exact=exact, regex=regex,
            path=path, date=date)

    def unpack(self, data: ByteStr) -&gt; Iterable[UnpackResult]:
        if self._is_nexe(data):
            self.log_info(&#39;unpacking as nexe&#39;)
            yield from self._unpack_nexe(data)
            return
        if self._is_pkg(data):
            self.log_info(&#39;unpacking as pkg&#39;)
            yield from self._unpack_pkg(data)
            return

    def _unpack_nexe(self, data: ByteStr):
        try:
            ep = re.compile(
                RB&#34;entry\s*=\s*path\.resolve\(path\.dirname\(process\.execPath\),\s*(%s)\)&#34; % formats.string)
            ep, = ep.finditer(data)
        except Exception:
            ep = None
            self.log_info(&#39;could not identify entry point&#39;)
        else:
            ep = ep.group(1) | esc(quoted=True) | str
            self.log_info(F&#39;entry point: {ep}&#39;)
        view = memoryview(data)
        for marker in re.finditer(re.escape(self._NEXE_SENTINEL), data):
            end = marker.end() + 16
            sizes = data[marker.end():end]
            if sizes.startswith(b&#34;&#39;)&#34;):
                continue
            reader = StructReader(sizes)
            code_size = int(reader.f64())
            blob_size = int(reader.f64())
            start = marker.start() - code_size - blob_size
            try:
                reader = StructReader(view[start:end])
                code = reader.read_exactly(code_size)
                blob = reader.read_exactly(blob_size)
            except EOFError:
                self.log_debug(F&#39;found marker at 0x{marker.start():X}, but failed to read data&#39;)
                continue
            else:
                self.log_debug(F&#39;found marker at 0x{marker.start():X}, data start at {start:X}&#39;)
            for rsrc in re.finditer(RB&#39;process\.__nexe\s*=&#39;, code):
                rsrc = JSONReader(code[rsrc.end():])
                rsrc = rsrc.read_json()
                if len(rsrc) == 1:
                    _, rsrc = rsrc.popitem()
                for path, (offset, length) in rsrc.items():
                    end = offset + length
                    if ep and self.args.entry and path != ep:
                        continue
                    yield UnpackResult(path, blob[offset:end])

    def _unpack_pkg(self, data: ByteStr):
        def _extract_coordinates(*v: bytes):
            for name in v:
                pattern = name + BR&#39;&#39;&#39;\s{0,3}=\s{0,3}([&#39;&#34;])([\s\d]+)\1&#39;&#39;&#39;
                value, = re.finditer(pattern, data)
                yield int(value.group(2).decode(&#39;utf8&#39;).strip(), 0)

        def _extract_data(*v: bytes):
            try:
                offset, length = _extract_coordinates(*v)
            except Exception:
                return None
            return data[offset:offset + length]

        payload = _extract_data(self._PKG_PAYLOAD_P, self._PKG_PAYLOAD_S)
        if not payload:
            raise ValueError(&#39;unable to extract payload&#39;)
        prelude = _extract_data(self._PKG_PRELUDE_P, self._PKG_PRELUDE_S)
        if not prelude:
            raise ValueError(&#39;unable to extract prelude&#39;)
        mapping = re.search(re.escape(self._PKG_COMMON_JS) + BR&#39;\s*\},\s*\{&#39;, prelude)
        if not mapping:
            raise ValueError(&#39;unable to find common.js mapping&#39;)

        reader = JSONReader(prelude[mapping.end() - 1:])

        files: Dict[str, dict] = reader.read_json()

        if files is None:
            raise ValueError(&#39;failed to read file list&#39;)

        entry = reader.skip_comma().read_string()
        links = reader.skip_comma().read_json()

        # _unknown1 = reader.skip_comma().read_json()
        # _unknown2 = reader.skip_comma().read_terminated_array(B&#39;)&#39;).strip()

        root = next(iter(files))
        skip = 0
        view = memoryview(payload)

        for k in range(len(root) + 1):
            test = root[:k].rstrip(&#39;/&#39;).rstrip(&#39;\\&#39;)
            if not all(path.startswith(test) for path in files):
                root = test[:-1]
                skip = k - 1
                break

        entry = entry[skip:]
        self.log_info(F&#39;detected root directory {root}, entry point is {entry}&#39;)

        for src, dst in links.items():
            new_files = {}
            self.log_info(&#39;link src:&#39;, src[skip:])
            self.log_info(&#39;link dst:&#39;, dst[skip:])
            for path, location in files.items():
                if not path.startswith(src):
                    continue
                new_path = dst + path[len(src):]
                new_files[new_path] = location
                self.log_debug(&#39;synthesizing linked file:&#39;, new_path)
            files.update(new_files)

        for path, location in files.items():
            path = path[skip:]
            if entry and self.args.entry and path != entry:
                continue
            data = None
            for kind, (offset, length) in location.items():
                stop = offset + length
                if kind == &#39;3&#39;:  # metadata
                    continue
                if kind == &#39;2&#39;:  # unknown
                    continue
                if kind in &#39;01&#39;:
                    data = view[offset:stop]
            if data is not None:
                yield UnpackResult(path, data)

    @classmethod
    def _is_nexe(cls, data: ByteStr) -&gt; bool:
        return cls._NEXE_SENTINEL in data

    @classmethod
    def _is_pkg(cls, data: ByteStr) -&gt; bool:
        if cls._PKG_PAYLOAD_P not in data:
            return False
        if cls._PKG_PAYLOAD_S not in data:
            return False
        if cls._PKG_PRELUDE_P not in data:
            return False
        if cls._PKG_PRELUDE_S not in data:
            return False
        if cls._PKG_COMMON_JS not in data:
            return False
        return True

    @classmethod
    def handles(cls, data: ByteStr) -&gt; Optional[bool]:
        return cls._is_nexe(data) or cls._is_pkg(data)</code></pre>
</details>
</dd>
<dt id="refinery.shell.xtnsis"><code class="flex name class">
<span>class <span class="ident">xtnsis</span></span>
<span>(</span><span>*paths, list=False, join_path=False, drop_path=False, fuzzy=0, exact=False, regex=False, path=b'path', date=b'date', pwd=b'')</span>
</code></dt>
<dd>
<section class="desc"><p>Extract files from NSIS archives.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/formats/archive/xtnsis.py#L1291-L1372" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class xtnsis(ArchiveUnit):
    &#34;&#34;&#34;
    Extract files from NSIS archives.
    &#34;&#34;&#34;

    @classmethod
    def _find_archive_offset(cls, data: bytearray, before: int = -1, flawmax=2):
        def signatures(*magics):
            for changes in range(flawmax + 1):
                for magic in magics:
                    if not changes:
                        yield 0, magic
                        continue
                    for positions in itertools.permutations(range(len(magic)), r=changes):
                        signature = bytearray(magic)
                        for p in positions:
                            signature[p] = 0x2E
                        yield changes, bytes(signature)
        best_guess = None
        search_space = memoryview(data)
        for flaws, sig in signatures(*NSArchive.MAGICS):
            if flaws &gt; 1:
                search_space = search_space[:0x20_000]
            matches = [m.start() - 4 for m in re.finditer(sig, search_space, flags=re.DOTALL)]
            if before &gt;= 0:
                matches = [match for match in matches if match &lt; before]
            matches.reverse()
            archive = None
            for match in matches:
                if match % 0x200 == 0:
                    archive = match
                    break
            if not archive:
                if matches and not best_guess:
                    best_guess = matches[-1]
            else:
                msg = F&#39;Archive signature was found at offset 0x{archive:X}&#39;
                if flaws &gt; 0:
                    msg = F&#39;{msg}; it has {flaws} imperfections and was likely modified&#39;
                cls.log_info(F&#39;{msg}.&#39;)
                return archive
        if best_guess:
            cls.log_info(F&#39;A signature was found at offset 0x{best_guess:08X}; it is not properly aligned.&#39;)
            return best_guess
        return None

    def unpack(self, data):
        memory = memoryview(data)
        before = -1
        _error = None
        while True:
            offset = self._find_archive_offset(data, before)
            if offset is None:
                _error = _error or ValueError(&#39;Unable to find an NSIS archive marker.&#39;)
                raise _error
            try:
                arc = NSArchive(memory[offset:])
            except Exception as e:
                _error = e
                before = offset
            else:
                break

        def info():
            yield F&#39;{arc.header.type.name} archive&#39;
            yield F&#39;compression type {arc.method.value}&#39;
            yield F&#39;mystery value 0x{arc.header.unknown_value:X}&#39;
            yield &#39;solid archive&#39; if arc.solid else &#39;fragmented archive&#39;
            yield &#39;64-bit header&#39; if arc.header.is64bit else &#39;32-bit header&#39;
            yield &#39;unicode&#39; if arc.header.unicode else &#39;ascii&#39;

        self.log_info(&#39;, &#39;.join(info()))

        for item in arc.header.items:
            yield self._pack(item.path, item.mtime, lambda i=item: arc._extract_item(i).data)

        yield self._pack(&#39;setup.bin&#39;, None, arc.header_data)
        yield self._pack(&#39;setup.nsis&#39;, None, arc.script.encode(self.codec))

    @classmethod
    def handles(cls, data: bytearray) -&gt; bool:
        return any(magic in data for magic in NSArchive.MAGICS)</code></pre>
</details>
</dd>
<dt id="refinery.shell.xtnuitka"><code class="flex name class">
<span>class <span class="ident">xtnuitka</span></span>
<span>(</span><span>*paths, list=False, join_path=False, drop_path=False, fuzzy=0, exact=False, regex=False, path=b'path')</span>
</code></dt>
<dd>
<section class="desc"><p>Extracts files packed by Nuitka using the &ndash;onefile option.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/formats/archive/xtnuitka.py#L11-L84" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class xtnuitka(PathExtractorUnit):
    &#34;&#34;&#34;
    Extracts files packed by Nuitka using the --onefile option.
    &#34;&#34;&#34;
    _MAGIC = B&#39;KA&#39;

    @PathExtractorUnit.Requires(&#39;pyzstd&#39;, &#39;arc&#39;)
    def _pyzstd():
        import pyzstd
        return pyzstd

    def unpack(self, data: ByteStr) -&gt; Iterable[UnpackResult]:
        class NuitkaData(Struct):
            unit = self

            def __init__(self, reader: StructReader):
                self.magic = reader.read_exactly(2)
                self.compression_flag = reader.read_exactly(1)
                if self.compressed:
                    zd = self.unit._pyzstd.ZstdDecompressor()
                    reader = StructReader(zd.decompress(reader.read()))
                self.files = {}
                self.truncated = False
                while not reader.eof:
                    path = reader.read_w_string(&#39;utf-16&#39;)
                    if not path:
                        break
                    size = reader.u64()
                    data = reader.read(size)
                    if len(data) == size:
                        self.files[path] = data
                    else:
                        self.truncated = True

            @property
            def compressed(self):
                return self.compression_flag == b&#39;Y&#39;

        if data.startswith(b&#39;MZ&#39;):
            arcs = list(self._pe_candidates(data))
        else:
            arcs = [data]

        for arc in arcs:
            archive = NuitkaData(arc)
            if archive.truncated:
                self.log_warn(&#39;the archive is truncated&#39;)
            if archive.magic != self._MAGIC:
                self.log_warn(&#39;the archive data does not start with the correct magic sequence&#39;)
            for path, data in archive.files.items():
                yield UnpackResult(path, data)

    @classmethod
    def handles(cls, data: ByteStr) -&gt; Optional[bool]:
        if data.startswith(b&#39;MZ&#39;):
            try:
                next(cls._pe_candidates(data))
            except StopIteration:
                return False
        else:
            return data.startswith(cls._MAGIC)

    @classmethod
    def _pe_candidates(cls, data: ByteStr):

        from refinery.units.formats.pe.peoverlay import peoverlay
        blob = data | peoverlay | bytearray
        if blob.startswith(cls._MAGIC):
            yield blob

        from refinery.units.formats.pe.perc import perc
        for blob in data | perc:
            if blob.startswith(cls._MAGIC):
                yield blob</code></pre>
</details>
</dd>
<dt id="refinery.shell.xtone"><code class="flex name class">
<span>class <span class="ident">xtone</span></span>
<span>(</span><span>*paths, list=False, join_path=False, drop_path=False, fuzzy=0, exact=False, regex=False, path=b'path')</span>
</code></dt>
<dd>
<section class="desc"><p>Extract embedded files from OneNote documents.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/formats/office/xtone.py#L11-L34" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class xtone(PathExtractorUnit):
    &#34;&#34;&#34;
    Extract embedded files from OneNote documents.
    &#34;&#34;&#34;
    @PathExtractorUnit.Requires(&#39;pyonenote&#39;, &#39;formats&#39;, &#39;office&#39;, &#39;extended&#39;)
    def _pyOneNote():
        import pyOneNote
        import pyOneNote.OneDocument
        return pyOneNote.OneDocument

    def unpack(self, data: bytearray):
        with MemoryFile(memoryview(data)) as stream:
            one = self._pyOneNote.OneDocment(stream)
        for guid, file in one.get_files().items():
            chunk = file[&#39;content&#39;]
            try:
                extension = file[&#39;extension&#39;]
            except KeyError:
                extension = F&#39;.{get_cached_file_magic_info(chunk).extension}&#39;
            yield UnpackResult(F&#39;{guid}{extension}&#39;, chunk)

    @classmethod
    def handles(cls, data: bytearray) -&gt; Optional[bool]:
        return UUID(&#39;e4525c7b-8cd8-a74d-aeb1-5378d02996d3&#39;).bytes in data</code></pre>
</details>
</dd>
<dt id="refinery.shell.xtp"><code class="flex name class">
<span>class <span class="ident">xtp</span></span>
<span>(</span><span>*pattern, filter=0, min=1, max=None, len=None, stripspace=False, duplicates=False, longest=False, take=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Extract Patterns: Uses regular expressions to extract indicators from the input data and
optionally filters these results heuristically. The unit is designed to extract indicators
such as domain names and IP addresses, see below for a complete list. To extract data
formats such as hex-encoded data, use <code><a title="refinery.carve" href="index.html#refinery.carve">carve</a></code>.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/pattern/xtp.py#L56-L400" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class xtp(PatternExtractor):
    &#34;&#34;&#34;
    Extract Patterns: Uses regular expressions to extract indicators from the input data and
    optionally filters these results heuristically. The unit is designed to extract indicators
    such as domain names and IP addresses, see below for a complete list. To extract data
    formats such as hex-encoded data, use `refinery.carve`.
    &#34;&#34;&#34;

    def __init__(
        self,
        *pattern: Arg(&#39;pattern&#39;, type=str,
            default=(
                indicators.hostname.name,
                indicators.url.name,
                indicators.email.name,
            ), help=(
                &#39;Choose the pattern to extract. The unit uses {{default}} by default. Use an &#39;
                &#39;asterix character to select all available patterns. The available patterns &#39;
                &#39;are: {}&#39;.format(&#39;, &#39;.join(p.display for p in indicators))
            )
        ),
        filter: Arg(&#39;-f&#39;, dest=&#39;filter&#39;, action=&#39;count&#39;,
            help=(
                &#39;If this setting is enabled, the xtp unit will attempt to reduce the number &#39;
                &#39;of false positives by certain crude heuristics. Specify multiple times to &#39;
                &#39;make the filtering more aggressive.&#39;
            )
        ) = 0,
        min=1, max=None, len=None, stripspace=False, duplicates=False, longest=False, take=None
    ):
        self.superinit(super(), **vars(), ascii=True, utf16=True)

        patterns = {
            p for name in pattern for p in indicators if fnmatch(p.display, name)
        }
        # if indicators.hostname in patterns:
        #     patterns.remove(indicators.hostname)
        #     patterns.add(indicators.ipv4)
        #     patterns.add(indicators.domain)
        patterns = [F&#39;(?P&lt;{p.name}&gt;{p.value})&#39; for p in patterns]
        if not patterns:
            raise RefineryCriticalException(&#39;The given mask does not match any known indicator pattern.&#39;)
        pattern = &#39;|&#39;.join(patterns)
        self.args.pattern = re.compile(pattern.encode(self.codec), flags=re.DOTALL)
        self.args.filter = filter

    _ALPHABETIC = ascii_letters.encode(&#39;ASCII&#39;)

    _LEGITIMATE_HOSTS = {
        &#39;acm.org&#39;                 : 1,
        &#39;adobe.com&#39;               : 1,
        &#39;aka.ms&#39;                  : 1,
        &#39;android.com&#39;             : 1,
        &#39;apache.org&#39;              : 1,
        &#39;apple.com&#39;               : 1,
        &#39;archive.org&#39;             : 2,
        &#39;azure.com&#39;               : 1,
        &#39;baidu.com&#39;               : 2,
        &#39;bootstrapcdn.com&#39;        : 2,
        &#39;cdnjs.cloudflare.com&#39;    : 4,
        &#39;comodo.net&#39;              : 1,
        &#39;comodoca.com&#39;            : 1,
        &#39;curl.haxx.se&#39;            : 1,
        &#39;curl.se&#39;                 : 1,
        &#39;digicert.com&#39;            : 1,
        &#39;dublincore.org&#39;          : 1,
        &#39;facebook.com&#39;            : 4,
        &#39;fontawesome.com&#39;         : 1,
        &#39;github.com&#39;              : 3,
        &#39;globalsign.com&#39;          : 1,
        &#39;globalsign.net&#39;          : 1,
        &#39;godaddy.com&#39;             : 1,
        &#39;google.com&#39;              : 4,
        &#39;googleapis.com&#39;          : 5,
        &#39;googleusercontent.com&#39;   : 5,
        &#39;gov&#39;                     : 2,
        &#39;gstatic.com&#39;             : 2,
        &#39;iana.org&#39;                : 1,
        &#39;intel.com&#39;               : 1,
        &#39;jquery.com&#39;              : 1,
        &#39;jsdelivr.net&#39;            : 2,
        &#39;live.com&#39;                : 1,
        &#39;microsoft.com&#39;           : 1,
        &#39;msdn.com&#39;                : 1,
        &#39;msn.com&#39;                 : 1,
        &#39;newtonsoft.com&#39;          : 3, # json.net
        &#39;nuget.org&#39;               : 3,
        &#39;office.com&#39;              : 1,
        &#39;office365.com&#39;           : 2,
        &#39;openssl.org&#39;             : 1,
        &#39;openxmlformats.org&#39;      : 1,
        &#39;oracle.com&#39;              : 1,
        &#39;purl.org&#39;                : 1,
        &#39;python.org&#39;              : 1,
        &#39;schema.org&#39;              : 2,
        &#39;sectigo.com&#39;             : 1,
        &#39;skype.com&#39;               : 1,
        &#39;sourceforge.net&#39;         : 4,
        &#39;stackoverflow.com&#39;       : 1,
        &#39;sun.com&#39;                 : 1,
        &#39;sway-cdn.com&#39;            : 1,
        &#39;sway-extensions.com&#39;     : 1,
        &#39;symantec.com&#39;            : 1,
        &#39;symauth.com&#39;             : 1,
        &#39;symcb.com&#39;               : 1,
        &#39;symcd.com&#39;               : 1,
        &#39;sysinternals.com&#39;        : 3,
        &#39;thawte.com&#39;              : 1,
        &#39;unicode.org&#39;             : 2,
        &#39;usertrust.com&#39;           : 1,
        &#39;verisign.com&#39;            : 1,
        &#39;w3.org&#39;                  : 1,
        &#39;wikipedia.org&#39;           : 1,
        &#39;wolfram.com&#39;             : 1,
        &#39;xml.org&#39;                 : 1,
        &#39;xmlsoap.org&#39;             : 1,
        &#39;yahoo.com&#39;               : 1,
    }

    for _ext in [
        &#39;build&#39;,
        &#39;data&#39;,
        &#39;do&#39;,
        &#39;help&#39;,
        &#39;java&#39;,
        &#39;md&#39;,
        &#39;mov&#39;,
        &#39;name&#39;,
        &#39;py&#39;,
        &#39;so&#39;,
        &#39;sys&#39;,
        &#39;zip&#39;,
    ]:
        _LEGITIMATE_HOSTS[_ext] = 4

    _DOMAIN_WHITELIST = [
        &#39;system.net&#39;,
        &#39;wscript.shell&#39;,
    ]

    _BRACKETING = {
        B&#34;&#39;&#34;[0]: B&#34;&#39;&#34;,
        B&#39;&#34;&#39;[0]: B&#39;&#34;&#39;,
        B&#39;(&#39;[0]: B&#39;)&#39;,
        B&#39;{&#39;[0]: B&#39;}&#39;,
        B&#39;[&#39;[0]: B&#39;]&#39;,
        B&#39;&lt;&#39;[0]: B&#39;&gt;&#39;,
    }

    def _check_match(self, data: Union[memoryview, bytearray], pos: int, name: str, value: bytes):
        term = self._BRACKETING.get(data[pos - 1], None)
        if term:
            pos = value.find(term)
            if pos &gt; 0:
                value = value[:pos]
        if not self.args.filter:
            return value
        if name == indicators.hostname.name:
            if all(part.isdigit() for part in value.split(B&#39;.&#39;)):
                name = indicators.ipv4.name
            elif B&#39;.&#39; not in value:
                name = indicators.ipv6.name
            else:
                name = indicators.domain.name
        if name == indicators.ipv4.name:
            ocets = [int(x) for x in value.split(B&#39;.&#39;)]
            if ocets.count(0) &gt;= 3:
                return None
            if self.args.filter &gt; 2 and sum(ocets) &lt; 10:
                return None
            for area in (
                bytes(data[pos - 20 : pos + 20]),
                bytes(data[pos * 2 - 40 : pos * 2 + 40 : 2]),
                bytes(data[pos * 2 - 41 : pos * 2 + 39 : 2]),
            ):
                if B&#39;version&#39; in area.lower():
                    return None
            ip = ip_address(value.decode(self.codec))
            if not ip.is_global:
                if self.args.filter &gt;= 3 or not ip.is_private:
                    return None
        elif name in {
            indicators.url.name,
            indicators.socket.name,
            indicators.hostname.name,
            indicators.domain.name,
            indicators.subdomain.name
        }:
            if self.args.filter &gt;= 2:
                if LetterWeights.IOC(value) &lt; 0.6:
                    self.log_info(F&#39;excluding indicator because with low score: {value}&#39;, clip=True)
                    return None
                if name != indicators.url.name and len(value) &gt; 0x100:
                    self.log_info(F&#39;excluding indicator because it is too long: {value}&#39;, clip=True)
                    return None
            ioc = value.decode(self.codec)
            if &#39;://&#39; not in ioc: ioc = F&#39;tcp://{ioc}&#39;
            parts = urlparse(ioc)
            host, _, _ = parts.netloc.partition(&#39;:&#39;)
            hl = host.lower()
            for white, level in self._LEGITIMATE_HOSTS.items():
                if self.args.filter &gt;= level and (hl == white or hl.endswith(F&#39;.{white}&#39;)):
                    self.log_info(F&#39;excluding indicator because domain {hl} is whitelisted via {white}: {value}&#39;, clip=True)
                    self.log_debug(F&#39;reduce level below {level} to allow, current level is {self.args.filter}&#39;)
                    return None
            if name == indicators.url.name:
                scheme = parts.scheme.lower()
                for p in (&#39;http&#39;, &#39;https&#39;, &#39;ftp&#39;, &#39;file&#39;, &#39;mailto&#39;):
                    if scheme.endswith(p):
                        pos = scheme.find(p)
                        value = value[pos:]
                        break
            if any(hl == w for w in self._DOMAIN_WHITELIST):
                self.log_info(F&#39;excluding indicator because domain {hl} is whitelisted: {value}&#39;)
                return None
            if name in {
                indicators.hostname.name,
                indicators.domain.name,
                indicators.subdomain.name
            }:
                if data[pos - 1] in b&#39;/\\&#39; and self.args.filter &gt;= 2:
                    return None
                hostparts = host.split(&#39;.&#39;)
                if self.args.filter &gt;= 2:
                    if not all(p.isdigit() for p in hostparts) and all(len(p) &lt; 4 for p in hostparts):
                        self.log_info(F&#39;excluding host with too many short parts: {value}&#39;)
                        return None
                if self.args.filter &gt;= 3:
                    if len(hostparts) &lt;= sum(3 for p in hostparts if p != p.lower() and p != p.upper()):
                        self.log_info(F&#39;excluding host with too many mixed case parts: {value}&#39;)
                        return None
                # These heuristics attempt to filter out member access to variables in
                # scripts which can be mistaken for domains because of the TLD inflation
                # we&#39;ve had.
                uppercase = sum(1 for c in host if c.isalpha() and c.upper() == c)
                lowercase = sum(1 for c in host if c.isalpha() and c.lower() == c)
                if lowercase and uppercase:
                    caseratio = uppercase / lowercase
                    if 0.1 &lt; caseratio &lt; 0.9:
                        self.log_info(F&#39;excluding indicator with too much uppercase letters: {value}&#39;)
                        return None
                if all(x.isidentifier() for x in hostparts):
                    if len(hostparts) == 2 and hostparts[0] in (&#39;this&#39;, &#39;self&#39;):
                        self.log_info(F&#39;excluding host that looks like a code snippet: {value}&#39;)
                        return None
                    if len(hostparts[-2]) &lt; 3:
                        self.log_info(F&#39;excluding host with too short root domain name: {value}&#39;)
                        return None
                    if any(x.startswith(&#39;_&#39;) for x in hostparts):
                        self.log_info(F&#39;excluding host with underscores: {value}&#39;)
                        return None
                    if len(hostparts[-1]) &gt; 3:
                        prefix = &#39;.&#39;.join(hostparts[:-1])
                        seen_before = len(set(re.findall(
                            R&#39;{}(?:\.\w+)+&#39;.format(prefix).encode(&#39;ascii&#39;), data)))
                        if seen_before &gt; 2:
                            self.log_debug(F&#39;excluding indicator that was already seen: {value}&#39;)
                            return None
        elif name == indicators.email.name:
            at = value.find(B&#39;@&#39;)
            ix = 0
            while value[ix] not in self._ALPHABETIC:
                ix += 1
            return None if at - ix &lt; 3 else value[ix:]
        elif name in (
            indicators.path.name,
            indicators.winpath.name,
            indicators.nixpath.name,
        ):
            if len(value) &lt; 8:
                self.log_info(F&#39;excluding path because it is too short: {value}&#39;)
                return None
            if len(value) &gt; 16 and len(re.findall(RB&#39;\\x\d\d&#39;, value)) &gt; len(value) // 10:
                self.log_info(F&#39;excluding long path containign hex: {value}&#39;, clip=True)
                return None
            try:
                path_string = value.decode(self.codec)
            except Exception:
                self.log_debug(F&#39;excluding path which did not decode: {value!r}&#39;, clip=True)
                return None
            try:
                path = Path(path_string)
            except Exception as E:
                self.log_debug(F&#39;error parsing path &#34;{path}&#34;: {E!s}&#39;)
                return None
            path_likeness = sum(v for v, x in [
                (1, path.suffix),
                (1, path_string.startswith(&#39;/&#39;)),
                (2, path_string.startswith(&#39;%&#39;)),
                (2, path_string.startswith(&#39;\\\\&#39;)),
                (2, path_string[1:3] == &#39;:\\&#39;),
            ] if x)
            if 2 + path_likeness &lt; min(self.args.filter, 2):
                self.log_info(F&#39;excluding long path because it has no characteristic parts: {value}&#39;)
                return None
            bad_parts = 0
            all_parts = len(path.parts)
            if self.args.filter &gt;= 1:
                date_likeness = sum(1
                    for t in [&#39;yyyy&#39;, &#39;yy&#39;, &#39;mm&#39;, &#39;dd&#39;, &#39;hh&#39;, &#39;ss&#39;]
                    if t in path.parts or t.upper() in path.parts)
                if len(value) &lt; 20 and date_likeness &gt;= all_parts - 1:
                    self.log_info(F&#39;excluding path that looks like a date format: {value}&#39;, clip=True)
                    return None
            if self.args.filter &gt;= 2:
                for k, part in enumerate(path.parts):
                    if not k:
                        drive, colon, slash = part.partition(&#39;:&#39;)
                        if colon and len(drive) == 1 and len(slash) &lt;= 1:
                            continue
                        if part[0] == part[~0] == &#39;%&#39;:
                            continue
                        if len(part) == 1:
                            continue
                    if (
                        LetterWeights.Path(part) &lt; 0.5 + (min(self.args.filter, 4) * 0.1)
                        or (self.args.filter &gt;= 2 and LetterWeights.Path(part[:1]) &lt; 0.5)
                    ):
                        bad_parts += 1
                        self.log_debug(F&#39;bad part {k + 1} in path: {part}&#39;)
            for filter_limit in (2, 3, 4):
                bad_ratio = 2 ** (filter_limit - 1)
                if self.args.filter &gt;= filter_limit and bad_parts * bad_ratio &gt;= all_parts:
                    self.log_info(F&#39;excluding path with bad parts: {value}&#39;, clip=True)
                    return None
        return value

    def process(self, data):
        whitelist = set()

        def check(match: re.Match):
            for name, value in match.groupdict().items():
                if value is not None:
                    break
            else:
                raise RefineryCriticalException(&#39;Received empty match.&#39;)
            if value in whitelist:
                return None
            result = self._check_match(match.string, match.start(), name, value)
            if result is not None:
                return self.labelled(result, pattern=name)
            whitelist.add(value)

        transforms = [check]
        yield from self.matches_filtered(memoryview(data), self.args.pattern, *transforms)</code></pre>
</details>
</dd>
<dt id="refinery.shell.xtpdf"><code class="flex name class">
<span>class <span class="ident">xtpdf</span></span>
<span>(</span><span>*paths, list=False, join_path=False, drop_path=False, fuzzy=0, exact=False, regex=False, path=b'path')</span>
</code></dt>
<dd>
<section class="desc"><p>Extract objects from PDF documents.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/formats/pdf.py#L20-L117" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class xtpdf(PathExtractorUnit):
    &#34;&#34;&#34;
    Extract objects from PDF documents.
    &#34;&#34;&#34;
    @PathExtractorUnit.Requires(&#39;pypdf&gt;=3.1.0&#39;, &#39;formats&#39;, &#39;default&#39;, &#39;extended&#39;)
    def _pypdf2():
        import pypdf
        import pypdf.generic
        return pypdf

    def _walk(self, blob, memo: Optional[Set[int]] = None, *path):
        while isinstance(blob, self._pypdf2.generic.IndirectObject):
            try:
                blob = blob.get_object()
            except Exception:
                break
        if memo is None:
            memo = {id(blob)}
        elif id(blob) in memo:
            return
        else:
            memo.add(id(blob))
        try:
            name = blob[&#39;/F&#39;]
            blob = blob[&#39;/EF&#39;][&#39;/F&#39;]
        except Exception:
            pass
        else:
            path = *path[:-1], F&#39;/{name}&#39;
        try:
            def extract():
                with NoLogging():
                    return get_data()
            if TYPE_CHECKING:
                blob = cast(EncodedStreamObject, blob)
            get_data = blob.get_data
        except AttributeError:
            pass
        else:
            yield UnpackResult(&#39;&#39;.join(path), extract, kind=&#39;object&#39;)
            return

        if isinstance(blob, self._pypdf2.generic.ByteStringObject):
            yield UnpackResult(&#39;&#39;.join(path), blob, kind=&#39;bytes&#39;)
            return
        if isinstance(blob, self._pypdf2.generic.TextStringObject):
            yield UnpackResult(&#39;&#39;.join(path), blob.encode(self.codec), kind=&#39;string&#39;)
            return

        if isinstance(blob, (
            self._pypdf2.generic.BooleanObject,
            self._pypdf2.generic.ByteStringObject,
            self._pypdf2.generic.FloatObject,
            self._pypdf2.generic.NameObject,
            self._pypdf2.generic.NullObject,
            self._pypdf2.generic.NumberObject,
            self._pypdf2.generic.RectangleObject,
        )):
            # unhandled PDF objects
            return

        if isinstance(blob, self._pypdf2.generic.TreeObject):
            blob = list(blob)

        pdf = self._pypdf2.generic.PdfObject

        if isinstance(blob, list):
            if (
                len(blob) % 2 == 0
                and all(isinstance(key, str) for key in islice(iter(blob), 0, None, 2))
                and all(isinstance(key, pdf) for key in islice(iter(blob), 1, None, 2))
            ):
                blob = dict(zip(*([iter(blob)] * 2)))
            else:
                for key, value in enumerate(blob):
                    yield from self._walk(value, memo, *path, F&#39;/{key}&#39;)
                return

        if not isdict(blob):
            return

        for key, value in blob.items():
            if not isinstance(key, str):
                continue
            if not key.startswith(&#39;/&#39;):
                key = F&#39;/{key}&#39;
            yield from self._walk(value, memo, *path, key)

    def unpack(self, data):
        with MemoryFile(data, read_as_bytes=True) as stream:
            with NoLogging():
                pdf = self._pypdf2.PdfReader(stream)
                catalog = pdf.trailer[&#39;/Root&#39;]
                yield from self._walk(catalog)

    @classmethod
    def handles(self, data: bytearray) -&gt; Optional[bool]:
        return data.startswith(B&#39;%PDF-&#39;)</code></pre>
</details>
</dd>
<dt id="refinery.shell.xtpyi"><code class="flex name class">
<span>class <span class="ident">xtpyi</span></span>
<span>(</span><span>*paths, list=False, join_path=False, drop_path=False, fuzzy=0, exact=False, regex=False, path=b'path', date=b'date', decompile, user_code=False, unmarshal=0)</span>
</code></dt>
<dd>
<section class="desc"><p>Extracts and decompiles files from a Python Installer (aka PyInstaller) archive.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/formats/archive/xtpyi.py#L517-L613" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class xtpyi(ArchiveUnit):
    &#34;&#34;&#34;
    Extracts and decompiles files from a Python Installer (aka PyInstaller) archive.
    &#34;&#34;&#34;
    def __init__(
        self, *paths, list=False, join_path=False, drop_path=False, fuzzy=0, exact=False, regex=False,
        path=b&#39;path&#39;, date=b&#39;date&#39;,
        decompile: Arg.Switch(&#39;-c&#39;, help=&#39;Attempt to decompile PYC files.&#39;),
        user_code: Arg.Switch(&#39;-u&#39;, group=&#39;FILTER&#39;, help=(
            &#39;Extract only source code files from the root of the archive. These usually implement &#39;
            &#39;the actual domain logic. This implies the --decompile option.&#39;)) = False,
        unmarshal: Arg(&#39;-y&#39;, action=&#39;count&#39;, group=&#39;FILTER&#39;, help=(
            &#39;(DANGEROUS) Unmarshal embedded PYZ archives. Warning: Maliciously crafted packages can &#39;
            &#39;potentially exploit this to execute code. It is advised to only use this option inside &#39;
            &#39;an isolated environment. Specify twice to decompile unmarshalled Python bytecode.&#39;
        )) = 0
    ):
        super().__init__(
            *paths,
            list=list,
            join_path=join_path,
            drop_path=drop_path,
            fuzzy=fuzzy,
            exact=exact,
            regex=regex,
            path=path,
            date=date,
            decompile=decompile,
            unmarshal=unmarshal,
            user_code=user_code,
        )

    @ArchiveUnit.Requires(&#39;xdis&#39;, &#39;arc&#39;, &#39;python&#39;, &#39;extended&#39;)
    def _xdis():
        import xdis.load
        import xdis.magics
        import xdis.marsh
        import xdis.op_imports
        import xdis.version_info
        import xdis
        A, B, C, *_ = sys.version_info
        version = F&#39;{A}.{B}.{C}&#39;
        canonic = F&#39;{A}.{B}&#39;
        if version not in xdis.magics.canonic_python_version:
            import importlib.util
            magic = importlib.util.MAGIC_NUMBER
            xdis.magics.add_magic_from_int(xdis.magics.magic2int(magic), version)
            xdis.magics.by_magic.setdefault(magic, set()).add(version)
            xdis.magics.by_version[version] = magic
            xdis.magics.magics[canonic] = magic
            xdis.magics.canonic_python_version[canonic] = canonic
            xdis.magics.add_canonic_versions(version, canonic)
            xdis.op_imports.op_imports.setdefault(canonic,
                next(iter(reversed(xdis.op_imports.op_imports.values()))))
        del A, B, C, version
        import xdis.std
        return xdis

    @ArchiveUnit.Requires(&#39;uncompyle6&#39;, &#39;arc&#39;, &#39;python&#39;, &#39;extended&#39;)
    def _uncompyle6():
        import uncompyle6
        import uncompyle6.main
        return uncompyle6

    @ArchiveUnit.Requires(&#39;decompyle3&#39;, &#39;arc&#39;, &#39;python&#39;)
    def _decompyle3():
        import decompyle3
        import decompyle3.main
        return decompyle3

    def unpack(self, data):
        view = memoryview(data)
        positions = [m.start() for m in re.finditer(re.escape(PyInstallerArchiveEpilogue.MagicSignature), view)]
        mode = Unmarshal(min(2, int(self.args.unmarshal)))
        self.log_debug(F&#39;unmarshal mode: {mode.name}&#39;)
        if not positions:
            raise LookupError(&#39;unable to find PyInstaller signature&#39;)
        if len(positions) &gt; 2:
            # first position is expected to be the sentinel value in the unpacker stub
            width = max(len(F&#39;{p:X}&#39;) for p in positions)
            for position in positions:
                self.log_info(F&#39;magic signature found at offset 0x{position:0{width}X}&#39;)
            self.log_warn(F&#39;found {len(positions) - 1} potential PyInstaller epilogue markers; using last one.&#39;)
        decompile = self.args.decompile
        uc_target = PiType.USERCODE if decompile else PiType.SOURCE
        archive = PyInstallerArchiveEpilogue(view, positions[-1], mode, decompile)
        for name, file in archive.files.items():
            if self.args.user_code:
                if file.type != uc_target:
                    continue
                if name.startswith(&#39;pyiboot&#39;):
                    continue
            yield self._pack(name, None, file.data, type=file.type.name)

    @classmethod
    def handles(cls, data: ByteStr) -&gt; Optional[bool]:
        return PyInstallerArchiveEpilogue.MagicSignature in data</code></pre>
</details>
</dd>
<dt id="refinery.shell.xtrtf"><code class="flex name class">
<span>class <span class="ident">xtrtf</span></span>
<span>(</span><span>*paths, list=False, join_path=False, drop_path=False, fuzzy=0, exact=False, regex=False, path=b'path')</span>
</code></dt>
<dd>
<section class="desc"><p>Extract embedded objects in RTF documents.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/formats/office/xtrtf.py#L10-L56" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class xtrtf(PathExtractorUnit):
    &#34;&#34;&#34;
    Extract embedded objects in RTF documents.
    &#34;&#34;&#34;
    @PathExtractorUnit.Requires(&#39;oletools&#39;, &#39;formats&#39;, &#39;office&#39;, &#39;extended&#39;)
    def _oletools():
        import oletools
        import oletools.rtfobj
        import oletools.oleobj
        return oletools

    def unpack(self, data):
        parser = self._oletools.rtfobj.RtfObjParser(data)
        parser.parse()
        width = len(str(len(parser.objects)))
        for k, item in enumerate(parser.objects):
            item: RtfObject
            path = item.filename or F&#39;carve{k:0{width}}.bin&#39;
            data = item.rawdata
            meta = {}
            if item.is_ole:
                if item.format_id == self._oletools.oleobj.OleObject.TYPE_EMBEDDED:
                    meta[&#39;ole_type&#39;] = &#39;EMBEDDED&#39;
                elif item.format_id == self._oletools.oleobj.OleObject.TYPE_LINKED:
                    meta[&#39;ole_type&#39;] = &#39;LINKED&#39;
                if item.is_package:
                    meta[&#39;src_path&#39;] = item.src_path
                    meta[&#39;tmp_path&#39;] = item.temp_path
                if item.clsid is not None:
                    meta[&#39;ole_info&#39;] = item.clsid_desc
                    meta[&#39;ole_guid&#39;] = item.clsid
                meta[&#39;ole_name&#39;] = item.class_name
            if item.oledata:
                data = item.oledata
                pos = item.rawdata.find(data)
                if pos &gt; 0:
                    meta[&#39;raw_header&#39;] = item.rawdata[:pos]
                if item.olepkgdata:
                    data = item.olepkgdata
                    pos = item.oledata.find(data)
                    if pos &gt;= 0:
                        meta[&#39;ole_header&#39;] = item.oledata[:pos]
            yield UnpackResult(path, data, **meta)

    @classmethod
    def handles(self, data: bytearray) -&gt; bool:
        return data[:500].lower().lstrip().startswith(b&#39;{\\rtf&#39;)</code></pre>
</details>
</dd>
<dt id="refinery.shell.xtsql"><code class="flex name class">
<span>class <span class="ident">xtsql</span></span>
<span>(</span><span>*paths, list=False, join_path=False, drop_path=False, fuzzy=0, exact=False, regex=False, path=b'path')</span>
</code></dt>
<dd>
<section class="desc"><p>Extract files from SQLite3 databases.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/formats/archive/xtsql.py#L13-L73" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class xtsql(PathExtractorUnit):
    &#34;&#34;&#34;
    Extract files from SQLite3 databases.
    &#34;&#34;&#34;
    def unpack(self, data: bytearray):
        def _json(object):
            with BytesAsStringEncoder as encoder:
                return encoder.dumps(object).encode(self.codec)

        if sys.version_info[:2] &lt; (3, 11):
            raise NotImplementedError(F&#39;python 3.11 is required to use {self.__class__.__name__}.&#39;)

        database = sqlite3.connect(&#39;:memory:&#39;)
        database.deserialize(data)
        cursor = database.cursor()
        result: dict[str, list[dict[str, int | float | str | bytes]]] = {}

        listing: list[tuple[str, str]] = cursor.execute(
            &#34;SELECT name, sql FROM sqlite_master WHERE type=&#39;table&#39;;&#34;).fetchall()

        for table, spec in listing:
            result[table] = t = []
            ct, _table, names = spec.partition(table)
            names = names.strip()
            if (
                table != _table
                or ct.strip().upper().split() != [&#39;CREATE&#39;, &#39;TABLE&#39;]
                or not names.endswith(&#39;)&#39;)
                or not names.startswith(&#39;(&#39;)
            ):
                raise ValueError(F&#39;Unexpeted SQL statement in master table: {spec}&#39;)
            names = [next(iter(name.strip().split()))
                for name in names[1:-1].split(&#39;,&#39;)]
            for row in cursor.execute(F&#39;SELECT * FROM {table}&#39;).fetchall():
                t.append(dict(zip(names, row)))

        yield UnpackResult(&#39;db&#39;, functools.partial(_json, result))

        for table, rows in result.items():

            yield UnpackResult(F&#39;db/{table}&#39;, functools.partial(_json, rows))

            for k, row in enumerate(rows):

                root = F&#39;db/{table}/{k}&#39;
                yield UnpackResult(root, functools.partial(_json, row))

                for name, value in row.items():
                    path = F&#39;{root}/{name}&#39;
                    if value is None:
                        continue
                    if isinstance(value, (int, float)):
                        value = str(value)
                    if isinstance(value, str):
                        value = value.encode(self.codec)
                    if isinstance(value, bytes):
                        yield UnpackResult(path, value)

    @classmethod
    def handles(cls, data: bytearray):
        return memoryview(data)[:15] == B&#39;SQLite format 3&#39;</code></pre>
</details>
</dd>
<dt id="refinery.shell.xttar"><code class="flex name class">
<span>class <span class="ident">xttar</span></span>
<span>(</span><span>*paths, list=False, join_path=False, drop_path=False, fuzzy=0, exact=False, regex=False, path=b'path', date=b'date', pwd=b'')</span>
</code></dt>
<dd>
<section class="desc"><p>Extract files from a Tar archive.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/formats/archive/xttar.py#L10-L40" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class xttar(ArchiveUnit):
    &#34;&#34;&#34;
    Extract files from a Tar archive.
    &#34;&#34;&#34;
    def unpack(self, data: bytearray):
        with MemoryFile(data) as stream:
            try:
                archive = tarfile.open(fileobj=stream)
            except Exception:
                ustar = data.find(B&#39;ustar&#39;)
                if ustar &lt; 257:
                    raise
                stream.seek(ustar - 257)
                archive = tarfile.open(fileobj=stream)
        for info in archive.getmembers():
            if not info.isfile():
                continue
            extractor = archive.extractfile(info)
            if extractor is None:
                continue
            date = datetime.datetime.fromtimestamp(info.mtime)
            yield self._pack(info.name, date, lambda e=extractor: e.read())

    @classmethod
    def handles(cls, data: bytearray) -&gt; bool:
        ustar = data.find(B&#39;ustar&#39;)
        if ustar &lt; 0:
            return False
        if ustar == 257:
            return True
        return data[ustar + 5:ustar + 8] in (B&#39;\x00\x30\x30&#39;, B&#39;\x20\x20\x00&#39;)</code></pre>
</details>
</dd>
<dt id="refinery.shell.xtvba"><code class="flex name class">
<span>class <span class="ident">xtvba</span></span>
<span>(</span><span>*paths, list=False, join_path=False, drop_path=False, fuzzy=0, exact=False, regex=False, path=b'path')</span>
</code></dt>
<dd>
<section class="desc"><p>Extract VBA macro code from Office documents.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/formats/office/xtvba.py#L9-L31" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class xtvba(PathExtractorUnit):
    &#34;&#34;&#34;
    Extract VBA macro code from Office documents.
    &#34;&#34;&#34;
    @PathExtractorUnit.Requires(&#39;oletools&#39;, &#39;formats&#39;, &#39;office&#39;, &#39;extended&#39;)
    def _olevba():
        with NoLogging(NoLogging.Mode.ALL):
            import oletools.olevba
            return oletools.olevba

    def unpack(self, data):
        sentinel = uuid4()
        try:
            parser = self._olevba.VBA_Parser(sentinel, data=bytes(data), relaxed=True)
        except self._olevba.FileOpenError:
            raise ValueError(&#39;Input data not recognized by VBA parser&#39;)
        for p1, stream_path, p2, code in parser.extract_all_macros():
            if not stream_path:
                if p1 == sentinel:
                    continue
                if p2 == sentinel:
                    continue
            yield UnpackResult(stream_path, code.encode(self.codec))</code></pre>
</details>
</dd>
<dt id="refinery.shell.xtw"><code class="flex name class">
<span>class <span class="ident">xtw</span></span>
<span>(</span><span>stripspace=False, duplicates=False, longest=False, take=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Extract Wallets: Extracts anything that looks like a cryptocurrency wallet address.
This works similar to the <code><a title="refinery.xtp" href="index.html#refinery.xtp">xtp</a></code> unit.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/pattern/xtw.py#L12-L33" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class xtw(PatternExtractor):
    &#34;&#34;&#34;
    Extract Wallets: Extracts anything that looks like a cryptocurrency wallet address.
    This works similar to the `refinery.xtp` unit.
    &#34;&#34;&#34;

    def __init__(self, stripspace=False, duplicates=False, longest=False, take=None):
        self.superinit(super(), **vars(), ascii=True, utf16=True)

    def process(self, data):
        pattern = &#39;|&#39;.join(FR&#39;(?P&lt;{p.name}&gt;\b{p.value}\b)&#39; for p in wallets)
        pattern = FR&#39;\b{pattern}\b&#39;.encode(&#39;latin1&#39;)

        def check(match: re.Match[bytes]):
            for name, value in match.groupdict().items():
                if value is not None:
                    break
            else:
                raise RefineryCriticalException(&#39;Received empty match.&#39;)
            return self.labelled(value, kind=name)

        yield from self.matches_filtered(memoryview(data), pattern, check)</code></pre>
</details>
</dd>
<dt id="refinery.shell.xtxml"><code class="flex name class">
<span>class <span class="ident">xtxml</span></span>
<span>(</span><span>*paths, format=None, list=False, join_path=False, drop_path=False, fuzzy=0, exact=False, regex=False, path=b'path')</span>
</code></dt>
<dd>
<section class="desc"><p>Extract values from an XML document.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/formats/xml.py#L10-L44" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class xtxml(XMLToPathExtractorUnit):
    &#34;&#34;&#34;
    Extract values from an XML document.
    &#34;&#34;&#34;
    def unpack(self, data):
        root = xml.parse(data.strip())
        meta = metavars(data)
        path = self._make_path_builder(meta, root)

        def walk(node: xml.XMLNode, *parts: str):
            def extract(node: xml.XMLNode = node):
                if not node.children:
                    return node.content.encode(self.codec)
                with MemoryFile() as stream:
                    node.write(stream)
                    return bytes(stream.getbuffer() | ppxml)

            attributes = {
                self._normalize_key(k): self._normalize_val(v)
                for k, v in node.attributes.items()
            }

            if not all(is_valid_variable_name(k) for k in attributes):
                attributes = {F&#39;_{k}&#39;: v for k, v in attributes.items()}

            yield UnpackResult(&#39;/&#39;.join(parts), extract, **attributes)

            for child in node.children:
                yield from walk(child, *parts, path(child))

        yield from walk(root, path(root))

    @classmethod
    def handles(self, data):
        return xml.is_xml(data)</code></pre>
</details>
</dd>
<dt id="refinery.shell.xtzip"><code class="flex name class">
<span>class <span class="ident">xtzip</span></span>
<span>(</span><span>*paths, list=False, join_path=False, drop_path=False, fuzzy=0, exact=False, regex=False, path=b'path', date=b'date', pwd=b'')</span>
</code></dt>
<dd>
<section class="desc"><p>Extract files from a Zip archive.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/formats/archive/xtzip.py#L14-L106" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class xtzip(ArchiveUnit):
    &#34;&#34;&#34;
    Extract files from a Zip archive.
    &#34;&#34;&#34;
    @ArchiveUnit.Requires(&#39;chardet&#39;, &#39;default&#39;, &#39;extended&#39;)
    def _chardet():
        import chardet
        return chardet

    @ArchiveUnit.Requires(&#39;pyzipper&#39;, &#39;arc&#39;, &#39;default&#39;, &#39;extended&#39;)
    def _pyzipper():
        import pyzipper
        return pyzipper

    @classmethod
    def _carver(cls):
        return carve_zip

    def unpack(self, data: bytearray):
        from zipfile import ZipInfo, ZipFile

        def password_invalid(password: Optional[bytes]):
            nonlocal archive, fallback
            if password:
                archive.setpassword(password)
            try:
                archive.testzip()
            except NotImplementedError:
                if fallback:
                    raise
                self.log_debug(&#39;compression method unsupported, switching to pyzipper&#39;)
                archive = self._pyzipper.AESZipFile(MemoryFile(data))
                fallback = True
                return password_invalid(password)
            except RuntimeError as E:
                if &#39;password&#39; not in str(E):
                    raise
                return True
            else:
                if password:
                    self.log_debug(&#39;using password:&#39;, password)
                return False

        password = bytes(self.args.pwd)
        fallback = False
        archive = ZipFile(MemoryFile(data))
        passwords = [password]

        if not password:
            passwords.extend(p.encode(self.codec) for p in self._COMMON_PASSWORDS)
        for p in passwords:
            if not password_invalid(p):
                break
        else:
            raise RuntimeError(&#39;Archive is password-protected.&#39;)

        for info in archive.infolist():
            def xt(archive: ZipFile = archive, info: ZipInfo = info):
                try:
                    return archive.read(info.filename)
                except RuntimeError as E:
                    if &#39;password&#39; not in str(E):
                        raise
                    if not password:
                        raise RuntimeError(&#39;archive is password-protected&#39;)
                    else:
                        raise RuntimeError(F&#39;invalid password: {password.decode(self.codec)}&#39;) from E
            if info.filename:
                if info.is_dir():
                    continue

            # courtesy of https://stackoverflow.com/a/37773438/9130824
            filename = info.filename
            if info.flag_bits &amp; ZIP_FILENAME_UTF8_FLAG == 0:
                filename_bytes = filename.encode(&#39;437&#39;)
                try:
                    guessed_encoding = self._chardet.detect(filename_bytes)[&#39;encoding&#39;]
                except ImportError:
                    guessed_encoding = None
                guessed_encoding = guessed_encoding or &#39;cp1252&#39;
                filename = filename_bytes.decode(guessed_encoding, &#39;replace&#39;)

            try:
                date = datetime(*info.date_time)
            except Exception as e:
                self.log_info(F&#39;{e!s} - unable to determine date from tuple {info.date_time} for: {filename}&#39;)
                date = None

            yield self._pack(filename, date, xt)

    @classmethod
    def handles(cls, data: bytearray) -&gt; Optional[bool]:
        return data.rfind(ZipEndOfCentralDirectory.SIGNATURE) &gt; 0</code></pre>
</details>
</dd>
<dt id="refinery.shell.xtzpaq"><code class="flex name class">
<span>class <span class="ident">xtzpaq</span></span>
<span>(</span><span>*paths, index=False, pwd=b'', date=b'date', path=b'path', regex=False, exact=False, fuzzy=0, drop_path=False, join_path=False, list=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Extract files from a ZPAQ archive.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/formats/archive/xtzpaq.py#L1226-L1497" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class xtzpaq(ArchiveUnit):
    &#34;&#34;&#34;
    Extract files from a ZPAQ archive.
    &#34;&#34;&#34;

    _MAGIC = B&#39;\x37\x6B\x53\x74\xA0\x31\x83\xD3\x8C\xB2\x28\xB0\xD3\x7A\x50\x51&#39;

    def __init__(
        self, *paths,
        index: Arg.Switch(&#39;-i&#39;, help=&#39;Archive is an index (no d-blocks).&#39;) = False,
        **more
    ):
        for _code, _size in {
            _TCU32: 4,
            _TCI32: 4,
            _TCU16: 2,
            _TCI16: 2,
        }.items():
            _item_size = array(_code).itemsize
            if _item_size == _size:
                continue
            raise RuntimeError(
                F&#39;Expected array type &#34;{_code}&#34; to have entries of size {_size}, but the API &#39;
                F&#39;reports a size of {_item_size}.&#39;)

        super().__init__(*paths, index=index, **more)

    @classmethod
    def handles(cls, data: bytearray) -&gt; Optional[bool]:
        return cls._MAGIC in data

    def unpack(self, archive: bytearray):
        def mkdate(date) -&gt; datetime:
            date = int(date)
            year = date // 1000000 // 10000
            month = date // 100000000 % 100
            day = date // 1000000 % 100
            hour = date // 10000 % 100
            minute = date // 100 % 100
            second = date % 100
            return datetime(year, month, day, hour, minute, second, 0)

        @dataclass
        class DT:
            date: int = 0
            attr: int = 0
            name: str = &#34;&#34;
            frag: List[int] = field(default_factory=list)

            @property
            def dt(self) -&gt; Optional[datetime]:
                if self.date &gt; 0:
                    return mkdate(self.date)

        # TODO: implement password-protected archives
        # key = self.args.pwd
        index = self.args.index
        bsize: Dict[int, int] = {}  # frag ID -&gt; d block compressed size
        dt: Dict[str, DT] = {}      # filename -&gt; date, attr, frags
        frag: List[bytes] = []      # ID -&gt; hash[20] size[4] data
        csize = 0                   # expected offset of next non d block
        streaming = False
        journaling = False

        done = False
        dc = Decompressor()
        src = dc.set_input(archive)

        while not done and dc.read_block():
            while not done:
                filename = dc.read_filename()
                if filename is None:
                    break
                self.log_info(&#39;reading file&#39;, filename)
                comment = dc.read_comment()
                jsize = 0
                if len(comment) &gt;= 4 and comment[-4:] == &#34;jDC\x01&#34;:
                    num = re.search(&#39;^\\d+&#39;, comment)
                    if not num:
                        raise RuntimeError(&#39;missing size in comment&#39;)
                    jsize = int(num[0])
                    if streaming:
                        raise RuntimeError(&#39;journaling block after streaming one&#39;)
                    journaling = True
                    self.log_info(&#39;archive type is journaling&#39;)
                else:
                    if journaling:
                        raise RuntimeError(&#39;streaming block after journaling one&#39;)
                    if index:
                        raise RuntimeError(&#39;streaming block in index&#39;)
                    streaming = True
                    self.log_info(&#39;archive type is streaming&#39;)

                # Test journaling filename. The format must be
                # jDC[YYYYMMDDHHMMSS][t][NNNNNNNNNN]
                # where YYYYMMDDHHMMSS is the date, t is the type {c,d,h,i}, and
                # NNNNNNNNNN is the 10 digit first fragment ID for types c,d,h.
                # They must be in ascending lexicographical order.

                frag_id = 0
                block_type = None

                if journaling:
                    if len(filename) != 28:
                        raise RuntimeError(&#39;filename size not 28&#39;)
                    if filename[:3] != &#39;jDC&#39;:
                        raise RuntimeError(&#39;filename not jDC&#39;)
                    block_type = filename[17]
                    if block_type not in &#39;cdhi&#39;:
                        raise RuntimeError(&#39;type not c,d,h,i&#39;)
                    try:
                        mkdate(filename[3:17])
                    except Exception as E:
                        raise RuntimeError(&#39;invalid date&#39;) from E
                    frag_id = int(filename[18:28])
                    if not 1 &lt;= frag_id &lt;= 4294967295:
                        raise RuntimeError(&#39;fragment ID out of range&#39;)

                seg = MemoryFile(size_limit=jsize)
                dc.set_output(seg)
                sha1 = hashlib.sha1()
                dc.set_hasher(sha1)
                dc.decompress_data()

                if journaling and len(seg) != jsize:
                    raise RuntimeError(&#39;incomplete output&#39;)

                checksum = dc.read_segment_end()
                if checksum is None:
                    self.log_debug(&#39;no checksum&#39;)
                elif checksum != sha1.digest():
                    raise RuntimeError(&#39;SHA1 mismatch&#39;)

                # check csize at first non-d block
                if csize and block_type in &#39;chi&#39;:
                    if csize != offset:
                        raise RuntimeError(F&#39;csize={csize} does not point to offset={offset}&#39;)
                    csize = 0

                # get csize from c block
                seglen = len(seg)
                seg = StructReader(seg.getbuffer())
                if block_type == &#39;c&#39;:
                    if seglen &lt; 8:
                        raise RuntimeError(&#34;c block too small&#34;)
                    csize = seg.u64()
                    offset = src.tell() + 1
                    self.log_debug(F&#39;csize={csize} at offset={offset}&#39;)
                    if csize &gt;&gt; 63:
                        self.log_warn(&#39;incomplete transaction at end of archive&#39;)
                        done = True
                    elif index and csize != 0:
                        raise RuntimeError(&#39;nonzero csize in index&#39;)
                    # Set csize to expected offset of first non d block
                    # assuming 1 more byte for unread end of block marker.
                    csize += offset

                if block_type == &#39;d&#39;:
                    if index:
                        raise RuntimeError(&#39;d block in index&#39;)
                    bsize[frag_id] = src.tell() + 1 - offset  # compressed size
                    self.log_debug(F&#39; {bsize[frag_id]} -&gt; {len(seg)}&#39;)
                    # Test frag size list at end. The format is f[id..id+n-1] fid n
                    # where fid may be id or 0. sizes must sum to the rest of block.
                    if seglen &lt; 8:
                        raise RuntimeError(&#39;d block too small&#39;)
                    seg.seekset(-8)
                    fid = seg.u32() or frag_id
                    n = seg.u32()
                    if fid != frag_id:
                        raise RuntimeError(&#39;missing ID&#39;)
                    if n &gt; (seglen - 8) // 4:
                        raise RuntimeError(&#39;frag list too big&#39;)
                    fragsum = 0  # computed sum of frag sizes
                    seg.seekset(-4 * (n + 2))
                    for _ in range(n):
                        fragsum += seg.u32()
                    if fragsum + n * 4 + 8 != seglen:
                        raise RuntimeError(&#39;bad frag size list&#39;)
                    # Save frag hashes and sizes. For output, save data too.
                    seg.seekset(fragsum)
                    data = memoryview(seg.getbuffer())
                    assert seg.remaining_bytes == n * 4 + 8
                    for i in range(n):
                        while len(frag) &lt;= frag_id + i:
                            frag.append(B&#39;&#39;)
                        if frag[frag_id + i]:
                            raise RuntimeError(&#39;duplicate frag ID&#39;)
                        f = seg.u32()
                        h = hashlib.sha1(data[:f]).digest()
                        frag[frag_id + i] = h + f.to_bytes(4, &#39;little&#39;) + data[:f]
                        data = data[f:]

                    assert len(data) == n * 4 + 8
                    assert seg.remaining_bytes == 8

                # Test and save h block. Format is: bsize (sha1[20] size)...
                # where bsize is the compressed size of the d block with the same id,
                # and each size corresonds to a fragment in that block. The list
                # must match the list in the d block if present.

                if block_type == &#39;h&#39;:
                    if seglen % 24 != 4:
                        raise RuntimeError(&#39;bad h block size&#39;)
                    b = seg.u32()
                    self.log_debug(F&#39;[{frag_id}..{frag_id + seglen // 24}[ {b}&#39;)
                    fragsum = 0 # uncompressed size of all frags
                    for i in range(seglen // 24):
                        fd = seg.read(24)
                        if index:
                            while len(frag) &lt;= frag_id + i:
                                frag.append(B&#39;&#39;)
                            if frag[frag_id + i]:
                                raise RuntimeError(&#39;data in index&#39;)
                            frag[frag_id + i] = fd
                        elif frag_id + i &gt;= len(frag) or len(frag[frag_id + i]) &lt; 24:
                            raise RuntimeError(&#39;no matching d block&#39;)
                        elif frag[frag_id + i][:24] != fd:
                            raise RuntimeError(&#39;frag size or hash mismatch&#39;)
                        fragsum += int.from_bytes(fd[20:24], &#39;little&#39;)

                # Test i blocks and save files to extract. Format is:
                #   date filename 0 na attr[0..na) ni ptr[0..ni)   (to update)
                #   0    filename                                  (to delete)
                # Date is 64 bits in YYYYMMDDHHMMSS format.

                if block_type == &#39;i&#39;:
                    while not seg.eof:
                        f = DT(seg.u64())
                        f.name = seg.read_c_string(&#39;utf8&#39;)
                        if f.date &gt; 0:
                            na = seg.u32()
                            if na &gt; 65535:
                                raise ValueError(&#39;attr size &gt; 65535&#39;)
                            f.attr = seg.read_integer(na * 8)
                            ni = seg.u32()
                            for i in range(ni):
                                a = seg.u32()
                                f.frag.append(a)
                                if index:
                                    continue
                                elif not 1 &lt;= a &lt; len(frag):
                                    raise RuntimeError(&#39;frag ID out of range&#39;)
                                elif not frag[a]:
                                    raise LookupError(&#39;missing frag data&#39;)
                        dt[f.name] = f

                if streaming:
                    yield self._pack(filename, None, seg.getvalue())

            offset = src.tell()

        self.log_debug(F&#39;{offset} bytes of archive tested&#39;)

        if not journaling:
            return

        for name, f in dt.items():
            if not f.date:
                continue
            size = sum(
                int.from_bytes(frag[fp][20:24], &#39;little&#39;)
                for fp in f.frag
                if 0 &lt; fp &lt; len(frag) and len(frag[fp]) &gt;= 24
            )
            out = MemoryFile()
            for fp in f.frag:
                if fp &lt; len(frag):
                    out.write(memoryview(frag[fp])[24:])
            if len(out) != size:
                self.log_warn(&#39;invalid size during unpacking&#39;)
            yield self._pack(name, f.dt, out.getvalue())</code></pre>
</details>
</dd>
<dt id="refinery.shell.xxh"><code class="flex name class">
<span>class <span class="ident">xxh</span></span>
<span>(</span><span>seed=0, text=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Implements the xxHash hashing algorithm.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/crypto/hash/xxhash.py#L7-L19" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class xxh(HashUnit):
    &#34;&#34;&#34;
    Implements the xxHash hashing algorithm.
    &#34;&#34;&#34;
    def __init__(
        self,
        seed: HashUnit.Arg.Number(metavar=&#39;seed&#39;, help=&#39;specify the seed value; the default is {default}&#39;) = 0,
        text=False
    ):
        super().__init__(text, seed=seed)

    def _algorithm(self, data):
        return xxhash(data, self.args.seed)</code></pre>
</details>
</dd>
<dt id="refinery.shell.xxtea"><code class="flex name class">
<span>class <span class="ident">xxtea</span></span>
<span>(</span><span>key, iv=b'', padding=None, mode=None, raw=False, swap=False, block_size=1)</span>
</code></dt>
<dd>
<section class="desc"></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/crypto/cipher/xxtea.py#L54-L84" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class xxtea(TEAUnit, cipher=BlockCipherFactory(XXTEA)):

    block_size: int = 8

    def __init__(
        self, key, iv=b&#39;&#39;, padding=None, mode=None, raw=False, swap=False,
        block_size: Arg.Number(&#39;-b&#39;, help=(
            &#39;Cipher block size in 32-bit words. The default value {default} implies that the input &#39;
            &#39;is treated as a single block, which is common behaviour of many implementations.&#39;)) = 1
    ):
        super().__init__(key, iv, padding, mode, raw, swap=swap, block_size=block_size)

    def _prepare_block(self, data: bytes):
        if self.args.block_size &lt; 2:
            blocks, remainder = divmod(len(data), 4)
            if remainder:
                blocks += 1
            self.block_size = blocks * 4
        else:
            self.block_size = self.args.block_size * 4

    def encrypt(self, data: bytes) -&gt; bytes:
        self._prepare_block(data)
        return super().encrypt(data)

    def decrypt(self, data: bytes) -&gt; bytes:
        self._prepare_block(data)
        return super().decrypt(data)

    def _new_cipher(self, **optionals) -&gt; CipherInterface:
        return super()._new_cipher(block_size=self.block_size, **optionals)</code></pre>
</details>
</dd>
<dt id="refinery.shell.zl"><code class="flex name class">
<span>class <span class="ident">zl</span></span>
<span>(</span><span>level=9, window=15, zlib_header=False, gzip_header=False)</span>
</code></dt>
<dd>
<section class="desc"><p>ZLib compression and decompression.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/compression/zl.py#L10-L98" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class zl(Unit):
    &#34;&#34;&#34;
    ZLib compression and decompression.
    &#34;&#34;&#34;

    def __init__(
        self,
        level  : Arg.Number(&#39;-l&#39;, bound=(0, 0X9), help=&#39;Specify a compression level between 0 and 9.&#39;) = 9,
        window : Arg.Number(&#39;-w&#39;, bound=(8, 0XF), help=&#39;Manually specify the window size between 8 and 15.&#39;) = 15,
        zlib_header: Arg.Switch(&#39;-z&#39;, group=&#39;MODE&#39;, help=&#39;Use a ZLIB header.&#39;) = False,
        gzip_header: Arg.Switch(&#39;-g&#39;, group=&#39;MODE&#39;, help=&#39;Use a GZIP header.&#39;) = False
    ):
        if zlib_header and gzip_header:
            raise ValueError(&#39;You can only specify one header type (ZLIB or GZIP).&#39;)
        return super().__init__(level=level, window=window, zlib_header=zlib_header, gzip_header=gzip_header)

    def _decompress_data(self, data, mode: int, step: int):
        zl = zlib.decompressobj(mode)
        memory = memoryview(data)
        result = bytearray()
        while not zl.eof:
            read = min(step, len(memory))
            try:
                chunk = zl.decompress(memory[:read])
            except zlib.error as e:
                raise RefineryPartialResult(exception_to_string(e), result) from e
            else:
                result.extend(chunk)
                consumed = read - len(zl.unused_data)
                if not memory or consumed == 0:
                    break
                memory = memory[consumed:]
        return result, memory

    def process(self, data):
        if data[0] == 0x78 or data[0:2] == B&#39;\x1F\x8B&#39; or self.args.zlib_header or self.args.gzip_header:
            modes = [self.args.window | 0x20, -self.args.window]
        else:
            modes = [-self.args.window, self.args.window | 0x20]
        modes.extend([0x10 | self.args.window, 0])
        view = memoryview(data)
        step = 32 if self.leniency &gt; 0 else len(data)
        for k in itertools.count(1):
            error = None
            rest = view
            for mode in modes:
                try:
                    out, rest = self._decompress_data(view, mode, step)
                except Exception as e:
                    error = error or e
                else:
                    self.log_info(F&#39;used mode {mode} to decompress chunk {k}&#39;)
                    yield out
                    error = None
                    break
            if error:
                raise error
            if not rest:
                break
            if len(rest) == len(view):
                break
            if len(rest) &gt; len(view):
                raise RuntimeError(&#39;Decompressor returned more tail data than input data.&#39;)
            yield out
            view = rest
        if k &lt;= 0:
            raise ValueError(&#39;Could not detect any zlib stream.&#39;)

    def reverse(self, data):
        mode = -self.args.window
        if self.args.zlib_header:
            mode = -mode
        if self.args.gzip_header:
            mode = -mode | 0x10
        self.log_debug(F&#39;using mode {mode:+2d} for compression&#39;)
        zl = zlib.compressobj(self.args.level, zlib.DEFLATED, mode)
        zz = zl.compress(data)
        return zz + zl.flush(zlib.Z_FINISH)

    @classmethod
    def handles(self, data: bytearray):
        for sig in (
            B&#39;\x1F\x8B&#39;,  # gzip header
            B&#39;\x78\x01&#39;,  # zlib low compression
            B&#39;\x78\x9C&#39;,  # zlib medium compression
            B&#39;\x78\xDA&#39;,  # zlib high compression
        ):
            if data[:2] == sig:
                return True</code></pre>
</details>
</dd>
<dt id="refinery.shell.zstd"><code class="flex name class">
<span>class <span class="ident">zstd</span></span>
</code></dt>
<dd>
<section class="desc"><p>ZStandard (ZSTD) compression and decompression.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/binref/refinery/blob/668af76808fd0a712ccb43c4417a03c18188e85d/refinery/units/compression/zstd.py#L6-L25" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class zstd(Unit):
    &#34;&#34;&#34;
    ZStandard (ZSTD) compression and decompression.
    &#34;&#34;&#34;
    @Unit.Requires(&#39;pyzstd&#39;, &#39;all&#39;)
    def _pyzstd():
        import pyzstd
        return pyzstd

    def process(self, data):
        zd = self._pyzstd.ZstdDecompressor()
        return zd.decompress(data)

    def reverse(self, data):
        zc = self._pyzstd.ZstdCompressor()
        return zc.compress(data) + zc.flush()

    @classmethod
    def handles(self, data: bytearray) -&gt; bool:
        return data[:4] == B&#39;\x28\xB5\x2F\xFD&#39;</code></pre>
</details>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul>
<li><a href="#shell-like-unit-interface">Shell-Like Unit Interface</a></li>
</ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="refinery" href="index.html">refinery</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Units</a></h3>
<ul>
<li>
<h4><code><a title="refinery.shell.a3x" href="#refinery.shell.a3x">a3x</a></code></h4>
<ul class="">
<li><code><a title="refinery.shell.a3x.unpack" href="#refinery.shell.a3x.unpack">unpack</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="refinery.shell.a85" href="#refinery.shell.a85">a85</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.add" href="#refinery.shell.add">add</a></code></h4>
<ul class="">
<li><code><a title="refinery.shell.add.operate" href="#refinery.shell.add.operate">operate</a></code></li>
<li><code><a title="refinery.shell.add.inplace" href="#refinery.shell.add.inplace">inplace</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="refinery.shell.adler32" href="#refinery.shell.adler32">adler32</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.aes" href="#refinery.shell.aes">aes</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.alu" href="#refinery.shell.alu">alu</a></code></h4>
<ul class="">
<li><code><a title="refinery.shell.alu.operate" href="#refinery.shell.alu.operate">operate</a></code></li>
<li><code><a title="refinery.shell.alu.inplace" href="#refinery.shell.alu.inplace">inplace</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="refinery.shell.aplib" href="#refinery.shell.aplib">aplib</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.asm" href="#refinery.shell.asm">asm</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.atbash" href="#refinery.shell.atbash">atbash</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.autoxor" href="#refinery.shell.autoxor">autoxor</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.b32" href="#refinery.shell.b32">b32</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.b58" href="#refinery.shell.b58">b58</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.b62" href="#refinery.shell.b62">b62</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.b64" href="#refinery.shell.b64">b64</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.b65536" href="#refinery.shell.b65536">b65536</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.b85" href="#refinery.shell.b85">b85</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.b92" href="#refinery.shell.b92">b92</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.base" href="#refinery.shell.base">base</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.bat" href="#refinery.shell.bat">bat</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.bitrev" href="#refinery.shell.bitrev">bitrev</a></code></h4>
<ul class="">
<li><code><a title="refinery.shell.bitrev.operate" href="#refinery.shell.bitrev.operate">operate</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="refinery.shell.bitsnip" href="#refinery.shell.bitsnip">bitsnip</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.blabla" href="#refinery.shell.blabla">blabla</a></code></h4>
<ul class="">
<li><code><a title="refinery.shell.blabla.keystream" href="#refinery.shell.blabla.keystream">keystream</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="refinery.shell.blk224" href="#refinery.shell.blk224">blk224</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.blk256" href="#refinery.shell.blk256">blk256</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.blk384" href="#refinery.shell.blk384">blk384</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.blk512" href="#refinery.shell.blk512">blk512</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.blowfish" href="#refinery.shell.blowfish">blowfish</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.blz" href="#refinery.shell.blz">blz</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.brotli" href="#refinery.shell.brotli">brotli</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.bruteforce" href="#refinery.shell.bruteforce">bruteforce</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.byteswap" href="#refinery.shell.byteswap">byteswap</a></code></h4>
<ul class="">
<li><code><a title="refinery.shell.byteswap.inplace" href="#refinery.shell.byteswap.inplace">inplace</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="refinery.shell.bz2" href="#refinery.shell.bz2">bz2</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.camellia" href="#refinery.shell.camellia">camellia</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.carve" href="#refinery.shell.carve">carve</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.carve_7z" href="#refinery.shell.carve_7z">carve_7z</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.carve_json" href="#refinery.shell.carve_json">carve_json</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.carve_lnk" href="#refinery.shell.carve_lnk">carve_lnk</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.carve_pe" href="#refinery.shell.carve_pe">carve_pe</a></code></h4>
<ul class="">
<li><code><a title="refinery.shell.carve_pe.unpack" href="#refinery.shell.carve_pe.unpack">unpack</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="refinery.shell.carve_rtf" href="#refinery.shell.carve_rtf">carve_rtf</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.carve_xml" href="#refinery.shell.carve_xml">carve_xml</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.carve_zip" href="#refinery.shell.carve_zip">carve_zip</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.cast" href="#refinery.shell.cast">cast</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.cca" href="#refinery.shell.cca">cca</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.ccp" href="#refinery.shell.ccp">ccp</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.cfmt" href="#refinery.shell.cfmt">cfmt</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.chacha" href="#refinery.shell.chacha">chacha</a></code></h4>
<ul class="">
<li><code><a title="refinery.shell.chacha.keystream" href="#refinery.shell.chacha.keystream">keystream</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="refinery.shell.chacha20" href="#refinery.shell.chacha20">chacha20</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.chacha20poly1305" href="#refinery.shell.chacha20poly1305">chacha20poly1305</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.chaskey" href="#refinery.shell.chaskey">chaskey</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.chop" href="#refinery.shell.chop">chop</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.clower" href="#refinery.shell.clower">clower</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.cm" href="#refinery.shell.cm">cm</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.couple" href="#refinery.shell.couple">couple</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.cp1252" href="#refinery.shell.cp1252">cp1252</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.crc32" href="#refinery.shell.crc32">crc32</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.csb" href="#refinery.shell.csb">csb</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.csd" href="#refinery.shell.csd">csd</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.csv" href="#refinery.shell.csv">csv</a></code></h4>
<ul class="">
<li><code><a title="refinery.shell.csv.json_to_csv" href="#refinery.shell.csv.json_to_csv">json_to_csv</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="refinery.shell.cswap" href="#refinery.shell.cswap">cswap</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.cupper" href="#refinery.shell.cupper">cupper</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.datefix" href="#refinery.shell.datefix">datefix</a></code></h4>
<ul class="">
<li><code><a title="refinery.shell.datefix.dostime" href="#refinery.shell.datefix.dostime">dostime</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="refinery.shell.decompress" href="#refinery.shell.decompress">decompress</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.dedup" href="#refinery.shell.dedup">dedup</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.defang" href="#refinery.shell.defang">defang</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.deob_js_arrays" href="#refinery.shell.deob_js_arrays">deob_js_arrays</a></code></h4>
<ul class="">
<li><code><a title="refinery.shell.deob_js_arrays.deobfuscate" href="#refinery.shell.deob_js_arrays.deobfuscate">deobfuscate</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="refinery.shell.deob_js_getattr" href="#refinery.shell.deob_js_getattr">deob_js_getattr</a></code></h4>
<ul class="">
<li><code><a title="refinery.shell.deob_js_getattr.deobfuscate" href="#refinery.shell.deob_js_getattr.deobfuscate">deobfuscate</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="refinery.shell.deob_js_tuples" href="#refinery.shell.deob_js_tuples">deob_js_tuples</a></code></h4>
<ul class="">
<li><code><a title="refinery.shell.deob_js_tuples.deobfuscate" href="#refinery.shell.deob_js_tuples.deobfuscate">deobfuscate</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="refinery.shell.deob_ps1" href="#refinery.shell.deob_ps1">deob_ps1</a></code></h4>
<ul class="">
<li><code><a title="refinery.shell.deob_ps1.deobfuscate" href="#refinery.shell.deob_ps1.deobfuscate">deobfuscate</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="refinery.shell.deob_ps1_b64convert" href="#refinery.shell.deob_ps1_b64convert">deob_ps1_b64convert</a></code></h4>
<ul class="">
<li><code><a title="refinery.shell.deob_ps1_b64convert.deobfuscate" href="#refinery.shell.deob_ps1_b64convert.deobfuscate">deobfuscate</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="refinery.shell.deob_ps1_brackets" href="#refinery.shell.deob_ps1_brackets">deob_ps1_brackets</a></code></h4>
<ul class="">
<li><code><a title="refinery.shell.deob_ps1_brackets.deobfuscate" href="#refinery.shell.deob_ps1_brackets.deobfuscate">deobfuscate</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="refinery.shell.deob_ps1_cases" href="#refinery.shell.deob_ps1_cases">deob_ps1_cases</a></code></h4>
<ul class="">
<li><code><a title="refinery.shell.deob_ps1_cases.deobfuscate" href="#refinery.shell.deob_ps1_cases.deobfuscate">deobfuscate</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="refinery.shell.deob_ps1_concat" href="#refinery.shell.deob_ps1_concat">deob_ps1_concat</a></code></h4>
<ul class="">
<li><code><a title="refinery.shell.deob_ps1_concat.deobfuscate" href="#refinery.shell.deob_ps1_concat.deobfuscate">deobfuscate</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="refinery.shell.deob_ps1_encodings" href="#refinery.shell.deob_ps1_encodings">deob_ps1_encodings</a></code></h4>
<ul class="">
<li><code><a title="refinery.shell.deob_ps1_encodings.deobfuscate" href="#refinery.shell.deob_ps1_encodings.deobfuscate">deobfuscate</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="refinery.shell.deob_ps1_escape" href="#refinery.shell.deob_ps1_escape">deob_ps1_escape</a></code></h4>
<ul class="">
<li><code><a title="refinery.shell.deob_ps1_escape.deobfuscate" href="#refinery.shell.deob_ps1_escape.deobfuscate">deobfuscate</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="refinery.shell.deob_ps1_format" href="#refinery.shell.deob_ps1_format">deob_ps1_format</a></code></h4>
<ul class="">
<li><code><a title="refinery.shell.deob_ps1_format.deobfuscate" href="#refinery.shell.deob_ps1_format.deobfuscate">deobfuscate</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="refinery.shell.deob_ps1_invoke" href="#refinery.shell.deob_ps1_invoke">deob_ps1_invoke</a></code></h4>
<ul class="">
<li><code><a title="refinery.shell.deob_ps1_invoke.deobfuscate" href="#refinery.shell.deob_ps1_invoke.deobfuscate">deobfuscate</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="refinery.shell.deob_ps1_secstr" href="#refinery.shell.deob_ps1_secstr">deob_ps1_secstr</a></code></h4>
<ul class="">
<li><code><a title="refinery.shell.deob_ps1_secstr.deobfuscate" href="#refinery.shell.deob_ps1_secstr.deobfuscate">deobfuscate</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="refinery.shell.deob_ps1_stringreplace" href="#refinery.shell.deob_ps1_stringreplace">deob_ps1_stringreplace</a></code></h4>
<ul class="">
<li><code><a title="refinery.shell.deob_ps1_stringreplace.deobfuscate" href="#refinery.shell.deob_ps1_stringreplace.deobfuscate">deobfuscate</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="refinery.shell.deob_ps1_typecast" href="#refinery.shell.deob_ps1_typecast">deob_ps1_typecast</a></code></h4>
<ul class="">
<li><code><a title="refinery.shell.deob_ps1_typecast.deobfuscate" href="#refinery.shell.deob_ps1_typecast.deobfuscate">deobfuscate</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="refinery.shell.deob_ps1_uncurly" href="#refinery.shell.deob_ps1_uncurly">deob_ps1_uncurly</a></code></h4>
<ul class="">
<li><code><a title="refinery.shell.deob_ps1_uncurly.deobfuscate" href="#refinery.shell.deob_ps1_uncurly.deobfuscate">deobfuscate</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="refinery.shell.deob_vba" href="#refinery.shell.deob_vba">deob_vba</a></code></h4>
<ul class="">
<li><code><a title="refinery.shell.deob_vba.deobfuscate" href="#refinery.shell.deob_vba.deobfuscate">deobfuscate</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="refinery.shell.deob_vba_arithmetic" href="#refinery.shell.deob_vba_arithmetic">deob_vba_arithmetic</a></code></h4>
<ul class="">
<li><code><a title="refinery.shell.deob_vba_arithmetic.deobfuscate" href="#refinery.shell.deob_vba_arithmetic.deobfuscate">deobfuscate</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="refinery.shell.deob_vba_brackets" href="#refinery.shell.deob_vba_brackets">deob_vba_brackets</a></code></h4>
<ul class="">
<li><code><a title="refinery.shell.deob_vba_brackets.deobfuscate" href="#refinery.shell.deob_vba_brackets.deobfuscate">deobfuscate</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="refinery.shell.deob_vba_char_function" href="#refinery.shell.deob_vba_char_function">deob_vba_char_function</a></code></h4>
<ul class="">
<li><code><a title="refinery.shell.deob_vba_char_function.deobfuscate" href="#refinery.shell.deob_vba_char_function.deobfuscate">deobfuscate</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="refinery.shell.deob_vba_chr_literals" href="#refinery.shell.deob_vba_chr_literals">deob_vba_chr_literals</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.deob_vba_comments" href="#refinery.shell.deob_vba_comments">deob_vba_comments</a></code></h4>
<ul class="">
<li><code><a title="refinery.shell.deob_vba_comments.deobfuscate" href="#refinery.shell.deob_vba_comments.deobfuscate">deobfuscate</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="refinery.shell.deob_vba_concat" href="#refinery.shell.deob_vba_concat">deob_vba_concat</a></code></h4>
<ul class="">
<li><code><a title="refinery.shell.deob_vba_concat.deobfuscate" href="#refinery.shell.deob_vba_concat.deobfuscate">deobfuscate</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="refinery.shell.deob_vba_constants" href="#refinery.shell.deob_vba_constants">deob_vba_constants</a></code></h4>
<ul class="">
<li><code><a title="refinery.shell.deob_vba_constants.deobfuscate" href="#refinery.shell.deob_vba_constants.deobfuscate">deobfuscate</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="refinery.shell.deob_vba_dummy_variables" href="#refinery.shell.deob_vba_dummy_variables">deob_vba_dummy_variables</a></code></h4>
<ul class="">
<li><code><a title="refinery.shell.deob_vba_dummy_variables.deobfuscate" href="#refinery.shell.deob_vba_dummy_variables.deobfuscate">deobfuscate</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="refinery.shell.deob_vba_stringreplace" href="#refinery.shell.deob_vba_stringreplace">deob_vba_stringreplace</a></code></h4>
<ul class="">
<li><code><a title="refinery.shell.deob_vba_stringreplace.deobfuscate" href="#refinery.shell.deob_vba_stringreplace.deobfuscate">deobfuscate</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="refinery.shell.deob_vba_stringreverse" href="#refinery.shell.deob_vba_stringreverse">deob_vba_stringreverse</a></code></h4>
<ul class="">
<li><code><a title="refinery.shell.deob_vba_stringreverse.deobfuscate" href="#refinery.shell.deob_vba_stringreverse.deobfuscate">deobfuscate</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="refinery.shell.des" href="#refinery.shell.des">des</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.des3" href="#refinery.shell.des3">des3</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.deskd" href="#refinery.shell.deskd">deskd</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.dexstr" href="#refinery.shell.dexstr">dexstr</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.dnarrays" href="#refinery.shell.dnarrays">dnarrays</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.dnblob" href="#refinery.shell.dnblob">dnblob</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.dncfx" href="#refinery.shell.dncfx">dncfx</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.dnds" href="#refinery.shell.dnds">dnds</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.dnfields" href="#refinery.shell.dnfields">dnfields</a></code></h4>
<ul class="">
<li><code><a title="refinery.shell.dnfields.unpack" href="#refinery.shell.dnfields.unpack">unpack</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="refinery.shell.dnhdr" href="#refinery.shell.dnhdr">dnhdr</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.dnmr" href="#refinery.shell.dnmr">dnmr</a></code></h4>
<ul class="">
<li><code><a title="refinery.shell.dnmr.unpack" href="#refinery.shell.dnmr.unpack">unpack</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="refinery.shell.dnrc" href="#refinery.shell.dnrc">dnrc</a></code></h4>
<ul class="">
<li><code><a title="refinery.shell.dnrc.unpack" href="#refinery.shell.dnrc.unpack">unpack</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="refinery.shell.dnsdomain" href="#refinery.shell.dnsdomain">dnsdomain</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.dnsfx" href="#refinery.shell.dnsfx">dnsfx</a></code></h4>
<ul class="">
<li><code><a title="refinery.shell.dnsfx.unpack" href="#refinery.shell.dnsfx.unpack">unpack</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="refinery.shell.dnstr" href="#refinery.shell.dnstr">dnstr</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.doctxt" href="#refinery.shell.doctxt">doctxt</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.drp" href="#refinery.shell.drp">drp</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.dsjava" href="#refinery.shell.dsjava">dsjava</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.dsphp" href="#refinery.shell.dsphp">dsphp</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.dump" href="#refinery.shell.dump">dump</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.eat" href="#refinery.shell.eat">eat</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.ef" href="#refinery.shell.ef">ef</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.emit" href="#refinery.shell.emit">emit</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.esc" href="#refinery.shell.esc">esc</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.evtx" href="#refinery.shell.evtx">evtx</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.fernet" href="#refinery.shell.fernet">fernet</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.gost" href="#refinery.shell.gost">gost</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.group" href="#refinery.shell.group">group</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.groupby" href="#refinery.shell.groupby">groupby</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.hc128" href="#refinery.shell.hc128">hc128</a></code></h4>
<ul class="">
<li><code><a title="refinery.shell.hc128.keystream" href="#refinery.shell.hc128.keystream">keystream</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="refinery.shell.hc256" href="#refinery.shell.hc256">hc256</a></code></h4>
<ul class="">
<li><code><a title="refinery.shell.hc256.keystream" href="#refinery.shell.hc256.keystream">keystream</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="refinery.shell.hex" href="#refinery.shell.hex">hex</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.hexload" href="#refinery.shell.hexload">hexload</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.HKDF" href="#refinery.shell.HKDF">HKDF</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.hmac" href="#refinery.shell.hmac">hmac</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.htmlesc" href="#refinery.shell.htmlesc">htmlesc</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.httprequest" href="#refinery.shell.httprequest">httprequest</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.httpresponse" href="#refinery.shell.httpresponse">httpresponse</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.iemap" href="#refinery.shell.iemap">iemap</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.iff" href="#refinery.shell.iff">iff</a></code></h4>
<ul class="">
<li><code><a title="refinery.shell.iff.match" href="#refinery.shell.iff.match">match</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="refinery.shell.iffp" href="#refinery.shell.iffp">iffp</a></code></h4>
<ul class="">
<li><code><a title="refinery.shell.iffp.match" href="#refinery.shell.iffp.match">match</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="refinery.shell.iffs" href="#refinery.shell.iffs">iffs</a></code></h4>
<ul class="">
<li><code><a title="refinery.shell.iffs.match" href="#refinery.shell.iffs.match">match</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="refinery.shell.iffx" href="#refinery.shell.iffx">iffx</a></code></h4>
<ul class="">
<li><code><a title="refinery.shell.iffx.match" href="#refinery.shell.iffx.match">match</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="refinery.shell.ifps" href="#refinery.shell.ifps">ifps</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.ifpsstr" href="#refinery.shell.ifpsstr">ifpsstr</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.imphash" href="#refinery.shell.imphash">imphash</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.isaac" href="#refinery.shell.isaac">isaac</a></code></h4>
<ul class="">
<li><code><a title="refinery.shell.isaac.keystream" href="#refinery.shell.isaac.keystream">keystream</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="refinery.shell.jcalg" href="#refinery.shell.jcalg">jcalg</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.jvdasm" href="#refinery.shell.jvdasm">jvdasm</a></code></h4>
<ul class="">
<li><code><a title="refinery.shell.jvdasm.unpack" href="#refinery.shell.jvdasm.unpack">unpack</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="refinery.shell.jvstr" href="#refinery.shell.jvstr">jvstr</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.kblob" href="#refinery.shell.kblob">kblob</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.keccak256" href="#refinery.shell.keccak256">keccak256</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.kramer" href="#refinery.shell.kramer">kramer</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.lnk" href="#refinery.shell.lnk">lnk</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.loop" href="#refinery.shell.loop">loop</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.lz4" href="#refinery.shell.lz4">lz4</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.lzf" href="#refinery.shell.lzf">lzf</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.lzg" href="#refinery.shell.lzg">lzg</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.lzip" href="#refinery.shell.lzip">lzip</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.lzjb" href="#refinery.shell.lzjb">lzjb</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.lzma" href="#refinery.shell.lzma">lzma</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.lznt1" href="#refinery.shell.lznt1">lznt1</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.lzo" href="#refinery.shell.lzo">lzo</a></code></h4>
<ul class="">
<li><code><a title="refinery.shell.lzo.decompress_stream" href="#refinery.shell.lzo.decompress_stream">decompress_stream</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="refinery.shell.lzw" href="#refinery.shell.lzw">lzw</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.machometa" href="#refinery.shell.machometa">machometa</a></code></h4>
<ul class="two-column">
<li><code><a title="refinery.shell.machometa.compute_symhash" href="#refinery.shell.machometa.compute_symhash">compute_symhash</a></code></li>
<li><code><a title="refinery.shell.machometa.parse_macho_header" href="#refinery.shell.machometa.parse_macho_header">parse_macho_header</a></code></li>
<li><code><a title="refinery.shell.machometa.parse_linked_images" href="#refinery.shell.machometa.parse_linked_images">parse_linked_images</a></code></li>
<li><code><a title="refinery.shell.machometa.parse_signature" href="#refinery.shell.machometa.parse_signature">parse_signature</a></code></li>
<li><code><a title="refinery.shell.machometa.parse_version" href="#refinery.shell.machometa.parse_version">parse_version</a></code></li>
<li><code><a title="refinery.shell.machometa.parse_load_commands" href="#refinery.shell.machometa.parse_load_commands">parse_load_commands</a></code></li>
<li><code><a title="refinery.shell.machometa.parse_imports" href="#refinery.shell.machometa.parse_imports">parse_imports</a></code></li>
<li><code><a title="refinery.shell.machometa.parse_exports" href="#refinery.shell.machometa.parse_exports">parse_exports</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="refinery.shell.map" href="#refinery.shell.map">map</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.maru" href="#refinery.shell.maru">maru</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.max_" href="#refinery.shell.max_">max_</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.md2" href="#refinery.shell.md2">md2</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.md4" href="#refinery.shell.md4">md4</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.md5" href="#refinery.shell.md5">md5</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.mimewords" href="#refinery.shell.mimewords">mimewords</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.min_" href="#refinery.shell.min_">min_</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.mmh128x32" href="#refinery.shell.mmh128x32">mmh128x32</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.mmh128x64" href="#refinery.shell.mmh128x64">mmh128x64</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.mmh32" href="#refinery.shell.mmh32">mmh32</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.morse" href="#refinery.shell.morse">morse</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.mscdk" href="#refinery.shell.mscdk">mscdk</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.mscf" href="#refinery.shell.mscf">mscf</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.msgpack" href="#refinery.shell.msgpack">msgpack</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.mspdb" href="#refinery.shell.mspdb">mspdb</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.mvg" href="#refinery.shell.mvg">mvg</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.n40" href="#refinery.shell.n40">n40</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.neg" href="#refinery.shell.neg">neg</a></code></h4>
<ul class="">
<li><code><a title="refinery.shell.neg.operate" href="#refinery.shell.neg.operate">operate</a></code></li>
<li><code><a title="refinery.shell.neg.inplace" href="#refinery.shell.neg.inplace">inplace</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="refinery.shell.netbios" href="#refinery.shell.netbios">netbios</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.ngrams" href="#refinery.shell.ngrams">ngrams</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.nop" href="#refinery.shell.nop">nop</a></code></h4>
<ul class="">
<li><code><a title="refinery.shell.nop.argparser" href="#refinery.shell.nop.argparser">argparser</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="refinery.shell.nrv2b" href="#refinery.shell.nrv2b">nrv2b</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.nrv2d" href="#refinery.shell.nrv2d">nrv2d</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.nrv2e" href="#refinery.shell.nrv2e">nrv2e</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.ntlm" href="#refinery.shell.ntlm">ntlm</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.officecrypt" href="#refinery.shell.officecrypt">officecrypt</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.opc" href="#refinery.shell.opc">opc</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.p1" href="#refinery.shell.p1">p1</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.p2" href="#refinery.shell.p2">p2</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.p3" href="#refinery.shell.p3">p3</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.pack" href="#refinery.shell.pack">pack</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.pad" href="#refinery.shell.pad">pad</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.pbkdf1" href="#refinery.shell.pbkdf1">pbkdf1</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.pbkdf2" href="#refinery.shell.pbkdf2">pbkdf2</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.pcap" href="#refinery.shell.pcap">pcap</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.pcap_http" href="#refinery.shell.pcap_http">pcap_http</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.pedebloat" href="#refinery.shell.pedebloat">pedebloat</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.peek" href="#refinery.shell.peek">peek</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.pemeta" href="#refinery.shell.pemeta">pemeta</a></code></h4>
<ul class="two-column">
<li><code><a title="refinery.shell.pemeta.parse_signature" href="#refinery.shell.pemeta.parse_signature">parse_signature</a></code></li>
<li><code><a title="refinery.shell.pemeta.parse_version" href="#refinery.shell.pemeta.parse_version">parse_version</a></code></li>
<li><code><a title="refinery.shell.pemeta.parse_exports" href="#refinery.shell.pemeta.parse_exports">parse_exports</a></code></li>
<li><code><a title="refinery.shell.pemeta.parse_imports" href="#refinery.shell.pemeta.parse_imports">parse_imports</a></code></li>
<li><code><a title="refinery.shell.pemeta.parse_header" href="#refinery.shell.pemeta.parse_header">parse_header</a></code></li>
<li><code><a title="refinery.shell.pemeta.parse_time_stamps" href="#refinery.shell.pemeta.parse_time_stamps">parse_time_stamps</a></code></li>
<li><code><a title="refinery.shell.pemeta.parse_dotnet" href="#refinery.shell.pemeta.parse_dotnet">parse_dotnet</a></code></li>
<li><code><a title="refinery.shell.pemeta.parse_debug" href="#refinery.shell.pemeta.parse_debug">parse_debug</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="refinery.shell.peoverlay" href="#refinery.shell.peoverlay">peoverlay</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.perc" href="#refinery.shell.perc">perc</a></code></h4>
<ul class="">
<li><code><a title="refinery.shell.perc.unpack" href="#refinery.shell.perc.unpack">unpack</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="refinery.shell.pesig" href="#refinery.shell.pesig">pesig</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.pestrip" href="#refinery.shell.pestrip">pestrip</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.pick" href="#refinery.shell.pick">pick</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.pkcs7" href="#refinery.shell.pkcs7">pkcs7</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.pkcs7sig" href="#refinery.shell.pkcs7sig">pkcs7sig</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.pop" href="#refinery.shell.pop">pop</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.ppjscript" href="#refinery.shell.ppjscript">ppjscript</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.ppjson" href="#refinery.shell.ppjson">ppjson</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.ppxml" href="#refinery.shell.ppxml">ppxml</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.ps1str" href="#refinery.shell.ps1str">ps1str</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.push" href="#refinery.shell.push">push</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.put" href="#refinery.shell.put">put</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.pyc" href="#refinery.shell.pyc">pyc</a></code></h4>
<ul class="">
<li><code><a title="refinery.shell.pyc.unpack" href="#refinery.shell.pyc.unpack">unpack</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="refinery.shell.pym" href="#refinery.shell.pym">pym</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.qb" href="#refinery.shell.qb">qb</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.qf" href="#refinery.shell.qf">qf</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.qlz" href="#refinery.shell.qlz">qlz</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.rabbit" href="#refinery.shell.rabbit">rabbit</a></code></h4>
<ul class="">
<li><code><a title="refinery.shell.rabbit.keystream" href="#refinery.shell.rabbit.keystream">keystream</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="refinery.shell.rc2" href="#refinery.shell.rc2">rc2</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.rc4" href="#refinery.shell.rc4">rc4</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.rc4mod" href="#refinery.shell.rc4mod">rc4mod</a></code></h4>
<ul class="">
<li><code><a title="refinery.shell.rc4mod.keystream" href="#refinery.shell.rc4mod.keystream">keystream</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="refinery.shell.rc5" href="#refinery.shell.rc5">rc5</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.rc6" href="#refinery.shell.rc6">rc6</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.recode" href="#refinery.shell.recode">recode</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.reduce" href="#refinery.shell.reduce">reduce</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.rep" href="#refinery.shell.rep">rep</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.repl" href="#refinery.shell.repl">repl</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.resplit" href="#refinery.shell.resplit">resplit</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.resub" href="#refinery.shell.resub">resub</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.rev" href="#refinery.shell.rev">rev</a></code></h4>
<ul class="">
<li><code><a title="refinery.shell.rev.inplace" href="#refinery.shell.rev.inplace">inplace</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="refinery.shell.rex" href="#refinery.shell.rex">rex</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.rijndael" href="#refinery.shell.rijndael">rijndael</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.ripemd128" href="#refinery.shell.ripemd128">ripemd128</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.ripemd160" href="#refinery.shell.ripemd160">ripemd160</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.rmv" href="#refinery.shell.rmv">rmv</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.rncrypt" href="#refinery.shell.rncrypt">rncrypt</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.rot" href="#refinery.shell.rot">rot</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.rotl" href="#refinery.shell.rotl">rotl</a></code></h4>
<ul class="">
<li><code><a title="refinery.shell.rotl.operate" href="#refinery.shell.rotl.operate">operate</a></code></li>
<li><code><a title="refinery.shell.rotl.inplace" href="#refinery.shell.rotl.inplace">inplace</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="refinery.shell.rotr" href="#refinery.shell.rotr">rotr</a></code></h4>
<ul class="">
<li><code><a title="refinery.shell.rotr.operate" href="#refinery.shell.rotr.operate">operate</a></code></li>
<li><code><a title="refinery.shell.rotr.inplace" href="#refinery.shell.rotr.inplace">inplace</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="refinery.shell.rsa" href="#refinery.shell.rsa">rsa</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.rsakey" href="#refinery.shell.rsakey">rsakey</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.salsa" href="#refinery.shell.salsa">salsa</a></code></h4>
<ul class="">
<li><code><a title="refinery.shell.salsa.keystream" href="#refinery.shell.salsa.keystream">keystream</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="refinery.shell.salsa20" href="#refinery.shell.salsa20">salsa20</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.scope" href="#refinery.shell.scope">scope</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.seal" href="#refinery.shell.seal">seal</a></code></h4>
<ul class="">
<li><code><a title="refinery.shell.seal.keystream" href="#refinery.shell.seal.keystream">keystream</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="refinery.shell.secstr" href="#refinery.shell.secstr">secstr</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.sep" href="#refinery.shell.sep">sep</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.serpent" href="#refinery.shell.serpent">serpent</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.sha1" href="#refinery.shell.sha1">sha1</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.sha224" href="#refinery.shell.sha224">sha224</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.sha256" href="#refinery.shell.sha256">sha256</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.sha384" href="#refinery.shell.sha384">sha384</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.sha3_224" href="#refinery.shell.sha3_224">sha3_224</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.sha3_256" href="#refinery.shell.sha3_256">sha3_256</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.sha3_384" href="#refinery.shell.sha3_384">sha3_384</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.sha3_512" href="#refinery.shell.sha3_512">sha3_512</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.sha512" href="#refinery.shell.sha512">sha512</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.shl" href="#refinery.shell.shl">shl</a></code></h4>
<ul class="">
<li><code><a title="refinery.shell.shl.operate" href="#refinery.shell.shl.operate">operate</a></code></li>
<li><code><a title="refinery.shell.shl.inplace" href="#refinery.shell.shl.inplace">inplace</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="refinery.shell.shr" href="#refinery.shell.shr">shr</a></code></h4>
<ul class="">
<li><code><a title="refinery.shell.shr.operate" href="#refinery.shell.shr.operate">operate</a></code></li>
<li><code><a title="refinery.shell.shr.inplace" href="#refinery.shell.shr.inplace">inplace</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="refinery.shell.sm4" href="#refinery.shell.sm4">sm4</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.snip" href="#refinery.shell.snip">snip</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.sorted" href="#refinery.shell.sorted">sorted</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.sosemanuk" href="#refinery.shell.sosemanuk">sosemanuk</a></code></h4>
<ul class="">
<li><code><a title="refinery.shell.sosemanuk.keystream" href="#refinery.shell.sosemanuk.keystream">keystream</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="refinery.shell.speck" href="#refinery.shell.speck">speck</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.stego" href="#refinery.shell.stego">stego</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.stretch" href="#refinery.shell.stretch">stretch</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.struct" href="#refinery.shell.struct">struct</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.sub" href="#refinery.shell.sub">sub</a></code></h4>
<ul class="">
<li><code><a title="refinery.shell.sub.operate" href="#refinery.shell.sub.operate">operate</a></code></li>
<li><code><a title="refinery.shell.sub.inplace" href="#refinery.shell.sub.inplace">inplace</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="refinery.shell.subfiles" href="#refinery.shell.subfiles">subfiles</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.swap" href="#refinery.shell.swap">swap</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.szdd" href="#refinery.shell.szdd">szdd</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.tea" href="#refinery.shell.tea">tea</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.termfit" href="#refinery.shell.termfit">termfit</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.terminate" href="#refinery.shell.terminate">terminate</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.transpose" href="#refinery.shell.transpose">transpose</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.trim" href="#refinery.shell.trim">trim</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.u16" href="#refinery.shell.u16">u16</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.ucrypt" href="#refinery.shell.ucrypt">ucrypt</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.url" href="#refinery.shell.url">url</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.urlfix" href="#refinery.shell.urlfix">urlfix</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.urlguards" href="#refinery.shell.urlguards">urlguards</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.urn" href="#refinery.shell.urn">urn</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.uuenc" href="#refinery.shell.uuenc">uuenc</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.vaddr" href="#refinery.shell.vaddr">vaddr</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.vbapc" href="#refinery.shell.vbapc">vbapc</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.vbastr" href="#refinery.shell.vbastr">vbastr</a></code></h4>
<ul class="">
<li><code><a title="refinery.shell.vbastr.unpack" href="#refinery.shell.vbastr.unpack">unpack</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="refinery.shell.vigenere" href="#refinery.shell.vigenere">vigenere</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.vmemref" href="#refinery.shell.vmemref">vmemref</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.vsect" href="#refinery.shell.vsect">vsect</a></code></h4>
<ul class="">
<li><code><a title="refinery.shell.vsect.unpack" href="#refinery.shell.vsect.unpack">unpack</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="refinery.shell.vsnip" href="#refinery.shell.vsnip">vsnip</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.vstack" href="#refinery.shell.vstack">vstack</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.winreg" href="#refinery.shell.winreg">winreg</a></code></h4>
<ul class="">
<li><code><a title="refinery.shell.winreg.unpack" href="#refinery.shell.winreg.unpack">unpack</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="refinery.shell.wshenc" href="#refinery.shell.wshenc">wshenc</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.xchacha" href="#refinery.shell.xchacha">xchacha</a></code></h4>
<ul class="">
<li><code><a title="refinery.shell.xchacha.keystream" href="#refinery.shell.xchacha.keystream">keystream</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="refinery.shell.xfcc" href="#refinery.shell.xfcc">xfcc</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.xj0" href="#refinery.shell.xj0">xj0</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.xjl" href="#refinery.shell.xjl">xjl</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.xkey" href="#refinery.shell.xkey">xkey</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.xlmdeobf" href="#refinery.shell.xlmdeobf">xlmdeobf</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.xlxtr" href="#refinery.shell.xlxtr">xlxtr</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.xor" href="#refinery.shell.xor">xor</a></code></h4>
<ul class="">
<li><code><a title="refinery.shell.xor.operate" href="#refinery.shell.xor.operate">operate</a></code></li>
<li><code><a title="refinery.shell.xor.inplace" href="#refinery.shell.xor.inplace">inplace</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="refinery.shell.xsalsa" href="#refinery.shell.xsalsa">xsalsa</a></code></h4>
<ul class="">
<li><code><a title="refinery.shell.xsalsa.keystream" href="#refinery.shell.xsalsa.keystream">keystream</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="refinery.shell.xt" href="#refinery.shell.xt">xt</a></code></h4>
<ul class="">
<li><code><a title="refinery.shell.xt.handlers" href="#refinery.shell.xt.handlers">handlers</a></code></li>
<li><code><a title="refinery.shell.xt.unpack" href="#refinery.shell.xt.unpack">unpack</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="refinery.shell.xt7z" href="#refinery.shell.xt7z">xt7z</a></code></h4>
<ul class="">
<li><code><a title="refinery.shell.xt7z.unpack" href="#refinery.shell.xt7z.unpack">unpack</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="refinery.shell.xtace" href="#refinery.shell.xtace">xtace</a></code></h4>
<ul class="">
<li><code><a title="refinery.shell.xtace.unpack" href="#refinery.shell.xtace.unpack">unpack</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="refinery.shell.xtasar" href="#refinery.shell.xtasar">xtasar</a></code></h4>
<ul class="">
<li><code><a title="refinery.shell.xtasar.unpack" href="#refinery.shell.xtasar.unpack">unpack</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="refinery.shell.xtcab" href="#refinery.shell.xtcab">xtcab</a></code></h4>
<ul class="">
<li><code><a title="refinery.shell.xtcab.unpack" href="#refinery.shell.xtcab.unpack">unpack</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="refinery.shell.xtcpio" href="#refinery.shell.xtcpio">xtcpio</a></code></h4>
<ul class="">
<li><code><a title="refinery.shell.xtcpio.unpack" href="#refinery.shell.xtcpio.unpack">unpack</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="refinery.shell.xtdoc" href="#refinery.shell.xtdoc">xtdoc</a></code></h4>
<ul class="">
<li><code><a title="refinery.shell.xtdoc.unpack" href="#refinery.shell.xtdoc.unpack">unpack</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="refinery.shell.xtea" href="#refinery.shell.xtea">xtea</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.xtgz" href="#refinery.shell.xtgz">xtgz</a></code></h4>
<ul class="">
<li><code><a title="refinery.shell.xtgz.unpack" href="#refinery.shell.xtgz.unpack">unpack</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="refinery.shell.xthtml" href="#refinery.shell.xthtml">xthtml</a></code></h4>
<ul class="">
<li><code><a title="refinery.shell.xthtml.unpack" href="#refinery.shell.xthtml.unpack">unpack</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="refinery.shell.xtinno" href="#refinery.shell.xtinno">xtinno</a></code></h4>
<ul class="">
<li><code><a title="refinery.shell.xtinno.unpack" href="#refinery.shell.xtinno.unpack">unpack</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="refinery.shell.xtiso" href="#refinery.shell.xtiso">xtiso</a></code></h4>
<ul class="">
<li><code><a title="refinery.shell.xtiso.unpack" href="#refinery.shell.xtiso.unpack">unpack</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="refinery.shell.xtiss" href="#refinery.shell.xtiss">xtiss</a></code></h4>
<ul class="">
<li><code><a title="refinery.shell.xtiss.unpack" href="#refinery.shell.xtiss.unpack">unpack</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="refinery.shell.xtjson" href="#refinery.shell.xtjson">xtjson</a></code></h4>
<ul class="">
<li><code><a title="refinery.shell.xtjson.unpack" href="#refinery.shell.xtjson.unpack">unpack</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="refinery.shell.xtmacho" href="#refinery.shell.xtmacho">xtmacho</a></code></h4>
<ul class="">
<li><code><a title="refinery.shell.xtmacho.unpack" href="#refinery.shell.xtmacho.unpack">unpack</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="refinery.shell.xtmagtape" href="#refinery.shell.xtmagtape">xtmagtape</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.xtmail" href="#refinery.shell.xtmail">xtmail</a></code></h4>
<ul class="">
<li><code><a title="refinery.shell.xtmail.unpack" href="#refinery.shell.xtmail.unpack">unpack</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="refinery.shell.xtmsi" href="#refinery.shell.xtmsi">xtmsi</a></code></h4>
<ul class="">
<li><code><a title="refinery.shell.xtmsi.unpack" href="#refinery.shell.xtmsi.unpack">unpack</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="refinery.shell.xtnode" href="#refinery.shell.xtnode">xtnode</a></code></h4>
<ul class="">
<li><code><a title="refinery.shell.xtnode.unpack" href="#refinery.shell.xtnode.unpack">unpack</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="refinery.shell.xtnsis" href="#refinery.shell.xtnsis">xtnsis</a></code></h4>
<ul class="">
<li><code><a title="refinery.shell.xtnsis.unpack" href="#refinery.shell.xtnsis.unpack">unpack</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="refinery.shell.xtnuitka" href="#refinery.shell.xtnuitka">xtnuitka</a></code></h4>
<ul class="">
<li><code><a title="refinery.shell.xtnuitka.unpack" href="#refinery.shell.xtnuitka.unpack">unpack</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="refinery.shell.xtone" href="#refinery.shell.xtone">xtone</a></code></h4>
<ul class="">
<li><code><a title="refinery.shell.xtone.unpack" href="#refinery.shell.xtone.unpack">unpack</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="refinery.shell.xtp" href="#refinery.shell.xtp">xtp</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.xtpdf" href="#refinery.shell.xtpdf">xtpdf</a></code></h4>
<ul class="">
<li><code><a title="refinery.shell.xtpdf.unpack" href="#refinery.shell.xtpdf.unpack">unpack</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="refinery.shell.xtpyi" href="#refinery.shell.xtpyi">xtpyi</a></code></h4>
<ul class="">
<li><code><a title="refinery.shell.xtpyi.unpack" href="#refinery.shell.xtpyi.unpack">unpack</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="refinery.shell.xtrtf" href="#refinery.shell.xtrtf">xtrtf</a></code></h4>
<ul class="">
<li><code><a title="refinery.shell.xtrtf.unpack" href="#refinery.shell.xtrtf.unpack">unpack</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="refinery.shell.xtsql" href="#refinery.shell.xtsql">xtsql</a></code></h4>
<ul class="">
<li><code><a title="refinery.shell.xtsql.unpack" href="#refinery.shell.xtsql.unpack">unpack</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="refinery.shell.xttar" href="#refinery.shell.xttar">xttar</a></code></h4>
<ul class="">
<li><code><a title="refinery.shell.xttar.unpack" href="#refinery.shell.xttar.unpack">unpack</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="refinery.shell.xtvba" href="#refinery.shell.xtvba">xtvba</a></code></h4>
<ul class="">
<li><code><a title="refinery.shell.xtvba.unpack" href="#refinery.shell.xtvba.unpack">unpack</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="refinery.shell.xtw" href="#refinery.shell.xtw">xtw</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.xtxml" href="#refinery.shell.xtxml">xtxml</a></code></h4>
<ul class="">
<li><code><a title="refinery.shell.xtxml.unpack" href="#refinery.shell.xtxml.unpack">unpack</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="refinery.shell.xtzip" href="#refinery.shell.xtzip">xtzip</a></code></h4>
<ul class="">
<li><code><a title="refinery.shell.xtzip.unpack" href="#refinery.shell.xtzip.unpack">unpack</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="refinery.shell.xtzpaq" href="#refinery.shell.xtzpaq">xtzpaq</a></code></h4>
<ul class="">
<li><code><a title="refinery.shell.xtzpaq.unpack" href="#refinery.shell.xtzpaq.unpack">unpack</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="refinery.shell.xxh" href="#refinery.shell.xxh">xxh</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.xxtea" href="#refinery.shell.xxtea">xxtea</a></code></h4>
<ul class="">
<li><code><a title="refinery.shell.xxtea.encrypt" href="#refinery.shell.xxtea.encrypt">encrypt</a></code></li>
<li><code><a title="refinery.shell.xxtea.decrypt" href="#refinery.shell.xxtea.decrypt">decrypt</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="refinery.shell.zl" href="#refinery.shell.zl">zl</a></code></h4>
</li>
<li>
<h4><code><a title="refinery.shell.zstd" href="#refinery.shell.zstd">zstd</a></code></h4>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
<script>
hljs.configure({languages: []})
hljs.initHighlightingOnLoad()
</script>
</body>
</html>